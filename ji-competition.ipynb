{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to run in the terminal before ipython notebook if we have the locale bug\n",
    "# export LC_ALL=en_US.UTF-8\n",
    "# export LANG=en_US.UTF-8\n",
    "# source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 #include \"theano_mod_helper.h\"\n",
      "6 \n",
      "7 #include <numpy/arrayobject.h>\n",
      "8 #include <iostream>\n",
      "9 \n",
      "10 #include \"cuda_ndarray.cuh\"\n",
      "11 \n",
      "12 #ifndef CNMEM_DLLEXPORT\n",
      "13 #define CNMEM_DLLEXPORT\n",
      "14 #endif\n",
      "15 \n",
      "16 #include \"cnmem.h\"\n",
      "17 #include \"cnmem.cpp\"\n",
      "18 \n",
      "19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "20 #define COMPUTE_GPU_MEM_USED 0\n",
      "21 \n",
      "22 //If true, we fill with NAN allocated device memory.\n",
      "23 #define ALLOC_MEMSET 0\n",
      "24 \n",
      "25 //If true, we print out when we free a device pointer, uninitialize a\n",
      "26 //CudaNdarray, or allocate a device pointer\n",
      "27 #define PRINT_FREE_MALLOC 0\n",
      "28 \n",
      "29 //If true, we do error checking at the start of functions, to make sure there\n",
      "30 //is not a pre-existing error when the function is called.\n",
      "31 //You probably need to set the environment variable\n",
      "32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "33 //preprocessor macro in cuda_ndarray.cuh\n",
      "34 //if you want this to work.\n",
      "35 #define PRECHECK_ERROR 0\n",
      "36 \n",
      "37 cublasHandle_t handle = NULL;\n",
      "38 int* err_var = NULL;\n",
      "39 \n",
      "40 /////////////////////////\n",
      "41 // Alloc and Free\n",
      "42 /////////////////////////\n",
      "43 \n",
      "44 static int g_gpu_context_active = 0;\n",
      "45 \n",
      "46 \n",
      "47 PyObject *\n",
      "48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "50 \n",
      "51 \n",
      "52 /**\n",
      "53  *\n",
      "54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "56  *\n",
      "57  */\n",
      "58 int _outstanding_mallocs[] = {0,0};\n",
      "59 \n",
      "60 #if COMPUTE_GPU_MEM_USED\n",
      "61 size_t _allocated_size = 0;\n",
      "62 size_t _max_allocated_size = 0;\n",
      "63 \n",
      "64 const int TABLE_SIZE = 10000;\n",
      "65 struct table_struct{\n",
      "66     void* ptr;\n",
      "67     size_t size;\n",
      "68 };\n",
      "69 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "70 #endif\n",
      "71 \n",
      "72 void * device_malloc(size_t size)\n",
      "73 {\n",
      "74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "75 }\n",
      "76 \n",
      "77 ///@TODO: thejaswi: link this option to a theano config variable?\n",
      "78 static bool g_use_cnmem = false;\n",
      "79 static const int g_max_devices = 8;\n",
      "80 int initCnmem(int card_number_provided, int card_nb, size_t mem) {\n",
      "81     static bool cnmemInitialized = false;\n",
      "82     if(cnmemInitialized) {\n",
      "83         return 0;\n",
      "84     }\n",
      "85     // On stderr to be at the same place as \"Using gpu device...\"\n",
      "86     int numDevices = 0;\n",
      "87     cnmemDevice_t devices[g_max_devices];\n",
      "88     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {\n",
      "89         PyErr_Format(PyExc_RuntimeError,\n",
      "90                      \"initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\\n\",\n",
      "91                      cudaGetErrorString(cudaGetLastError()));\n",
      "92         return -1;\n",
      "93     }\n",
      "94     if(card_number_provided){\n",
      "95         numDevices = 1;\n",
      "96         int i = 0;\n",
      "97         devices[i].device = card_nb;\n",
      "98         devices[i].size = mem;\n",
      "99         ///@TODO: thejaswi: add support for multiple streams\n",
      "100         devices[i].numStreams = 0;\n",
      "101         devices[i].streams = NULL;\n",
      "102         devices[i].streamSizes = NULL;\n",
      "103     }else{\n",
      "104         for(int i=0;i<numDevices;++i) {\n",
      "105             devices[i].device = i;\n",
      "106             devices[i].size = mem;\n",
      "107             ///@TODO: thejaswi: add support for multiple streams\n",
      "108             devices[i].numStreams = 0;\n",
      "109             devices[i].streams = NULL;\n",
      "110         }\n",
      "111     }\n",
      "112 \n",
      "113     ///@TODO: thejaswi: passing custom cnmem flags?\n",
      "114     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);\n",
      "115     if(status != CNMEM_STATUS_SUCCESS) {\n",
      "116         PyErr_Format(PyExc_RuntimeError,\n",
      "117                      \"initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\\n\",\n",
      "118                      cnmemGetErrorString(status), numDevices);\n",
      "119         return -1;\n",
      "120     }\n",
      "121     cnmemInitialized = true;\n",
      "122     return 0;\n",
      "123 }\n",
      "124 \n",
      "125 void * device_malloc(size_t size, int verbose)\n",
      "126 {\n",
      "127     #if PRECHECK_ERROR\n",
      "128         cudaThreadSynchronize();\n",
      "129         cudaError_t prevError = cudaGetLastError();\n",
      "130         if (cudaSuccess != prevError)\n",
      "131         {\n",
      "132             fprintf(stderr,\n",
      "133                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "134                     cudaGetErrorString(prevError)\n",
      "135                     );\n",
      "136         }\n",
      "137     #endif\n",
      "138     void * rval=NULL;\n",
      "139     ///@TODO: thejaswi: support for multiple-streams?\n",
      "140     if(g_use_cnmem) {\n",
      "141         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;\n",
      "142         status = cnmemMalloc(&rval, size, NULL);\n",
      "143         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "144             PyErr_Format(PyExc_MemoryError,\n",
      "145                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "146                          (unsigned long long)size, cnmemGetErrorString(status));\n",
      "147             return NULL;\n",
      "148         }\n",
      "149     }\n",
      "150     else {\n",
      "151         cudaError_t err = cudaMalloc(&rval, size);\n",
      "152         if (cudaSuccess != err)\n",
      "153         {\n",
      "154             // Clear the error flag, cudaMalloc doesn't do it.\n",
      "155             // Currently this returns the same thing as err, but if in future\n",
      "156             // it returns something else I still don't see why we should ignore\n",
      "157             // it.  All we want to do here is reset the flag.\n",
      "158             cudaGetLastError();\n",
      "159             if (verbose)\n",
      "160             {\n",
      "161                 size_t free = 0, total = 0;\n",
      "162                 cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "163                 if (err2 != cudaSuccess){\n",
      "164                     cudaGetLastError();\n",
      "165                     fprintf(stderr,\n",
      "166                             \"Error when trying to find the memory information\"\n",
      "167                             \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "168                 }\n",
      "169                 #if COMPUTE_GPU_MEM_USED\n",
      "170                     fprintf(stderr,\n",
      "171                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "172                             \" new total bytes allocated: %llu.\"\n",
      "173                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "174                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)_allocated_size,\n",
      "175                             (unsigned long long)free, (unsigned long long)total);\n",
      "176                 #else\n",
      "177                     fprintf(stderr,\n",
      "178                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "179                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "180                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "181                 #endif\n",
      "182             }\n",
      "183             PyErr_Format(PyExc_MemoryError,\n",
      "184                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "185                          (unsigned long long)size, cudaGetErrorString(err));\n",
      "186             return NULL;\n",
      "187         }\n",
      "188     }\n",
      "189     if (rval != NULL){\n",
      "190         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "191         // Could this be what happen if size is 0?\n",
      "192         _outstanding_mallocs[0] += 1;\n",
      "193 \n",
      "194 #if COMPUTE_GPU_MEM_USED\n",
      "195         _allocated_size += size;\n",
      "196         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "197         int i = 0;\n",
      "198         for(;i<TABLE_SIZE;i++){\n",
      "199             if(NULL==_alloc_size_table[i].ptr){\n",
      "200                 _alloc_size_table[i].ptr=rval;\n",
      "201                 _alloc_size_table[i].size=size;\n",
      "202                 break;\n",
      "203             }\n",
      "204         }\n",
      "205         if (i == TABLE_SIZE){\n",
      "206             fprintf(stderr,\n",
      "207                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "208                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "209         }\n",
      "210 #endif\n",
      "211     }\n",
      "212     //fprintf(stderr,\n",
      "213     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "214     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "215 \n",
      "216     if(ALLOC_MEMSET){\n",
      "217         //We init them to nan to make sure we catch more debug case.\n",
      "218         cudaMemset(rval, 0xFF, size);\n",
      "219         //printf(\"MEMSET\\n\");\n",
      "220     }\n",
      "221     #if PRINT_FREE_MALLOC\n",
      "222         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "223     #endif\n",
      "224     return rval;\n",
      "225 }\n",
      "226 \n",
      "227 int device_free(void *ptr)\n",
      "228 {\n",
      "229     #if PRECHECK_ERROR\n",
      "230         cudaThreadSynchronize();\n",
      "231         cudaError_t prevError = cudaGetLastError();\n",
      "232         if (cudaSuccess != prevError)\n",
      "233         {\n",
      "234             fprintf(stderr,\n",
      "235                     \"Error existed before calling device_free. %s\\n\",\n",
      "236                     cudaGetErrorString(prevError)\n",
      "237                     );\n",
      "238         }\n",
      "239     #endif\n",
      "240     #if PRINT_FREE_MALLOC\n",
      "241         size_t free = 0, total = 0;\n",
      "242         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "243         if (err2 != cudaSuccess){\n",
      "244             cudaGetLastError();\n",
      "245             fprintf(stderr,\n",
      "246                     \"Error when tring to find the memory information\"\n",
      "247                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "248         }\n",
      "249         #if COMPUTE_GPU_MEM_USED\n",
      "250         {\n",
      "251             int i = 0;\n",
      "252             for(;i<TABLE_SIZE;i++)\n",
      "253                 if(_alloc_size_table[i].ptr==ptr){\n",
      "254                     break;\n",
      "255                 }\n",
      "256             assert(i<TABLE_SIZE);\n",
      "257             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "258                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "259                     ptr, _alloc_size_table[i].size, free, total);\n",
      "260         }\n",
      "261         #else\n",
      "262             fprintf(stderr, \"device_free %p.\"\n",
      "263                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "264                     ptr, free, total);\n",
      "265         #endif\n",
      "266     #endif\n",
      "267 \n",
      "268     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "269     if(!g_gpu_context_active) {\n",
      "270         return 0;\n",
      "271     }\n",
      "272 \n",
      "273     ///@TODO: thejaswi: multi-stream support\n",
      "274     if(g_use_cnmem) {\n",
      "275         cnmemStatus_t status = cnmemFree(ptr, NULL);\n",
      "276         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "277             fprintf(stderr, \"device_free: cnmemFree call failed! Reason=%s\\n\",\n",
      "278                     cnmemGetErrorString(status));\n",
      "279         }\n",
      "280     }\n",
      "281     else {\n",
      "282         // We need sync as the Theano's GC could remove intermediate variable that\n",
      "283         // are still needed as the gpu kernel are running or in the queue.\n",
      "284         CNDA_BEGIN_ALLOW_THREADS\n",
      "285         cudaThreadSynchronize();\n",
      "286         CNDA_END_ALLOW_THREADS\n",
      "287 \n",
      "288         cudaError_t err =  cudaFree(ptr);\n",
      "289         if (cudaSuccess != err)\n",
      "290         {\n",
      "291             // Clear the error flag, cudaFree doesn't do it.\n",
      "292             // Currently this returns the same thing as err, but if in future\n",
      "293             // it returns something else I still don't see why we should ignore\n",
      "294             // it.  All we want to do here is reset the flag.\n",
      "295             cudaGetLastError();\n",
      "296             size_t free = 0, total = 0;\n",
      "297             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "298             if (err2 != cudaSuccess){\n",
      "299                 cudaGetLastError();\n",
      "300                 fprintf(stderr,\n",
      "301                         \"Error when tring to find the memory information\"\n",
      "302                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "303             }\n",
      "304             #if COMPUTE_GPU_MEM_USED\n",
      "305             {\n",
      "306                 int i = 0;\n",
      "307                 for(;i<TABLE_SIZE;i++)\n",
      "308                     if(_alloc_size_table[i].ptr==ptr){\n",
      "309                         break;\n",
      "310                     }\n",
      "311                 assert(i<TABLE_SIZE);\n",
      "312                 fprintf(stderr,\n",
      "313                         \"Error freeing device pointer %p (%s) of size %llu. %llu byte already allocated.\"\n",
      "314                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "315                         ptr, cudaGetErrorString(err),\n",
      "316                         (unsigned long long)_alloc_size_table[i].size, (unsigned long long)_allocated_size, (unsigned long long)free, (unsigned long long)total);\n",
      "317             }\n",
      "318             #else\n",
      "319                 fprintf(stderr,\n",
      "320                         \"Error freeing device pointer %p (%s).\"\n",
      "321                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "322                         ptr,\n",
      "323                         cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "324             #endif\n",
      "325             if (NULL != PyErr_Occurred()){\n",
      "326                 fprintf(stderr,\n",
      "327                         \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "328                         \" Python error set. This happen during the clean up when there is a\"\n",
      "329                         \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "330                         \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "331                 return -1;\n",
      "332             }\n",
      "333             PyErr_Format(PyExc_MemoryError,\n",
      "334                     \"error freeing device pointer %p (%s)\",\n",
      "335                     ptr,\n",
      "336                     cudaGetErrorString(err));\n",
      "337             return -1;\n",
      "338         }\n",
      "339     }\n",
      "340     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "341     #if COMPUTE_GPU_MEM_USED\n",
      "342         int i=0;\n",
      "343         size_t total_freed = 0;\n",
      "344         for(;i<TABLE_SIZE;i++)\n",
      "345             if(_alloc_size_table[i].ptr==ptr){\n",
      "346                 _allocated_size -= _alloc_size_table[i].size;\n",
      "347                 total_freed += _alloc_size_table[i].size;\n",
      "348                 _alloc_size_table[i].ptr=0;\n",
      "349                 _alloc_size_table[i].size=0;\n",
      "350 \n",
      "351                 break;\n",
      "352             }\n",
      "353         //if(i==TABLE_SIZE)\n",
      "354         //    printf(\"Unallocated unknow size!\\n\");\n",
      "355         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "356     #endif\n",
      "357     return 0;\n",
      "358 }\n",
      "359 \n",
      "360 static PyObject *\n",
      "361 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "362 {\n",
      "363     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "364 }\n",
      "365 \n",
      "366 \n",
      "367 static void *work_mem = NULL;\n",
      "368 static size_t work_size = 0;\n",
      "369 \n",
      "370 /*\n",
      "371  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "372  * request a single chunk of memory at a time since it is reused.\n",
      "373  */\n",
      "374 void *get_work_mem(size_t sz) {\n",
      "375     if (sz <= work_size)\n",
      "376         return work_mem;\n",
      "377     device_free(work_mem);\n",
      "378     work_mem = device_malloc(sz);\n",
      "379     work_size = sz;\n",
      "380     if (work_mem == NULL)\n",
      "381         work_size = 0;\n",
      "382     return work_mem;\n",
      "383 }\n",
      "384 \n",
      "385 /////////////////////////\n",
      "386 // Static helper methods\n",
      "387 /////////////////////////\n",
      "388 \n",
      "389 static void\n",
      "390 CudaNdarray_null_init(CudaNdarray*self)\n",
      "391 {\n",
      "392     self->base = NULL;\n",
      "393     self->nd = -1;\n",
      "394     self->host_structure = NULL;\n",
      "395     self->data_allocated = 0;\n",
      "396     self->dev_structure_fresh = 1;\n",
      "397     self->dev_structure = NULL;\n",
      "398     self->devdata = NULL;\n",
      "399 }\n",
      "400 \n",
      "401 static int\n",
      "402 CudaNdarray_uninit(CudaNdarray*self)\n",
      "403 {\n",
      "404     #if PRINT_FREE_MALLOC\n",
      "405         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "406     #endif\n",
      "407     int rval = 0;\n",
      "408     if (self->data_allocated) {\n",
      "409         assert(self->devdata);\n",
      "410         if (device_free(self->devdata))\n",
      "411         {\n",
      "412             fprintf(stderr,\n",
      "413                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "414                     self, self->devdata);\n",
      "415             rval = -1;\n",
      "416         }\n",
      "417         self->devdata = NULL;\n",
      "418         self->data_allocated = 0;\n",
      "419     }\n",
      "420     if (self->dev_structure)\n",
      "421     {\n",
      "422         if (device_free(self->dev_structure))\n",
      "423         {\n",
      "424             fprintf(stderr,\n",
      "425                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "426                     self->dev_structure, self);\n",
      "427             rval = -1;\n",
      "428         }\n",
      "429         self->dev_structure = NULL;\n",
      "430     }\n",
      "431     if (self->host_structure)\n",
      "432     {\n",
      "433         free(self->host_structure);\n",
      "434         self->host_structure = NULL;\n",
      "435     }\n",
      "436     self->nd = -1;\n",
      "437     Py_XDECREF(self->base);\n",
      "438     self->base = NULL;\n",
      "439     return rval;\n",
      "440 }\n",
      "441 \n",
      "442 \n",
      "443 //make the rightmost coords change fastest\n",
      "444 //TODO: why does a downward for-loop not work????\n",
      "445 //TODO: use the log2_dims and driver code to remove / and %\n",
      "446 //TODO: skip the last division (when d == 0)\n",
      "447 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "448 __global__ void name (unsigned int numEls,  \\\n",
      "449         unsigned int nd, \\\n",
      "450         const int * dim,  \\\n",
      "451         const float * a_data, const int * a_str, \\\n",
      "452         float * z_data, const int * z_str) \\\n",
      "453 { \\\n",
      "454     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "455     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "456  \\\n",
      "457     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "458     { \\\n",
      "459         unsigned int ii = i; \\\n",
      "460         const float * a_i = a_data; \\\n",
      "461         float * z_i = z_data; \\\n",
      "462         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "463         { \\\n",
      "464             unsigned int d = nd - _d-1;  \\\n",
      "465             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "466             ii = ii / dim[d]; \\\n",
      "467             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "468             z_i += i_d * z_str[d]; \\\n",
      "469         } \\\n",
      "470         z_i[0] = F(a_i[0]); \\\n",
      "471     } \\\n",
      "472 }\n",
      "473 \n",
      "474 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "475 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "476 \n",
      "477 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "478 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "479 \n",
      "480 /////////////////////////////\n",
      "481 // Satisfying reqs to be Type\n",
      "482 /////////////////////////////\n",
      "483 \n",
      "484 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "485 static void\n",
      "486 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "487 {\n",
      "488     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "489     if(Py_REFCNT(self) > 1)\n",
      "490       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "491     CudaNdarray_uninit(self);\n",
      "492     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "493     --_outstanding_mallocs[1];\n",
      "494     if (0)\n",
      "495     {\n",
      "496         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "497                 _outstanding_mallocs[0],\n",
      "498                 _outstanding_mallocs[1]);\n",
      "499     }\n",
      "500 }\n",
      "501 \n",
      "502 static PyObject *\n",
      "503 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "504 {\n",
      "505     CudaNdarray *self;\n",
      "506 \n",
      "507     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "508     if (self != NULL)\n",
      "509     {\n",
      "510         CudaNdarray_null_init(self);\n",
      "511         ++_outstanding_mallocs[1];\n",
      "512     }\n",
      "513     return (PyObject *)self;\n",
      "514 }\n",
      "515 static int\n",
      "516 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "517 {\n",
      "518     PyObject *arr=NULL;\n",
      "519 \n",
      "520     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "521         return -1;\n",
      "522     if (! PyArray_Check(arr))\n",
      "523     {\n",
      "524         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "525         return -1;\n",
      "526     }\n",
      "527     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "528     return rval;\n",
      "529 }\n",
      "530 static PyMemberDef CudaNdarray_members[] =\n",
      "531 {\n",
      "532     /*\n",
      "533     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "534      \"first name\"},\n",
      "535     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "536      \"last name\"},\n",
      "537     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "538      \"noddy number\"},\n",
      "539      */\n",
      "540     {NULL}  /* Sentinel */\n",
      "541 };\n",
      "542 \n",
      "543 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "544 {\n",
      "545     PyObject * dtype = NULL;\n",
      "546     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "547         return NULL;\n",
      "548     if (dtype) {\n",
      "549         PyArray_Descr* dtype2;\n",
      "550         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "551         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "552         {\n",
      "553             PyObject * str = PyObject_Repr(dtype);\n",
      "554             PyErr_Format(PyExc_TypeError,\n",
      "555                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "556                          PyString_AsString(str)\n",
      "557                          );\n",
      "558             Py_CLEAR(str);\n",
      "559             return NULL;\n",
      "560         }\n",
      "561         int typeNum = dtype2->type_num;\n",
      "562         Py_DECREF(dtype2);\n",
      "563         if (typeNum != NPY_FLOAT32)\n",
      "564         {\n",
      "565             PyObject * str = PyObject_Repr(dtype);\n",
      "566             PyErr_Format(PyExc_TypeError,\n",
      "567                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "568                          typeNum\n",
      "569                          );\n",
      "570             Py_CLEAR(str);\n",
      "571             return NULL;\n",
      "572         }\n",
      "573     }\n",
      "574 \n",
      "575     int verbose = 0;\n",
      "576     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "577         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "578         assert (npydims);\n",
      "579         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "580         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "581         free(npydims);\n",
      "582         if (!rval){\n",
      "583             return NULL;\n",
      "584         }\n",
      "585         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "586         return rval;\n",
      "587     }\n",
      "588     if ((self->nd < 0) || (self->devdata == 0))\n",
      "589     {\n",
      "590         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "591         return NULL;\n",
      "592     }\n",
      "593     CudaNdarray * contiguous_self = NULL;\n",
      "594     if (CudaNdarray_is_c_contiguous(self))\n",
      "595     {\n",
      "596         contiguous_self = self;\n",
      "597         Py_INCREF(contiguous_self);\n",
      "598         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "599     }\n",
      "600     else\n",
      "601     {\n",
      "602         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "603         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "604     }\n",
      "605     if (!contiguous_self)\n",
      "606     {\n",
      "607         return NULL;\n",
      "608     }\n",
      "609 \n",
      "610     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "611     assert (npydims);\n",
      "612     for (int i = 0; i < self->nd; ++i)\n",
      "613         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "614     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "615                                                                npydims,\n",
      "616                                                                REAL_TYPENUM);\n",
      "617     free(npydims);\n",
      "618     if (!rval)\n",
      "619     {\n",
      "620         Py_DECREF(contiguous_self);\n",
      "621         return NULL;\n",
      "622     }\n",
      "623 \n",
      "624     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "625 \n",
      "626     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "627     void *rval_data = PyArray_DATA(rval);\n",
      "628     cudaError_t err;\n",
      "629     CNDA_BEGIN_ALLOW_THREADS;\n",
      "630 \n",
      "631     err = cudaMemcpy(rval_data, contiguous_self->devdata,\n",
      "632                      rval_size * sizeof(real),\n",
      "633                      cudaMemcpyDeviceToHost\n",
      "634                      );\n",
      "635     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "636     CNDA_END_ALLOW_THREADS;\n",
      "637 \n",
      "638     if (cudaSuccess != err)\n",
      "639     {\n",
      "640         PyErr_Format(PyExc_RuntimeError, \"error (%s)copying data to host\",\n",
      "641                      cudaGetErrorString(err));\n",
      "642         Py_DECREF(rval);\n",
      "643         rval = NULL;\n",
      "644     }\n",
      "645 \n",
      "646     Py_DECREF(contiguous_self);\n",
      "647     return (PyObject *)rval;\n",
      "648 }\n",
      "649 \n",
      "650 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "651 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "652 // but this naming is very weird, makes it look like a macro\n",
      "653 // we should figure out the correct convention and change to that\n",
      "654 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "655 {\n",
      "656 \n",
      "657     size_t total_elements = 1;\n",
      "658 \n",
      "659     for(size_t i=0;i<n;i++){\n",
      "660         // Detect overflow on unsigned integer\n",
      "661         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "662             PyErr_Format(PyExc_RuntimeError,\n",
      "663                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "664                          (unsigned long long)total_elements,\n",
      "665                          (unsigned long long)dims[i]);\n",
      "666             return NULL;\n",
      "667         }\n",
      "668         total_elements*=dims[i];\n",
      "669     }\n",
      "670 \n",
      "671     // total_elements now contains the size of the array, in reals\n",
      "672     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "673         PyErr_Format(PyExc_RuntimeError,\n",
      "674                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "675                      (unsigned long long)total_elements);\n",
      "676         return NULL;\n",
      "677     }\n",
      "678     size_t total_size = total_elements * sizeof(real);\n",
      "679 \n",
      "680     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "681     if (!rval)\n",
      "682     {\n",
      "683         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "684         return NULL;\n",
      "685     }\n",
      "686 \n",
      "687     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "688     {\n",
      "689         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "690         Py_DECREF(rval);\n",
      "691         return NULL;\n",
      "692     }\n",
      "693 \n",
      "694     // Fill with zeros\n",
      "695     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "696     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "697     {\n",
      "698         PyErr_Format(PyExc_MemoryError,\n",
      "699                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "700                      (unsigned long long)total_size);\n",
      "701         Py_DECREF(rval);\n",
      "702         return NULL;\n",
      "703     }\n",
      "704 \n",
      "705     if (cnda_copy_structure_to_device(rval))\n",
      "706     {\n",
      "707         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "708         Py_DECREF(rval);\n",
      "709         return NULL;\n",
      "710     }\n",
      "711     return (PyObject*) rval;\n",
      "712 }\n",
      "713 \n",
      "714 // declared as a static method (hence 1st parameter is not used)\n",
      "715 // Based on _Copy and _dimshuffle\n",
      "716 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "717 {\n",
      "718     if(!shape)\n",
      "719     {\n",
      "720         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "721         return NULL;\n",
      "722     }\n",
      "723     if(!PySequence_Check(shape))\n",
      "724     {\n",
      "725         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "726         return NULL;\n",
      "727     }\n",
      "728 \n",
      "729     int shplen = PySequence_Length(shape);\n",
      "730 \n",
      "731     if (shplen == 0)\n",
      "732     {\n",
      "733         return CudaNdarray_ZEROS(0, NULL);\n",
      "734     }\n",
      "735 \n",
      "736     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "737 \n",
      "738     if (!newdims)\n",
      "739     {\n",
      "740         PyErr_SetString(PyExc_MemoryError,\n",
      "741             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "742         return NULL;\n",
      "743     }\n",
      "744 \n",
      "745     // start from the end to compute strides\n",
      "746     for (int i = shplen-1; i >= 0; --i)\n",
      "747     {\n",
      "748         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "749         if(shp_el_obj == NULL)\n",
      "750         {\n",
      "751             // shouldn't happen since we checked length before...\n",
      "752             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "753             free(newdims);\n",
      "754             return NULL;\n",
      "755         }\n",
      "756 \n",
      "757         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "758         Py_DECREF(shp_el_obj);\n",
      "759 \n",
      "760         if (shp_el < 0)\n",
      "761         {\n",
      "762             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "763             free(newdims);\n",
      "764             return NULL;\n",
      "765         }\n",
      "766 \n",
      "767         newdims[i] = shp_el;\n",
      "768     }\n",
      "769 \n",
      "770     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "771 \n",
      "772     free(newdims);\n",
      "773 \n",
      "774     return (PyObject*)rval;\n",
      "775 }\n",
      "776 \n",
      "777 \n",
      "778 \n",
      "779 \n",
      "780 \n",
      "781 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "782 {\n",
      "783     PyObject * rval = CudaNdarray_New();\n",
      "784     if ((!rval) || (-1 == self->nd))\n",
      "785     {\n",
      "786         return rval;\n",
      "787     }\n",
      "788     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "789     {\n",
      "790         Py_DECREF(rval);\n",
      "791         return NULL;\n",
      "792     }\n",
      "793     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "794     {\n",
      "795         Py_DECREF(rval);\n",
      "796         return NULL;\n",
      "797     }\n",
      "798     return rval;\n",
      "799 }\n",
      "800 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "801 {\n",
      "802     assert(PyDict_Check(memo));\n",
      "803     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "804     assert(selfkey);\n",
      "805     if (PyDict_Contains(memo, selfkey))\n",
      "806     {\n",
      "807         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "808         Py_DECREF(selfkey);\n",
      "809         Py_XINCREF(rval);\n",
      "810         return rval;\n",
      "811     }\n",
      "812     else\n",
      "813     {\n",
      "814         PyObject * rval = CudaNdarray_Copy(self);\n",
      "815         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "816         if (NULL == rval)\n",
      "817         {\n",
      "818             Py_DECREF(selfkey);\n",
      "819             return NULL;\n",
      "820         }\n",
      "821         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "822         {\n",
      "823             Py_DECREF(rval);\n",
      "824             Py_DECREF(selfkey);\n",
      "825             return NULL;\n",
      "826         }\n",
      "827         Py_DECREF(selfkey);\n",
      "828         return rval;\n",
      "829     }\n",
      "830 }\n",
      "831 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "832 {\n",
      "833     if (!PySequence_Check(py_reduce_mask))\n",
      "834     {\n",
      "835         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "836         return NULL;\n",
      "837     }\n",
      "838     int len = PySequence_Length(py_reduce_mask);\n",
      "839     if (len != self->nd)\n",
      "840     {\n",
      "841         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "842         return NULL;\n",
      "843     }\n",
      "844     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "845     if (!self_sum)\n",
      "846     {\n",
      "847         return NULL;\n",
      "848     }\n",
      "849     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "850     //      and use it if it is big enough.\n",
      "851     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "852     int * sum_dims = dimshuffle_pattern + len;\n",
      "853     int n_remaining_dims = 0;\n",
      "854     if (!dimshuffle_pattern)\n",
      "855     {\n",
      "856         Py_DECREF(self_sum);\n",
      "857         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "858         return NULL;\n",
      "859     }\n",
      "860     for (int i = 0; i < len; ++i)\n",
      "861     {\n",
      "862         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "863         int o_i_int = PyInt_AsLong(o_i);\n",
      "864         Py_XDECREF(o_i);\n",
      "865         if (PyErr_Occurred())\n",
      "866         {\n",
      "867             Py_DECREF(self_sum);\n",
      "868             free(dimshuffle_pattern);\n",
      "869             return NULL;\n",
      "870         }\n",
      "871         if (o_i_int) // this is a dimension over which we are reducing\n",
      "872         {\n",
      "873             sum_dims[i] = 1;\n",
      "874         }\n",
      "875         else\n",
      "876         {\n",
      "877             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "878             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "879         }\n",
      "880     }\n",
      "881     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "882             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "883             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "884     {\n",
      "885         Py_DECREF(self_sum);\n",
      "886         free(dimshuffle_pattern);\n",
      "887         return NULL;\n",
      "888     }\n",
      "889     free(dimshuffle_pattern);\n",
      "890     return (PyObject*)self_sum;\n",
      "891 }\n",
      "892 \n",
      "893 // Reshape self to the new shape gived by the tuple shape.\n",
      "894 //\n",
      "895 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "896 // TODO: make it return a view when the strides allow it even if it is not\n",
      "897 //       c contiguous\n",
      "898 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "899 {\n",
      "900     if(!CudaNdarray_is_c_contiguous(self))\n",
      "901     {\n",
      "902         // allocate new space\n",
      "903         //TODO: test to see if we can re-use old one and take a new param to\n",
      "904         //  use this\n",
      "905         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "906         if (!rval)\n",
      "907         {\n",
      "908             return NULL;\n",
      "909         }\n",
      "910 \n",
      "911         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "912         Py_XDECREF(rval);\n",
      "913         return (PyObject*)ret;\n",
      "914     }\n",
      "915 \n",
      "916     // check shape tuple\n",
      "917     unsigned int rval_nd;\n",
      "918     unsigned int * rval_dims;\n",
      "919     size_t rval_size = 1;\n",
      "920 \n",
      "921     if (PyTuple_Check(shape)){\n",
      "922         // copy shape to integer array\n",
      "923         rval_nd = PyTuple_Size(shape);\n",
      "924     }else if (PyInt_Check(shape)){\n",
      "925         rval_nd = 1;\n",
      "926     }else{\n",
      "927         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "928         return NULL;\n",
      "929     }\n",
      "930     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "931 \n",
      "932     if(PyTuple_Check(shape)){\n",
      "933         for (int i = 0; i < rval_nd; ++i)\n",
      "934         {\n",
      "935             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "936             if (PyErr_Occurred()) //error in AsLong\n",
      "937             {\n",
      "938                 free(rval_dims);\n",
      "939                 return NULL;\n",
      "940             }\n",
      "941             if(rval_dims[i]<0){\n",
      "942                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >=0)\",rval_dims[i]);\n",
      "943                 free(rval_dims);\n",
      "944                 return NULL;\n",
      "945             }\n",
      "946             rval_size = rval_size * rval_dims[i];\n",
      "947         }\n",
      "948     }else{\n",
      "949         rval_size = PyInt_AsLong(shape);\n",
      "950         rval_dims[0] = rval_size;\n",
      "951     }\n",
      "952     // calculate new size, assert same as old size\n",
      "953     if (rval_size != CudaNdarray_SIZE(self))\n",
      "954     {\n",
      "955         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %lld to %lld\", CudaNdarray_SIZE(self), rval_size);\n",
      "956         free(rval_dims);\n",
      "957         return NULL;\n",
      "958     }\n",
      "959     if (rval_size==0)\n",
      "960     {\n",
      "961         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "962         free(rval_dims);\n",
      "963         return rval;\n",
      "964     }\n",
      "965 \n",
      "966     //return a view, not a copy\n",
      "967     //we can do this as we checked self is c_contiguous\n",
      "968     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "969 \n",
      "970     if (!rval || 0 != rval->data_allocated\n",
      "971         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "972     {\n",
      "973         Py_XDECREF(rval);\n",
      "974         free(rval_dims);\n",
      "975         return NULL;\n",
      "976     }\n",
      "977     //set dim and stride\n",
      "978     int size = 1;\n",
      "979     for (int i = rval_nd-1; i >= 0; --i)\n",
      "980     {\n",
      "981         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "982         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "983         size = size * rval_dims[i];\n",
      "984     }\n",
      "985     free(rval_dims);\n",
      "986     return (PyObject*)rval;\n",
      "987 }\n",
      "988 \n",
      "989 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "990 {\n",
      "991     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "992     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "993     {\n",
      "994         Py_XDECREF(rval);\n",
      "995         rval = NULL;\n",
      "996     }\n",
      "997     else\n",
      "998     {\n",
      "999         for (int i = 0; i < self->nd; ++i)\n",
      "1000         {\n",
      "1001             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "1002             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "1003         }\n",
      "1004     }\n",
      "1005     return (PyObject*)rval;\n",
      "1006 }\n",
      "1007 \n",
      "1008 /*\n",
      "1009  * d0,... are the output dims\n",
      "1010  * indices are a list of index to operate on\n",
      "1011  *         They are int32 viewed as float32.\n",
      "1012  * a is the output\n",
      "1013  * b is the input\n",
      "1014  * dB0, the source leading dimensions size\n",
      "1015  */\n",
      "1016 template <int operator_num>\n",
      "1017 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "1018                          const npy_int64* indices,\n",
      "1019                          float* a,\n",
      "1020                          const int sA0, const int sA1, const int sA2,\n",
      "1021                          const float* b, const int dB0,\n",
      "1022                          const int sB0, const int sB1, const int sB2,\n",
      "1023                          int* err){\n",
      "1024     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1025         npy_int64 idx = indices[i0];\n",
      "1026         if (idx<0)\n",
      "1027             idx += dB0; // To allow negative indexing.\n",
      "1028         if ((idx < 0) || (idx >= dB0)){\n",
      "1029             // Any value other the 0 probably work. But to be more safe, I want\n",
      "1030             // to change all bits to prevent problem with concurrent write that\n",
      "1031             // could cross cache line. But this should not happen with the\n",
      "1032             // current code and driver.\n",
      "1033             *err = 0xFFFF;\n",
      "1034             continue;\n",
      "1035         }\n",
      "1036         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "1037             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "1038                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "1039                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "1040                 a[a_idx] = b[b_idx];\n",
      "1041             }\n",
      "1042         }\n",
      "1043     }\n",
      "1044 }\n",
      "1045 \n",
      "1046 // We try to be similar to the PyArray_TakeFrom function\n",
      "1047 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "1048 //TODO: support other clip mode then raise(clip, wrap)\n",
      "1049 //self is the input that we copy data from.\n",
      "1050 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "1051 //    that is in fact a view to int64 indices\n",
      "1052 PyObject*\n",
      "1053 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "1054     int verbose = 0;\n",
      "1055     PyObject * indices_obj = NULL;\n",
      "1056     //int axis; Default None, that mean the flattened array.\n",
      "1057     PyObject * axis_obj = Py_None;\n",
      "1058     PyObject * out_obj = Py_None;\n",
      "1059     PyObject * clipmode_obj = NULL;\n",
      "1060     int max_threads = 1; // max threads per blocks\n",
      "1061 \n",
      "1062     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1063                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1064         return NULL;\n",
      "1065 \n",
      "1066     //Check argument indices\n",
      "1067     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1068     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1069     //TODO: Support ndarray of other dtype then int32\n",
      "1070     //TODO: support list of indices that are not c_contiguous\n",
      "1071     CudaNdarray * indices = NULL;\n",
      "1072     if (CudaNdarray_Check(indices_obj)) {\n",
      "1073         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1074         indices = (CudaNdarray*) indices_obj;\n",
      "1075         Py_INCREF(indices);\n",
      "1076     } else if (PyArray_Check(indices_obj)) {\n",
      "1077         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1078         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {\n",
      "1079             PyErr_SetString(PyExc_TypeError,\n",
      "1080                             \"CudaNdarray_TakeFrom: need a ndarray for indices\"\n",
      "1081                             \" with dtype int64\");\n",
      "1082             return NULL;\n",
      "1083         }\n",
      "1084         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1085             PyErr_SetString(PyExc_TypeError,\n",
      "1086                             \"CudaNdarray_TakeFrom: need a CudaNdarray of\"\n",
      "1087                             \" indices with only 1 dimensions\");\n",
      "1088             return NULL;\n",
      "1089         }\n",
      "1090         // We need indices_obj to be contiguous, in order to take a view\n",
      "1091         // with a different dtype.\n",
      "1092         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {\n",
      "1093             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);\n",
      "1094             if (!indices_obj_contig)\n",
      "1095                 return NULL;\n",
      "1096             indices_obj = indices_obj_contig;\n",
      "1097         } else {\n",
      "1098             // Keep the refcount consistent\n",
      "1099             Py_INCREF(indices_obj);\n",
      "1100         }\n",
      "1101         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1102         PyObject * indices_float32 = NULL;\n",
      "1103         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1104                                                   float32_descr, NULL);\n",
      "1105         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1106         if (!indices_float32) {\n",
      "1107             Py_DECREF(indices_obj);\n",
      "1108             return NULL;\n",
      "1109         }\n",
      "1110 \n",
      "1111         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1112         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1113         if (! indices){\n",
      "1114             Py_DECREF(indices_obj);\n",
      "1115             Py_DECREF(indices_float32);\n",
      "1116             return NULL;\n",
      "1117         }\n",
      "1118         if (CudaNdarray_CopyFromArray(indices,\n",
      "1119                                       (PyArrayObject *)indices_float32)){\n",
      "1120             Py_DECREF(indices_obj);\n",
      "1121             Py_DECREF(indices_float32);\n",
      "1122             return NULL;\n",
      "1123         }\n",
      "1124         Py_DECREF(indices_obj);\n",
      "1125         Py_DECREF(indices_float32);\n",
      "1126     } else {\n",
      "1127         PyObject* py_s = PyObject_Str(indices_obj);\n",
      "1128         const char* s = PyString_AsString(py_s);\n",
      "1129         Py_DECREF(py_s);\n",
      "1130         PyErr_Format(PyExc_TypeError,\n",
      "1131                      \"CudaNdarray_TakeFrom: need an ndarray of int64 or a\"\n",
      "1132                      \" CudaNdarray(float32) that is a view from int64 data\"\n",
      "1133                      \" for indices. Got %s\", s);\n",
      "1134         return NULL;\n",
      "1135     }\n",
      "1136 \n",
      "1137     if (verbose) {\n",
      "1138         printf(\"indices used on the gpu\\n\");\n",
      "1139         fprint_CudaNdarray(stdout, indices);\n",
      "1140         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1141         PyObject_Print(used_indices, stdout, 0);\n",
      "1142         Py_DECREF(used_indices);\n",
      "1143     }\n",
      "1144     if (verbose) printf(\"after print of object\\n\");\n",
      "1145     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1146         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1147                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1148         Py_DECREF(indices);\n",
      "1149         return NULL;\n",
      "1150     }\n",
      "1151     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1152 \n",
      "1153     //Check argument axis\n",
      "1154     //TODO: implement the default and other axis\n",
      "1155     long axis = PyInt_AsLong(axis_obj);\n",
      "1156 \n",
      "1157     if (axis != 0) {\n",
      "1158         PyErr_Format(PyExc_NotImplementedError,\n",
      "1159                      \"CudaNdarray_TakeFrom: only axis=0 is currently supported.\"\n",
      "1160                      \" Got %ld.\", axis);\n",
      "1161         Py_DECREF(indices);\n",
      "1162         return NULL;\n",
      "1163     }\n",
      "1164 \n",
      "1165     //Check argument out_obj\n",
      "1166     CudaNdarray * out = NULL;\n",
      "1167     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1168         out = (CudaNdarray*) out_obj;\n",
      "1169     if (out && (out->nd != self->nd ||\n",
      "1170                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1171         out = NULL;\n",
      "1172     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1173     dims[0] = nb_indices;\n",
      "1174 \n",
      "1175     for (int i=1 ; i<self->nd ; i++) {\n",
      "1176         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1177         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1178             out = NULL;\n",
      "1179         }\n",
      "1180     }\n",
      "1181     if (!out) {\n",
      "1182         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1183         if (!out){\n",
      "1184             Py_DECREF(indices);\n",
      "1185             free(dims);\n",
      "1186             return NULL;\n",
      "1187         }\n",
      "1188         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1189             Py_DECREF(out);\n",
      "1190             Py_DECREF(indices);\n",
      "1191             free(dims);\n",
      "1192             return NULL;\n",
      "1193         }\n",
      "1194     }else {\n",
      "1195         Py_INCREF(out);\n",
      "1196     }\n",
      "1197 \n",
      "1198     //Check argument clipmode\n",
      "1199     if (clipmode_obj) {\n",
      "1200         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1201         if (! clipmode){\n",
      "1202             Py_DECREF(indices);\n",
      "1203             Py_DECREF(out);\n",
      "1204             free(dims);\n",
      "1205             return NULL;\n",
      "1206         }\n",
      "1207         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1208             PyErr_Format(PyExc_NotImplementedError,\n",
      "1209                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1210                          clipmode);\n",
      "1211             Py_DECREF(indices);\n",
      "1212             Py_DECREF(out);\n",
      "1213             free(dims);\n",
      "1214             return NULL;\n",
      "1215         }\n",
      "1216     }\n",
      "1217     void (*k3)(const int, const int, const int,\n",
      "1218                const npy_int64*,\n",
      "1219                float*, const int, const int, const int,\n",
      "1220                const float*, const int,\n",
      "1221                const int, const int, const int,\n",
      "1222                int*);\n",
      "1223     k3 = k_take_3<CPY>;\n",
      "1224 \n",
      "1225     // Create the memory place that will store the error information.\n",
      "1226     if(init_err_var() != 0) return NULL;\n",
      "1227 \n",
      "1228     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1229     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1230         // We take 0 elements, so no need for the rest of the code.\n",
      "1231         // This speed up that case AND fix crash otherwise.\n",
      "1232         free(dims);\n",
      "1233         Py_DECREF(indices);\n",
      "1234         return (PyObject *)out;\n",
      "1235     }\n",
      "1236 \n",
      "1237     switch (self->nd) {\n",
      "1238         case 1:\n",
      "1239             {\n",
      "1240                 dim3 n_threads(1, 1, 1);\n",
      "1241                 if (verbose)\n",
      "1242                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1243                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1244                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1245                            cudaGetLastError(), self->nd,\n",
      "1246                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1247                 k3<<<n_blocks, n_threads>>>(\n",
      "1248                         dims[0],\n",
      "1249                         1,\n",
      "1250                         1,\n",
      "1251                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1252                         CudaNdarray_DEV_DATA(out),\n",
      "1253                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1254                         1,\n",
      "1255                         1,\n",
      "1256                         CudaNdarray_DEV_DATA(self),\n",
      "1257                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1258                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1259                         1,\n",
      "1260                         1,\n",
      "1261                         err_var);\n",
      "1262             }\n",
      "1263             break;\n",
      "1264         case 2:\n",
      "1265             {\n",
      "1266                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1267 \n",
      "1268                 if (verbose)\n",
      "1269                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1270                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1271                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1272                            cudaGetLastError(), self->nd,\n",
      "1273                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1274 \n",
      "1275                 k3<<<n_blocks, n_threads>>>(\n",
      "1276                         dims[0], //dimensions\n",
      "1277                         dims[1],\n",
      "1278                         1,\n",
      "1279                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1280                         CudaNdarray_DEV_DATA(out),\n",
      "1281                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1282                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1283                         1,\n",
      "1284                         CudaNdarray_DEV_DATA(self),\n",
      "1285                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1286                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1287                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1288                         1,\n",
      "1289                         err_var);\n",
      "1290             }\n",
      "1291             break;\n",
      "1292         case 3:\n",
      "1293             {\n",
      "1294                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1295                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1296                 dim3 n_threads(tx, ty, 1);\n",
      "1297                 if (verbose)\n",
      "1298                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1299                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1300                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1301                            cudaGetLastError(), self->nd,\n",
      "1302                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1303                 k3<<<n_blocks, n_threads>>>(\n",
      "1304                         dims[0], //dimensions\n",
      "1305                         dims[1],\n",
      "1306                         dims[2],\n",
      "1307                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1308                         CudaNdarray_DEV_DATA(out),\n",
      "1309                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1310                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1311                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1312                         CudaNdarray_DEV_DATA(self),\n",
      "1313                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1314                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1315                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1316                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1317                         err_var);\n",
      "1318             }\n",
      "1319             break;\n",
      "1320     default:\n",
      "1321         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1322                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1323                         \" dimensions are currently supported\");\n",
      "1324 \n",
      "1325     }\n",
      "1326     free(dims);\n",
      "1327     CNDA_THREAD_SYNC;\n",
      "1328     cudaError_t err = cudaGetLastError();\n",
      "1329     if (cudaSuccess != err) {\n",
      "1330         PyErr_Format(PyExc_RuntimeError,\n",
      "1331                      \"Cuda error: %s: %s.\\n\",\n",
      "1332                      \"CudaNdarray_TakeFrom\",\n",
      "1333                      cudaGetErrorString(err));\n",
      "1334         Py_DECREF(indices);\n",
      "1335         Py_DECREF(out);\n",
      "1336         return NULL;\n",
      "1337     }\n",
      "1338 \n",
      "1339     int index_err = check_err_var();\n",
      "1340     Py_DECREF(indices);\n",
      "1341     if (index_err != 0) {\n",
      "1342         Py_DECREF(out);\n",
      "1343         return NULL;\n",
      "1344     }\n",
      "1345 \n",
      "1346     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1347     return (PyObject *)out;\n",
      "1348 }\n",
      "1349 \n",
      "1350 \n",
      "1351 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1352 {\n",
      "1353     int pos, stride;\n",
      "1354     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1355         return NULL;\n",
      "1356     if ((pos < 0) || (pos >= self->nd))\n",
      "1357     {\n",
      "1358         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1359         return NULL;\n",
      "1360     }\n",
      "1361     CudaNdarray_set_stride(self, pos, stride);\n",
      "1362     if (cnda_copy_structure_to_device(self))\n",
      "1363     {\n",
      "1364         return NULL;\n",
      "1365     }\n",
      "1366     Py_INCREF(Py_None);\n",
      "1367     return Py_None;\n",
      "1368 }\n",
      "1369 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1370 {\n",
      "1371     int pos, dim;\n",
      "1372     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1373         return NULL;\n",
      "1374     if ((pos < 0) || (pos >= self->nd))\n",
      "1375     {\n",
      "1376         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1377         return NULL;\n",
      "1378     }\n",
      "1379     CudaNdarray_set_dim(self, pos, dim);\n",
      "1380     if (cnda_copy_structure_to_device(self))\n",
      "1381     {\n",
      "1382         return NULL;\n",
      "1383     }\n",
      "1384     Py_INCREF(Py_None);\n",
      "1385     return Py_None;\n",
      "1386 }\n",
      "1387 \n",
      "1388 static PyObject *\n",
      "1389 CudaNdarray_exp(CudaNdarray* self)\n",
      "1390 {\n",
      "1391     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1392     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1393     {\n",
      "1394         Py_XDECREF(rval);\n",
      "1395         return NULL;\n",
      "1396     }\n",
      "1397     unsigned int size = 1;\n",
      "1398     for (int i = 0; i < self->nd; i++)\n",
      "1399     {\n",
      "1400         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1401     }\n",
      "1402     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1403     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1404     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1405             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1406             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1407 \n",
      "1408     //TODO: don't do this right away, do it when we need the result\n",
      "1409     CNDA_THREAD_SYNC;\n",
      "1410     cudaError_t err = cudaGetLastError();\n",
      "1411     if( cudaSuccess != err)\n",
      "1412     {\n",
      "1413         Py_DECREF(rval);\n",
      "1414         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1415         return NULL;\n",
      "1416     }\n",
      "1417 \n",
      "1418     return (PyObject*)rval;\n",
      "1419 }\n",
      "1420 \n",
      "1421 static PyMethodDef CudaNdarray_methods[] =\n",
      "1422 {\n",
      "1423     {\"__array__\",\n",
      "1424         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1425         \"Copy from the device to a numpy ndarray\"},\n",
      "1426     {\"__copy__\",\n",
      "1427         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1428         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1429     {\"__deepcopy__\",\n",
      "1430         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1431         \"Create a copy of this object\"},\n",
      "1432     {\"zeros\",\n",
      "1433         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1434         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1435     {\"copy\",\n",
      "1436         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1437         \"Create a copy of this object\"},\n",
      "1438     {\"is_c_contiguous\",\n",
      "1439         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1440         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1441     {\"reduce_sum\",\n",
      "1442         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1443         \"Reduce over the given dimensions by summation\"},\n",
      "1444     {\"exp\",\n",
      "1445         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1446         \"Return the exponential of all elements\"},\n",
      "1447     {\"reshape\",\n",
      "1448         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1449         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1450             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1451     {\"view\",\n",
      "1452         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1453         \"Return an alias of this ndarray\"},\n",
      "1454     {\"_set_stride\",\n",
      "1455         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1456         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1457     {\"take\",\n",
      "1458         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1459         \"Equivalent of numpy.take\"},\n",
      "1460     {\"_set_shape_i\",\n",
      "1461         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1462         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1463     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1464 };\n",
      "1465 \n",
      "1466 \n",
      "1467 ////////////////////\n",
      "1468 // Number protocol\n",
      "1469 ////////////////////\n",
      "1470 \n",
      "1471 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1472     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1473     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1474 \n",
      "1475     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1476         dest[i] = a[i] + b[i];\n",
      "1477     }\n",
      "1478 }\n",
      "1479 \n",
      "1480 // Will be called by __add__ in Python\n",
      "1481 static PyObject *\n",
      "1482 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1483 {\n",
      "1484     if (! CudaNdarray_Check(py_self)) {\n",
      "1485         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1486         return NULL;\n",
      "1487     }\n",
      "1488     if (! CudaNdarray_Check(py_other)) {\n",
      "1489         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1490         return NULL;\n",
      "1491     }\n",
      "1492     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1493     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1494     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1495         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1496         return NULL;\n",
      "1497     }\n",
      "1498 \n",
      "1499     //standard elemwise size checks\n",
      "1500     if (self->nd != other->nd)\n",
      "1501     {\n",
      "1502         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1503         return NULL;\n",
      "1504     }\n",
      "1505     //standard elemwise dim checks\n",
      "1506     unsigned int size = 1;\n",
      "1507     for (int i = 0; i< self->nd; ++i)\n",
      "1508     {\n",
      "1509         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1510         {\n",
      "1511             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1512             return NULL;\n",
      "1513         }\n",
      "1514         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1515     }\n",
      "1516     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1517     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1518     {\n",
      "1519         Py_XDECREF(rval);\n",
      "1520         return NULL;\n",
      "1521     }\n",
      "1522 \n",
      "1523     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1524       return (PyObject *) rval;\n",
      "1525     }\n",
      "1526 \n",
      "1527     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1528     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1529     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1530             self->devdata, other->devdata, rval->devdata, size);\n",
      "1531     CNDA_THREAD_SYNC;\n",
      "1532     cudaError_t err = cudaGetLastError();\n",
      "1533     if( cudaSuccess != err)\n",
      "1534     {\n",
      "1535         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1536         Py_DECREF(rval);\n",
      "1537         return NULL;\n",
      "1538     }\n",
      "1539     return (PyObject *) rval;\n",
      "1540 }\n",
      "1541 \n",
      "1542 template <int operator_num>\n",
      "1543 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1544         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1545         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1546     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1547         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1548             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1549                 switch (operator_num)\n",
      "1550                 {\n",
      "1551                   case IADD:\n",
      "1552                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1553                     break;\n",
      "1554                   case IDIV:\n",
      "1555                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1556                     break;\n",
      "1557                   case CPY:\n",
      "1558                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1559                     break;\n",
      "1560                 }\n",
      "1561             }\n",
      "1562         }\n",
      "1563     }\n",
      "1564 }\n",
      "1565 \n",
      "1566 template <int operator_num>\n",
      "1567 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1568                          float* a, const int sA0, const int sA1,\n",
      "1569                          const int sA2, const int sA3,\n",
      "1570                          const float* b, const int sB0, const int sB1,\n",
      "1571                          const int sB2, const int sB3){\n",
      "1572     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1573         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1574             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1575                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1576                     switch (operator_num) {\n",
      "1577                         case IADD:\n",
      "1578                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1579                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1580                             break;\n",
      "1581                         case IDIV:\n",
      "1582                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1583                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1584                             break;\n",
      "1585                         case CPY:\n",
      "1586                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1587                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1588                             break;\n",
      "1589                     }\n",
      "1590                 }\n",
      "1591             }\n",
      "1592         }\n",
      "1593     }\n",
      "1594 }\n",
      "1595 \n",
      "1596 template <int operator_num>\n",
      "1597 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1598                           const int d2, const int d3,\n",
      "1599                           const int d4, const int d5,\n",
      "1600                           float* a, const int sA0, const int sA1,\n",
      "1601                           const int sA2, const int sA3,\n",
      "1602                           const int sA4, const int sA5,\n",
      "1603                           const float* b, const int sB0, const int sB1,\n",
      "1604                           const int sB2, const int sB3,\n",
      "1605                           const int sB4, const int sB5\n",
      "1606                           ){\n",
      "1607     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1608         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1609             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1610                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1611                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1612                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1613                             switch (operator_num) {\n",
      "1614                             case IADD:\n",
      "1615                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1616                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1617                                 break;\n",
      "1618                             case IDIV:\n",
      "1619                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1620                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1621                                 break;\n",
      "1622                             case CPY:\n",
      "1623                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1624                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1625                                 break;\n",
      "1626                             }\n",
      "1627                         }\n",
      "1628                     }\n",
      "1629                 }\n",
      "1630             }\n",
      "1631         }\n",
      "1632     }\n",
      "1633 }\n",
      "1634 \n",
      "1635 /*\n",
      "1636 CudaNdarray_inplace_elemwise\n",
      "1637 Compute elemwise, working inplace on A.\n",
      "1638 Currently implemented A / B, A + B and A = B\n",
      "1639 (the last is not tested and not used!)\n",
      "1640 \n",
      "1641 py_self - the CudaNdarray that we'll modify (A)\n",
      "1642 py_other - the other argument (B)\n",
      "1643 fct_nb - which operation to perform (operator_t)\n",
      "1644 \n",
      "1645 Returns 0 on success.\n",
      "1646 Returns -1 on failure, and sets Python exception.\n",
      "1647 \n",
      "1648 */\n",
      "1649 int\n",
      "1650 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1651 {\n",
      "1652     int verbose = 0;\n",
      "1653     void (*k3)(const int, const int, const int,\n",
      "1654                     float*, const int, const int, const int,\n",
      "1655                     const float*, const int, const int, const int);\n",
      "1656     void (*k4)(const int, const int, const int, const int,\n",
      "1657                     float*, const int, const int,\n",
      "1658                     const int, const int,\n",
      "1659                     const float*, const int, const int,\n",
      "1660                     const int, const int);\n",
      "1661     void (*k6)(const int, const int,\n",
      "1662                const int, const int,\n",
      "1663                const int, const int,\n",
      "1664                float*, const int, const int,\n",
      "1665                const int, const int,\n",
      "1666                const int, const int,\n",
      "1667                const float*, const int, const int,\n",
      "1668                const int, const int,\n",
      "1669                const int, const int);\n",
      "1670     switch (fct_nb)\n",
      "1671     {\n",
      "1672         case IADD:\n",
      "1673             k3 = k_ielem_3<IADD>;\n",
      "1674             k4 = k_ielem_4<IADD>;\n",
      "1675             k6 = k_ielem_6<IADD>;\n",
      "1676             break;\n",
      "1677         case IDIV:\n",
      "1678             k3 = k_ielem_3<IDIV>;\n",
      "1679             k4 = k_ielem_4<IDIV>;\n",
      "1680             k6 = k_ielem_6<IDIV>;\n",
      "1681             break;\n",
      "1682         case CPY:\n",
      "1683             k3 = k_ielem_3<CPY>;\n",
      "1684             k4 = k_ielem_4<CPY>;\n",
      "1685             k6 = k_ielem_6<CPY>;\n",
      "1686             break;\n",
      "1687         default:\n",
      "1688             assert (0);\n",
      "1689             PyErr_Format(\n",
      "1690                 PyExc_TypeError,\n",
      "1691                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1692                 (int)fct_nb);\n",
      "1693             return -1;\n",
      "1694     }\n",
      "1695     if (!CudaNdarray_Check(py_self)) {\n",
      "1696         PyErr_SetString(\n",
      "1697             PyExc_TypeError,\n",
      "1698             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1699         return -1;\n",
      "1700     }\n",
      "1701     CudaNdarray * new_other = NULL;\n",
      "1702     if (!CudaNdarray_Check(py_other)) {\n",
      "1703         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1704         if(!new_other)\n",
      "1705         {\n",
      "1706             return -1;\n",
      "1707         }\n",
      "1708         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1709         {\n",
      "1710             Py_XDECREF(new_other);\n",
      "1711             return -1;\n",
      "1712         }\n",
      "1713         py_other = (PyObject *) new_other;\n",
      "1714     }\n",
      "1715 \n",
      "1716     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1717     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1718 \n",
      "1719     if (verbose)\n",
      "1720     {\n",
      "1721         fprintf(stderr,\n",
      "1722             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1723             self->nd, other->nd);\n",
      "1724     }\n",
      "1725 \n",
      "1726     //standard elemwise nb dim checks\n",
      "1727     if (self->nd < other->nd)\n",
      "1728     {\n",
      "1729         PyErr_Format(\n",
      "1730             PyExc_TypeError,\n",
      "1731             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1732             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1733             self->nd, other->nd);\n",
      "1734         Py_XDECREF(new_other);\n",
      "1735         return -1;\n",
      "1736     }\n",
      "1737 \n",
      "1738     //broadcast to the same number of dimensions.\n",
      "1739     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1740     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1741     int added_dims = self->nd - other->nd;\n",
      "1742     // Add the added broadcasted dimensions\n",
      "1743     for (int i = 0; i< added_dims; ++i)\n",
      "1744     {\n",
      "1745         other_dims[i] = 1;\n",
      "1746         other_strides[i] = 0;\n",
      "1747     }\n",
      "1748     // Copy the existing dimensions\n",
      "1749     for (int i = 0; i< other->nd; ++i)\n",
      "1750     {\n",
      "1751         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1752         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1753     }\n",
      "1754 \n",
      "1755     //standard elemwise dim checks\n",
      "1756     unsigned int size = 1;\n",
      "1757     for (int i = 0; i< self->nd; ++i)\n",
      "1758     {\n",
      "1759         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1760             && (other_dims[i] != 1))\n",
      "1761         {\n",
      "1762             PyErr_SetString(\n",
      "1763                 PyExc_ValueError,\n",
      "1764                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1765             Py_XDECREF(new_other);\n",
      "1766             return -1;\n",
      "1767         }\n",
      "1768         // if we're broadcasting other, then make sure it has stride 0\n",
      "1769         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1770             || (other_strides[i] == 0));\n",
      "1771         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1772     }\n",
      "1773 \n",
      "1774     if (size==0)\n",
      "1775     {\n",
      "1776         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1777         if (!(other_size == 0 || other_size == 1))\n",
      "1778         {\n",
      "1779             PyErr_SetString(\n",
      "1780                 PyExc_ValueError,\n",
      "1781                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1782                 \" un-initialized array when the new value have more than\"\n",
      "1783                 \" 0 or 1 broadcastable dimensions\");\n",
      "1784             Py_XDECREF(new_other);\n",
      "1785             return 0;\n",
      "1786         }\n",
      "1787         Py_XDECREF(new_other);\n",
      "1788         return 0;\n",
      "1789     }\n",
      "1790 \n",
      "1791     switch(self->nd)\n",
      "1792     {\n",
      "1793         case 0:\n",
      "1794             {\n",
      "1795                 dim3 n_blocks(1, 1, 1);\n",
      "1796                 dim3 n_threads(1);\n",
      "1797                 k3<<<n_blocks, n_threads>>>(\n",
      "1798                         1, //d0\n",
      "1799                         1, //d1\n",
      "1800                         1, //d2\n",
      "1801                         CudaNdarray_DEV_DATA(self),\n",
      "1802                         1, //strides\n",
      "1803                         1,\n",
      "1804                         1,\n",
      "1805                         CudaNdarray_DEV_DATA(other),\n",
      "1806                         1, //strides\n",
      "1807                         1,\n",
      "1808                         1);\n",
      "1809                 CNDA_THREAD_SYNC;\n",
      "1810                 cudaError_t err = cudaGetLastError();\n",
      "1811                 if (cudaSuccess != err)\n",
      "1812                 {\n",
      "1813                     PyErr_Format(\n",
      "1814                         PyExc_RuntimeError,\n",
      "1815                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1816                         \"k3\",\n",
      "1817                         cudaGetErrorString(err));\n",
      "1818                     Py_XDECREF(new_other);\n",
      "1819                     return -1;\n",
      "1820                 }\n",
      "1821             }\n",
      "1822             break;\n",
      "1823         case 1:\n",
      "1824             {\n",
      "1825                 dim3 n_blocks(1, 1, 1);\n",
      "1826                 dim3 n_threads(\n",
      "1827                         std::min(\n",
      "1828                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1829                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1830                 k3<<<n_blocks, n_threads>>>(\n",
      "1831                         1, //dimensions\n",
      "1832                         1,\n",
      "1833                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1834                         CudaNdarray_DEV_DATA(self),\n",
      "1835                         1, //strides\n",
      "1836                         1,\n",
      "1837                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1838                         CudaNdarray_DEV_DATA(other),\n",
      "1839                         1, //strides\n",
      "1840                         1,\n",
      "1841                         other_strides[0]);\n",
      "1842                 CNDA_THREAD_SYNC;\n",
      "1843                 cudaError_t err = cudaGetLastError();\n",
      "1844                 if (cudaSuccess != err)\n",
      "1845                 {\n",
      "1846                     PyErr_Format(\n",
      "1847                         PyExc_RuntimeError,\n",
      "1848                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1849                         \"k3\",\n",
      "1850                         cudaGetErrorString(err));\n",
      "1851                     Py_XDECREF(new_other);\n",
      "1852                     return -1;\n",
      "1853                 }\n",
      "1854             }\n",
      "1855             break;\n",
      "1856         case 2:\n",
      "1857             {\n",
      "1858                 //TODO:  if both self and other are f-contiguous\n",
      "1859                 //       Then flip the block and thread dimensions\n",
      "1860                 //       to make contiguous reads & writes\n",
      "1861                 dim3 n_blocks(1,\n",
      "1862                         std::min(\n",
      "1863                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1864                             NUM_VECTOR_OP_BLOCKS));\n",
      "1865                 dim3 n_threads(\n",
      "1866                         std::min(\n",
      "1867                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1868                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1869                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1870                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1871                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1872                         CudaNdarray_DEV_DATA(self),\n",
      "1873                         1,\n",
      "1874                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1875                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1876                         CudaNdarray_DEV_DATA(other),\n",
      "1877                         1,\n",
      "1878                         other_strides[0],\n",
      "1879                         other_strides[1]);\n",
      "1880                 CNDA_THREAD_SYNC;\n",
      "1881                 cudaError_t err = cudaGetLastError();\n",
      "1882                 if (cudaSuccess != err)\n",
      "1883                 {\n",
      "1884                     PyErr_Format(\n",
      "1885                         PyExc_RuntimeError,\n",
      "1886                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1887                         \"k3\",\n",
      "1888                         cudaGetErrorString(err));\n",
      "1889                     Py_XDECREF(new_other);\n",
      "1890                     return -1;\n",
      "1891                 }\n",
      "1892             }\n",
      "1893             break;\n",
      "1894         case 3:\n",
      "1895             {\n",
      "1896                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1897                 //       has a contiguous dimension on the thread idx.\n",
      "1898                 dim3 n_blocks(\n",
      "1899                         std::min(\n",
      "1900                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1901                             NUM_VECTOR_OP_BLOCKS),\n",
      "1902                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1903                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1904                     n_blocks.y /= 2;\n",
      "1905                 dim3 n_threads(\n",
      "1906                         std::min(\n",
      "1907                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1908                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1909                 k3<<<n_blocks, n_threads>>>(\n",
      "1910                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1911                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1912                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1913                         CudaNdarray_DEV_DATA(self),\n",
      "1914                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1915                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1916                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1917                         CudaNdarray_DEV_DATA(other),\n",
      "1918                         other_strides[0],\n",
      "1919                         other_strides[1],\n",
      "1920                         other_strides[2]);\n",
      "1921                 CNDA_THREAD_SYNC;\n",
      "1922                 cudaError_t err = cudaGetLastError();\n",
      "1923                 if (cudaSuccess != err)\n",
      "1924                 {\n",
      "1925                     PyErr_Format(\n",
      "1926                         PyExc_RuntimeError,\n",
      "1927                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1928                         \"k3\",\n",
      "1929                         cudaGetErrorString(err));\n",
      "1930                     Py_XDECREF(new_other);\n",
      "1931                     return -1;\n",
      "1932                 }\n",
      "1933             }\n",
      "1934             break;\n",
      "1935         case 4:\n",
      "1936             {\n",
      "1937                 dim3 n_blocks(\n",
      "1938                         std::min(\n",
      "1939                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1940                             NUM_VECTOR_OP_BLOCKS),\n",
      "1941                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1942                         );\n",
      "1943                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1944                     n_blocks.y /= 2;\n",
      "1945                 dim3 n_threads(\n",
      "1946                         std::min(\n",
      "1947                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1948                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1949                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1950                             );\n",
      "1951                 k4<<<n_blocks, n_threads>>>(\n",
      "1952                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1953                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1954                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1955                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1956                         CudaNdarray_DEV_DATA(self),\n",
      "1957                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1958                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1959                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1960                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1961                         CudaNdarray_DEV_DATA(other),\n",
      "1962                         other_strides[0],\n",
      "1963                         other_strides[1],\n",
      "1964                         other_strides[2],\n",
      "1965                         other_strides[3]);\n",
      "1966                 CNDA_THREAD_SYNC;\n",
      "1967                 cudaError_t err = cudaGetLastError();\n",
      "1968                 if (cudaSuccess != err)\n",
      "1969                 {\n",
      "1970                     PyErr_Format(\n",
      "1971                         PyExc_RuntimeError,\n",
      "1972                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1973                         \"k4\",\n",
      "1974                         cudaGetErrorString(err));\n",
      "1975                     Py_XDECREF(new_other);\n",
      "1976                     return -1;\n",
      "1977                 }\n",
      "1978             }\n",
      "1979             break;\n",
      "1980         case 5:\n",
      "1981             {\n",
      "1982                 dim3 n_blocks(\n",
      "1983                         std::min(\n",
      "1984                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1985                             NUM_VECTOR_OP_BLOCKS),\n",
      "1986                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1987                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1988                     n_blocks.y /= 2;\n",
      "1989                 dim3 n_threads(\n",
      "1990                         std::min(\n",
      "1991                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1992                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1993                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1994                     );\n",
      "1995                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1996                 {\n",
      "1997                      k4<<<n_blocks, n_threads>>>(\n",
      "1998                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1999                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "2000                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2001                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "2002                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2003                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2004                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2005                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2006                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2007                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "2008                             other_strides[1],\n",
      "2009                             other_strides[2],\n",
      "2010                             other_strides[3],\n",
      "2011                             other_strides[4]);\n",
      "2012                     CNDA_THREAD_SYNC;\n",
      "2013                     cudaError_t err = cudaGetLastError();\n",
      "2014                     if( cudaSuccess != err)\n",
      "2015                     {\n",
      "2016                         PyErr_Format(\n",
      "2017                             PyExc_RuntimeError,\n",
      "2018                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2019                             \"k5 with loop over k4\",\n",
      "2020                             cudaGetErrorString(err),\n",
      "2021                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2022                         Py_XDECREF(new_other);\n",
      "2023                         return -1;\n",
      "2024                     }\n",
      "2025                 }\n",
      "2026             }\n",
      "2027             break;\n",
      "2028         case 6:\n",
      "2029             {\n",
      "2030                 dim3 n_blocks(\n",
      "2031                         std::min(\n",
      "2032                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2033                             NUM_VECTOR_OP_BLOCKS),\n",
      "2034                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2035                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2036                         );\n",
      "2037                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2038                     n_blocks.y /= 2;\n",
      "2039                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2040                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2041                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2042                 //    n_blocks.z /= 2;\n",
      "2043                 n_blocks.z = 1;\n",
      "2044                 dim3 n_threads(\n",
      "2045                         std::min(\n",
      "2046                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2047                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2048                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2049                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2050                             );\n",
      "2051                 k6<<<n_blocks, n_threads>>>(\n",
      "2052                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2053                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2054                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2055                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2056                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2057                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2058                         CudaNdarray_DEV_DATA(self),\n",
      "2059                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2060                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2061                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2062                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2063                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2064                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2065                         CudaNdarray_DEV_DATA(other),\n",
      "2066                         other_strides[0],\n",
      "2067                         other_strides[1],\n",
      "2068                         other_strides[2],\n",
      "2069                         other_strides[3],\n",
      "2070                         other_strides[4],\n",
      "2071                         other_strides[5]);\n",
      "2072                 CNDA_THREAD_SYNC;\n",
      "2073                 cudaError_t err = cudaGetLastError();\n",
      "2074                 if (cudaSuccess != err)\n",
      "2075                 {\n",
      "2076                     PyErr_Format(\n",
      "2077                         PyExc_RuntimeError,\n",
      "2078                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2079                         \"k6\",\n",
      "2080                         cudaGetErrorString(err),\n",
      "2081                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2082                         (long) n_threads.x);\n",
      "2083                     Py_XDECREF(new_other);\n",
      "2084                     return -1;\n",
      "2085                 }\n",
      "2086             }\n",
      "2087             break;\n",
      "2088         default:\n",
      "2089         {\n",
      "2090             PyErr_Format(\n",
      "2091                 PyExc_NotImplementedError,\n",
      "2092                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2093                 self->nd);\n",
      "2094             Py_XDECREF(new_other);\n",
      "2095             return -1;\n",
      "2096         }\n",
      "2097     }\n",
      "2098     if (verbose)\n",
      "2099         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2100     Py_XDECREF(new_other);\n",
      "2101     return 0;\n",
      "2102 }\n",
      "2103 \n",
      "2104 /*\n",
      "2105  * We need this inplace Add to support IncSubTensor\n",
      "2106  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2107  */\n",
      "2108 // Will be called by __iadd__ in Python\n",
      "2109 PyObject *\n",
      "2110 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2111 {\n",
      "2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2113     {\n",
      "2114         return NULL;\n",
      "2115     }\n",
      "2116     Py_INCREF(py_self);\n",
      "2117     return py_self;\n",
      "2118 }\n",
      "2119 \n",
      "2120 /*\n",
      "2121  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2122  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2123  */\n",
      "2124 // Will be called by __idiv__ in Python\n",
      "2125 static PyObject *\n",
      "2126 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2127 {\n",
      "2128     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2129     {\n",
      "2130         return NULL;\n",
      "2131     }\n",
      "2132     Py_INCREF(py_self);\n",
      "2133     return py_self;\n",
      "2134 }\n",
      "2135 \n",
      "2136 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2137 #if PY_MAJOR_VERSION == 3\n",
      "2138 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2139 {\n",
      "2140     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2141     0,  //binaryfunc nb_subtract;\n",
      "2142     0,  //binaryfunc nb_multiply;\n",
      "2143     0,  //binaryfunc nb_remainder;\n",
      "2144     0,  //binaryfunc nb_divmod;\n",
      "2145     0,  //ternaryfunc nb_power;\n",
      "2146     0,  //unaryfunc nb_negative;\n",
      "2147     0,  //unaryfunc nb_positive;\n",
      "2148     0,  //unaryfunc nb_absolute;\n",
      "2149     0,  //inquiry nb_bool;\n",
      "2150     0,  //unaryfunc nb_invert;\n",
      "2151     0,  //binaryfunc nb_lshift;\n",
      "2152     0,  //binaryfunc nb_rshift;\n",
      "2153     0,  //binaryfunc nb_and;\n",
      "2154     0,  //binaryfunc nb_xor;\n",
      "2155     0,  //binaryfunc nb_or;\n",
      "2156     0,  //unaryfunc nb_int;\n",
      "2157     0,  //void *nb_reserved;\n",
      "2158     0,  //unaryfunc nb_float;\n",
      "2159 \n",
      "2160     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2161     0,  //binaryfunc nb_inplace_subtract;\n",
      "2162     0,  //binaryfunc nb_inplace_multiply;\n",
      "2163     0,  //binaryfunc nb_inplace_remainder;\n",
      "2164     0,  //ternaryfunc nb_inplace_power;\n",
      "2165     0,  //binaryfunc nb_inplace_lshift;\n",
      "2166     0,  //binaryfunc nb_inplace_rshift;\n",
      "2167     0,  //binaryfunc nb_inplace_and;\n",
      "2168     0,  //binaryfunc nb_inplace_xor;\n",
      "2169     0,  //binaryfunc nb_inplace_or;\n",
      "2170 \n",
      "2171     0,  //binaryfunc nb_floor_divide;\n",
      "2172     0,  //binaryfunc nb_true_divide;\n",
      "2173     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2174     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2175 \n",
      "2176     0,  //unaryfunc nb_index\n",
      "2177 };\n",
      "2178 #else\n",
      "2179 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2180 {\n",
      "2181     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2182     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2183     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2184     0,  //binaryfunc nb_divide;        __div__\n",
      "2185     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2186     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2187     0,  //ternaryfunc nb_power;        __pow__\n",
      "2188     0,  //unaryfunc nb_negative;       __neg__\n",
      "2189     0,  //unaryfunc nb_positive;       __pos__\n",
      "2190     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2191     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2192     0,  //unaryfunc nb_invert;         __invert__\n",
      "2193     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2194     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2195     0,  //binaryfunc nb_and;           __and__\n",
      "2196     0,  //binaryfunc nb_xor;           __xor__\n",
      "2197     0,  //binaryfunc nb_or;            __or__\n",
      "2198     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2199     0,  //unaryfunc nb_int;            __int__\n",
      "2200     0,  //unaryfunc nb_long;           __long__\n",
      "2201     0,  //unaryfunc nb_float;          __float__\n",
      "2202     0,  //unaryfunc nb_oct;            __oct__\n",
      "2203     0,  //unaryfunc nb_hex;            __hex__\n",
      "2204 \n",
      "2205     /* Added in release 2.0 */\n",
      "2206     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2207     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2208     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2209     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2210     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2211     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2212     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2213     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2214     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2215     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2216     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2217 \n",
      "2218     /* Added in release 2.2 */\n",
      "2219     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2220     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2221     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2222     0,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2223 \n",
      "2224 #if PY_MINOR_VERSION > 4\n",
      "2225     /* Added in release 2.5 */\n",
      "2226     0  //unaryfunc nb_index;  __index__\n",
      "2227 #endif\n",
      "2228 };\n",
      "2229 #endif\n",
      "2230 \n",
      "2231 \n",
      "2232 /////////////////////\n",
      "2233 // Mapping protocol\n",
      "2234 /////////////////////\n",
      "2235 \n",
      "2236 // Will by called by __len__ in Python\n",
      "2237 static Py_ssize_t\n",
      "2238 CudaNdarray_len(PyObject * py_self)\n",
      "2239 {\n",
      "2240     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2241     if (self->nd <= 0)\n",
      "2242     {\n",
      "2243         return (Py_ssize_t) 0;\n",
      "2244     }\n",
      "2245     else\n",
      "2246     {\n",
      "2247         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2248     }\n",
      "2249 }\n",
      "2250 \n",
      "2251 // Will by called by __getitem__ in Python\n",
      "2252 PyObject *\n",
      "2253 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2254 {\n",
      "2255     int verbose = 0;\n",
      "2256     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2257     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2258     PyObject * py_rval = NULL;\n",
      "2259     CudaNdarray * rval = NULL;\n",
      "2260     PyObject * intobj = NULL;\n",
      "2261 \n",
      "2262     //PyObject_Print(key, stderr, 0);\n",
      "2263 \n",
      "2264     if (key == Py_Ellipsis)\n",
      "2265     {\n",
      "2266         Py_INCREF(py_self);\n",
      "2267         return py_self;\n",
      "2268     }\n",
      "2269     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2270     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2271     {\n",
      "2272         int d_idx = PyInt_AsLong(intobj);\n",
      "2273         Py_DECREF(intobj); intobj=NULL;\n",
      "2274         //int d_idx = PyInt_AsLong(key);\n",
      "2275         if (self->nd == 0)\n",
      "2276         {\n",
      "2277             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2278             return NULL;\n",
      "2279         }\n",
      "2280         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2281         int offset = 0;\n",
      "2282 \n",
      "2283         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2284         {\n",
      "2285             //normal indexing\n",
      "2286             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2287         }\n",
      "2288         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2289         {\n",
      "2290             //end-based indexing\n",
      "2291             // d_idx is negative\n",
      "2292             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2293         }\n",
      "2294         else\n",
      "2295         {\n",
      "2296             PyErr_Format(PyExc_IndexError,\n",
      "2297                          \"index out of bounds. Asked %d, but size of %d\",\n",
      "2298                          d_idx, d_dim);\n",
      "2299             return NULL;\n",
      "2300         }\n",
      "2301 \n",
      "2302         //allocate our subtensor view\n",
      "2303         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2304         rval = (CudaNdarray*) py_rval;\n",
      "2305         if (!rval) return NULL;\n",
      "2306         assert (0 == rval->data_allocated);\n",
      "2307 \n",
      "2308         //initialize the view's data pointer to our own.\n",
      "2309         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2310         {\n",
      "2311             Py_DECREF(rval);\n",
      "2312             return NULL;\n",
      "2313         }\n",
      "2314         for (int d = 1; d < self->nd; ++d)\n",
      "2315         {\n",
      "2316             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2317             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2318         }\n",
      "2319     }\n",
      "2320     else\n",
      "2321     {\n",
      "2322         PyErr_Clear();\n",
      "2323     }\n",
      "2324     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2325     {\n",
      "2326         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2327         if (self->nd == 0)\n",
      "2328         {\n",
      "2329             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2330             return NULL;\n",
      "2331         }\n",
      "2332 \n",
      "2333         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2334         Py_ssize_t start, stop, step, slen;\n",
      "2335         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2336         {\n",
      "2337             if (verbose)\n",
      "2338                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2339             return NULL;\n",
      "2340         }\n",
      "2341         if (verbose)\n",
      "2342         {\n",
      "2343             std::cerr << \"start \" << start << \"\\n\";\n",
      "2344             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2345             std::cerr << \"step \" << step << \"\\n\";\n",
      "2346             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2347         }\n",
      "2348 \n",
      "2349         //allocate our subtensor view\n",
      "2350         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2351         rval = (CudaNdarray*) py_rval;\n",
      "2352         if (!rval) return NULL;\n",
      "2353         assert (0 == rval->data_allocated);\n",
      "2354 \n",
      "2355 \n",
      "2356         //initialize the view's data pointer to our own.\n",
      "2357         if (CudaNdarray_set_device_data(rval,\n",
      "2358                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2359                     self))\n",
      "2360         {\n",
      "2361             Py_DECREF(rval);\n",
      "2362             return NULL;\n",
      "2363         }\n",
      "2364         //initialize dimension 0 of rval\n",
      "2365         CudaNdarray_set_stride(rval, 0,\n",
      "2366                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2367         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2368         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2369         // initialize dimensions > 0 of rval\n",
      "2370         for (int d = 1; d < self->nd; ++d)\n",
      "2371         {\n",
      "2372             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2373             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2374         }\n",
      "2375     }\n",
      "2376     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2377     {\n",
      "2378         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2379         //elements of the tuple can be either integers or slices\n",
      "2380         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2381 \n",
      "2382         if (PyTuple_Size(key) > self->nd)\n",
      "2383         {\n",
      "2384             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2385             return NULL;\n",
      "2386         }\n",
      "2387 \n",
      "2388         //calculate the number of dimensions in the return value\n",
      "2389         int rval_nd = self->nd;\n",
      "2390         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2391         {\n",
      "2392             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2393             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2394             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2395         }\n",
      "2396 \n",
      "2397         //allocate our subtensor view\n",
      "2398         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2399         rval = (CudaNdarray*) py_rval;\n",
      "2400         if (!rval) return NULL;\n",
      "2401         assert (0 == rval->data_allocated);\n",
      "2402 \n",
      "2403         //initialize the view's data pointer to our own.\n",
      "2404         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2405         {\n",
      "2406             Py_DECREF(rval);\n",
      "2407             return NULL;\n",
      "2408         }\n",
      "2409 \n",
      "2410         // rval_d will refer to the current dimension in the rval.\n",
      "2411         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2412         // keys\n",
      "2413         int rval_d = 0;\n",
      "2414 \n",
      "2415         for (int d = 0; d < self->nd; ++d)\n",
      "2416         {\n",
      "2417             // keys can be shorter than self->nd.\n",
      "2418             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2419             if (d >=PyTuple_Size(key))\n",
      "2420             {\n",
      "2421                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2422                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2423                 ++rval_d;\n",
      "2424             }\n",
      "2425             else\n",
      "2426             {\n",
      "2427                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2428 \n",
      "2429                 if (PySlice_Check(key_d))\n",
      "2430                 {\n",
      "2431                     Py_ssize_t start, stop, step, slen;\n",
      "2432                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2433                     {\n",
      "2434                         Py_DECREF(rval);\n",
      "2435                         return NULL;\n",
      "2436                     }\n",
      "2437                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2438                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2439                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2440                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2441                     if (0)\n",
      "2442                     {\n",
      "2443                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2444                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2445                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2446                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2447                     }\n",
      "2448                     ++rval_d;\n",
      "2449                 }\n",
      "2450                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2451                 {\n",
      "2452                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2453                     int d_idx = PyInt_AsLong(intobj);\n",
      "2454                     Py_DECREF(intobj);\n",
      "2455                     intobj = NULL;\n",
      "2456                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2457 \n",
      "2458                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2459                     {\n",
      "2460                         //normal indexing\n",
      "2461                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2462                     }\n",
      "2463                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2464                     {\n",
      "2465                         //end-based indexing\n",
      "2466                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2467                     }\n",
      "2468                     else\n",
      "2469                     {\n",
      "2470                         PyErr_Format(PyExc_IndexError,\n",
      "2471                                      \"index out of bounds. Asked %d for dimensions %d, but size of %d\",\n",
      "2472                                      d_idx, d, d_dim);\n",
      "2473                         Py_DECREF(rval);\n",
      "2474                         return NULL;\n",
      "2475                     }\n",
      "2476                 }\n",
      "2477                 else\n",
      "2478                 {\n",
      "2479                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2480                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2481                     Py_DECREF(rval);\n",
      "2482                     return NULL;\n",
      "2483                 }\n",
      "2484             }\n",
      "2485         }\n",
      "2486     }\n",
      "2487     if (py_rval)\n",
      "2488     {\n",
      "2489         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2490         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2491     }\n",
      "2492     else\n",
      "2493     {\n",
      "2494         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2495         return NULL;\n",
      "2496     }\n",
      "2497     return py_rval;\n",
      "2498 }\n",
      "2499 \n",
      "2500 // Will by called by __setitem__ in Python\n",
      "2501 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2502 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2503 // Can only be assigned from a CudaNdarray on the right side\n",
      "2504 // Or a ndarray\n",
      "2505 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2506 static int\n",
      "2507 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2508 {\n",
      "2509     int verbose = 0;\n",
      "2510     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2511     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2512     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2513     CudaNdarray* new_value = NULL;\n",
      "2514 \n",
      "2515     if(!rval){\n",
      "2516         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2517         Py_XDECREF(rval);\n",
      "2518         return -1;\n",
      "2519     }\n",
      "2520 \n",
      "2521     if(rval != (CudaNdarray*)o &&\n",
      "2522                 (rval->data_allocated ||\n",
      "2523                  // The new array should have a base\n",
      "2524                  !(((CudaNdarray*)rval)->base) ||\n",
      "2525                  // If the original array has no base, the base of the new\n",
      "2526                  // array should be the original one\n",
      "2527                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2528                  // Else, the two arrays should have the same base\n",
      "2529                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2530     {\n",
      "2531         // This case shouldn't happen, based on what I see in Subscript\n",
      "2532         // but just in case it happens sometime in the future\n",
      "2533 \n",
      "2534         PyErr_Format(PyExc_RuntimeError,\n",
      "2535                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2536                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2537                      \" o.base=%p o=%p\",\n",
      "2538                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2539         Py_DECREF(rval);\n",
      "2540         return -1;\n",
      "2541     }\n",
      "2542 \n",
      "2543     PyObject * intobj = NULL;\n",
      "2544     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2545         if (verbose)\n",
      "2546             fprintf(stderr,\n",
      "2547                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2548                     \" value is a ndarray\\n\");\n",
      "2549         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2550         if(!new_value)\n",
      "2551         {\n",
      "2552             return -1;\n",
      "2553         }\n",
      "2554         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2555         {\n",
      "2556             Py_XDECREF(new_value);\n",
      "2557             Py_XDECREF(rval);\n",
      "2558             return -1;\n",
      "2559         }\n",
      "2560         value = (PyObject *) new_value;\n",
      "2561     }\n",
      "2562     else if ((intobj=PyNumber_Int(value)))\n",
      "2563     {\n",
      "2564         if (verbose)\n",
      "2565             fprintf(stderr,\n",
      "2566                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2567         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2568             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2569                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2570                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2571             Py_XDECREF(rval);\n",
      "2572             return -1;\n",
      "2573         }\n",
      "2574 \n",
      "2575         long val = PyInt_AsLong(intobj);\n",
      "2576         Py_DECREF(intobj); intobj=NULL;\n",
      "2577         if (val == 0)\n",
      "2578         {\n",
      "2579             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2580                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2581             Py_XDECREF(rval);\n",
      "2582             if (err)\n",
      "2583             {\n",
      "2584                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2585                 // Currently this returns the same thing as err, but if in future\n",
      "2586                 // it returns something else I still don't see why we should ignore\n",
      "2587                 // it.  All we want to do here is reset the flag.\n",
      "2588                 cudaGetLastError();\n",
      "2589                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2590                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2591                 return -1;\n",
      "2592             }\n",
      "2593             return 0;\n",
      "2594         } else {\n",
      "2595             Py_XDECREF(rval);\n",
      "2596             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2597                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2598                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2599                 return -1;\n",
      "2600         }\n",
      "2601     }\n",
      "2602 \n",
      "2603     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2604 \n",
      "2605     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2606     {\n",
      "2607         PyErr_SetString(PyExc_TypeError,\n",
      "2608           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2609           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2610         Py_XDECREF(new_value);\n",
      "2611         return -1;\n",
      "2612     }\n",
      "2613 \n",
      "2614     if (verbose)\n",
      "2615         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2616 \n",
      "2617     if (cnda_copy_structure_to_device(rval))\n",
      "2618     {\n",
      "2619         PyErr_SetString(PyExc_RuntimeError,\n",
      "2620                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2621         Py_DECREF(rval);\n",
      "2622         Py_XDECREF(new_value);\n",
      "2623 \n",
      "2624         if (verbose)\n",
      "2625             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2626         return -1;\n",
      "2627     }\n",
      "2628 \n",
      "2629     PyObject *baseSavedForComparison = rval->base;\n",
      "2630 \n",
      "2631     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2632     {\n",
      "2633         Py_DECREF((PyObject*)rval);\n",
      "2634         Py_XDECREF(new_value);\n",
      "2635 \n",
      "2636         if (verbose)\n",
      "2637             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2638         return -1;\n",
      "2639     }\n",
      "2640 \n",
      "2641     assert (rval->base == baseSavedForComparison);\n",
      "2642     assert (rval->dev_structure_fresh);\n",
      "2643 \n",
      "2644     // Clean up locally-created references\n",
      "2645     Py_DECREF(rval);\n",
      "2646     Py_XDECREF(new_value);\n",
      "2647 \n",
      "2648     return 0;\n",
      "2649 }\n",
      "2650 \n",
      "2651 \n",
      "2652 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2653     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2654     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2655     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2656 };\n",
      "2657 \n",
      "2658 ////////////////////\n",
      "2659 //\n",
      "2660 ////////////////////\n",
      "2661 \n",
      "2662 static PyObject *\n",
      "2663 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2664 {\n",
      "2665     if (self->nd < 0)\n",
      "2666     {\n",
      "2667         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2668         return NULL;\n",
      "2669     }\n",
      "2670     PyObject * rval = PyTuple_New(self->nd);\n",
      "2671     for (int i = 0; i < self->nd; ++i)\n",
      "2672     {\n",
      "2673         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2674         {\n",
      "2675             Py_XDECREF(rval);\n",
      "2676             return NULL;\n",
      "2677         }\n",
      "2678 \n",
      "2679     }\n",
      "2680     return rval;\n",
      "2681 }\n",
      "2682 \n",
      "2683 static int\n",
      "2684 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2685 {\n",
      "2686     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2687     return -1;\n",
      "2688 }\n",
      "2689 \n",
      "2690 static PyObject *\n",
      "2691 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2692 {\n",
      "2693     if (self->nd < 0)\n",
      "2694     {\n",
      "2695         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2696         return NULL;\n",
      "2697     }\n",
      "2698     PyObject * rval = PyTuple_New(self->nd);\n",
      "2699     for (int i = 0; i < self->nd; ++i)\n",
      "2700     {\n",
      "2701         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2702         {\n",
      "2703             Py_XDECREF(rval);\n",
      "2704             return NULL;\n",
      "2705         }\n",
      "2706 \n",
      "2707     }\n",
      "2708     return rval;\n",
      "2709 }\n",
      "2710 \n",
      "2711 static int\n",
      "2712 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2713 {\n",
      "2714     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2715     if (PyTuple_Check(value)){\n",
      "2716         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2717             PyErr_SetString(PyExc_ValueError,\n",
      "2718                             \"The new strides tuple must have the same length\"\n",
      "2719                             \" as the number of dimensions\");\n",
      "2720             return -1;\n",
      "2721         }\n",
      "2722     }else if (PyList_Check(value)){\n",
      "2723         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2724             PyErr_SetString(PyExc_ValueError,\n",
      "2725                             \"The new strides list must have the same length\"\n",
      "2726                             \" as the number of dimensions\");\n",
      "2727             return -1;\n",
      "2728         }\n",
      "2729     }else{\n",
      "2730         PyErr_SetString(PyExc_ValueError,\n",
      "2731                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2732         return -1;\n",
      "2733     }\n",
      "2734     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2735     if (PyTuple_Check(value)){\n",
      "2736         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2737             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2738             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2739         }\n",
      "2740     }else if (PyList_Check(value)){\n",
      "2741         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2742             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2743             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2744         }\n",
      "2745     }\n",
      "2746     /*\n",
      "2747     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2748     // to do it.\n",
      "2749     npy_intp dims[PyTuple_Size(value)];\n",
      "2750     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2751         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2752     }\n",
      "2753     if (!PyArray_CheckStrides(4,\n",
      "2754                               CudaNdarray_NDIM(self),\n",
      "2755                               0, 0,\n",
      "2756                               dims,\n",
      "2757                               newstrides_bytes)){\n",
      "2758         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2759         return -1;\n",
      "2760         }\n",
      "2761     */\n",
      "2762     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2763         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2764     }\n",
      "2765     return 0;\n",
      "2766 }\n",
      "2767 \n",
      "2768 static PyObject *\n",
      "2769 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2770 {\n",
      "2771     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2772     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2773     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2774 }\n",
      "2775 \n",
      "2776 static int\n",
      "2777 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2778 {\n",
      "2779     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2780     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2781     if (PyErr_Occurred())\n",
      "2782     {\n",
      "2783         return -1;\n",
      "2784     }\n",
      "2785     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2786 }\n",
      "2787 \n",
      "2788 static PyObject *\n",
      "2789 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2790 {\n",
      "2791     return PyString_FromString(\"float32\");\n",
      "2792 }\n",
      "2793 \n",
      "2794 static PyObject *\n",
      "2795 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2796 {\n",
      "2797     return PyInt_FromLong(self->nd);\n",
      "2798 }\n",
      "2799 \n",
      "2800 static PyObject *\n",
      "2801 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2802 {\n",
      "2803     PyObject * base = self->base;\n",
      "2804     if (!base)\n",
      "2805     {\n",
      "2806         // We cannot return a NULL pointer, use None instead\n",
      "2807         base = Py_None;\n",
      "2808     }\n",
      "2809     Py_INCREF(base);\n",
      "2810     return base;\n",
      "2811 }\n",
      "2812 \n",
      "2813 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2814 {\n",
      "2815   PyObject * k = PyString_FromString(key);\n",
      "2816   PyObject * v = PyInt_FromLong(val);\n",
      "2817   PyDict_SetItem(dict, k, v);\n",
      "2818   Py_DECREF(k);\n",
      "2819   Py_DECREF(v);\n",
      "2820 }\n",
      "2821 \n",
      "2822 PyObject *\n",
      "2823 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2824 {\n",
      "2825   int dev_id = -1;\n",
      "2826   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2827     return NULL;\n",
      "2828   cudaDeviceProp deviceProp;\n",
      "2829   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2830 \n",
      "2831   PyObject * dict = PyDict_New();\n",
      "2832   PyObject * str= PyString_FromString(\"name\");\n",
      "2833   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2834   PyDict_SetItem(dict, str, i);\n",
      "2835   Py_DECREF(str);\n",
      "2836   Py_DECREF(i);\n",
      "2837 \n",
      "2838   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2839   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2840 #if CUDART_VERSION >= 2020\n",
      "2841   int driverVersion = 0, runtimeVersion = 0;\n",
      "2842   cudaDriverGetVersion(&driverVersion);\n",
      "2843   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2844   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2845   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2846 #endif\n",
      "2847 #if CUDART_VERSION >= 2000\n",
      "2848 \n",
      "2849   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2850   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2851   int sm_cores = -1;\n",
      "2852   if(deviceProp.major==1)\n",
      "2853     sm_cores = 32;\n",
      "2854   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2855     sm_cores = 32;\n",
      "2856   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2857     sm_cores = 48;\n",
      "2858   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2859 #endif\n",
      "2860   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2861   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2862   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2863   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2864   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2865   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2866   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2867   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2868   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2869   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2870   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2871   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2872   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2873   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2874 #if CUDART_VERSION >= 2000\n",
      "2875   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2876 #endif\n",
      "2877 #if CUDART_VERSION >= 2020\n",
      "2878   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2879   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2880   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2881   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2882   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2883 #endif\n",
      "2884 #if CUDART_VERSION >= 3000\n",
      "2885   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2886 #endif\n",
      "2887 #if CUDART_VERSION >= 3010\n",
      "2888   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2889 #endif\n",
      "2890 #if CUDART_VERSION >= 3020\n",
      "2891   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2892 #endif\n",
      "2893 \n",
      "2894   return dict;\n",
      "2895 }\n",
      "2896 \n",
      "2897 /*\n",
      "2898  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2899  */\n",
      "2900 PyObject *\n",
      "2901 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2902 {\n",
      "2903     size_t free = 0, total = 0;\n",
      "2904     if(g_gpu_context_active == 0){\n",
      "2905         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2906         return NULL;\n",
      "2907     }\n",
      "2908 \n",
      "2909     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2910     if (err != cudaSuccess){\n",
      "2911         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2912         // Currently this returns the same thing as err, but if in future\n",
      "2913         // it returns something else I still don't see why we should ignore\n",
      "2914         // it.  All we want to do here is reset the flag.\n",
      "2915         cudaGetLastError();\n",
      "2916         PyErr_Format(PyExc_RuntimeError,\n",
      "2917                      \"Error while getting memory info about the gpu: %s\",\n",
      "2918                      cudaGetErrorString(err));\n",
      "2919         return NULL;\n",
      "2920     }\n",
      "2921     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
      "2922 }\n",
      "2923 \n",
      "2924 /*\n",
      "2925  * Synchronize with all the gpu device stream.\n",
      "2926  */\n",
      "2927 PyObject *\n",
      "2928 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2929 {\n",
      "2930     CNDA_BEGIN_ALLOW_THREADS\n",
      "2931     cudaThreadSynchronize();\n",
      "2932     CNDA_END_ALLOW_THREADS\n",
      "2933     Py_INCREF(Py_None);\n",
      "2934     return Py_None;\n",
      "2935 }\n",
      "2936 \n",
      "2937 /*\n",
      "2938  * Exist and return true if we link with cublas v2.\n",
      "2939  */\n",
      "2940 PyObject *\n",
      "2941 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2942 {\n",
      "2943     Py_INCREF(Py_True);\n",
      "2944     return Py_True;\n",
      "2945 }\n",
      "2946 \n",
      "2947 PyObject *\n",
      "2948 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)\n",
      "2949 {\n",
      "2950     void * rval = NULL;\n",
      "2951     cudaError_t err;\n",
      "2952     int num_gpus = 0;\n",
      "2953 \n",
      "2954     err = cudaGetDeviceCount(&num_gpus);\n",
      "2955     if (cudaSuccess != err){\n",
      "2956         printf(\"ERR!\\\\n\");\n",
      "2957             PyErr_Format(PyExc_RuntimeError,\n",
      "2958                          \"Not able to get number of GPUs (%s).\",\n",
      "2959                          cudaGetErrorString(err));\n",
      "2960             return NULL;\n",
      "2961     }\n",
      "2962 \n",
      "2963     for (int device = 0; device < num_gpus; device++) {\n",
      "2964         cudaSetDevice(device);\n",
      "2965         err = cudaDeviceSynchronize(); // << CUDA context gets created here.\n",
      "2966         cudaGetLastError(); // reset the error state     \n",
      "2967         if (cudaSuccess == err)\n",
      "2968             break;\n",
      "2969     }\n",
      "2970         \n",
      "2971     if (cudaSuccess != err){\n",
      "2972             printf(\"ERR!\\\\n\");\n",
      "2973                 PyErr_Format(PyExc_RuntimeError,\n",
      "2974                              \"Not able to select available GPU from %d cards (%s).\",\n",
      "2975                              num_gpus, cudaGetErrorString(err));\n",
      "2976                 return NULL;\n",
      "2977     }\n",
      "2978 \n",
      "2979     Py_INCREF(Py_None);\n",
      "2980     return Py_None;\n",
      "2981 }\n",
      "2982 \n",
      "2983 #if COMPUTE_GPU_MEM_USED\n",
      "2984 /*\n",
      "2985  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2986  */\n",
      "2987 PyObject *\n",
      "2988 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2989 {\n",
      "2990     PyObject* a = PyLong_FromLong(_allocated_size);\n",
      "2991     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
      "2992 \n",
      "2993     PyObject* tuple = PyTuple_New(2);\n",
      "2994     PyTuple_SetItem(tuple, 0, a);\n",
      "2995     PyTuple_SetItem(tuple, 1, b);\n",
      "2996     return tuple;\n",
      "2997 }\n",
      "2998 #endif\n",
      "2999 \n",
      "3000 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "3001     {\"shape\",\n",
      "3002         (getter)CudaNdarray_get_shape,\n",
      "3003         (setter)CudaNdarray_set_shape,\n",
      "3004         \"shape of this ndarray (tuple)\",\n",
      "3005         NULL},\n",
      "3006     {\"_strides\",\n",
      "3007         (getter)CudaNdarray_get_strides,\n",
      "3008         (setter)CudaNdarray_set_strides,\n",
      "3009         \"data pointer strides (in elements)\",\n",
      "3010         NULL},\n",
      "3011     {\"strides\",\n",
      "3012         (getter)CudaNdarray_get_strides,\n",
      "3013         (setter)CudaNdarray_set_strides,\n",
      "3014         \"data pointer strides (in elements)\",\n",
      "3015         NULL},\n",
      "3016     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "3017     {\"gpudata\",\n",
      "3018         (getter)CudaNdarray_get_dev_data,\n",
      "3019         NULL,\n",
      "3020         \"device data pointer\",\n",
      "3021         NULL},\n",
      "3022     {\"_dev_data\",\n",
      "3023         (getter)CudaNdarray_get_dev_data,\n",
      "3024         (setter)CudaNdarray_set_dev_data,\n",
      "3025         \"device data pointer\",\n",
      "3026         NULL},\n",
      "3027     {\"dtype\",\n",
      "3028         (getter)CudaNdarray_get_dtype,\n",
      "3029         NULL,\n",
      "3030         \"The dtype of the element. Now always float32\",\n",
      "3031         NULL},\n",
      "3032     {\"size\",\n",
      "3033         (getter)CudaNdarray_SIZE_Object,\n",
      "3034         NULL,\n",
      "3035         \"The number of elements in this object.\",\n",
      "3036         NULL},\n",
      "3037     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "3038     {\"mem_size\",\n",
      "3039         (getter)CudaNdarray_SIZE_Object,\n",
      "3040         NULL,\n",
      "3041         \"The number of elements in this object.\",\n",
      "3042         NULL},\n",
      "3043     {\"ndim\",\n",
      "3044         (getter)CudaNdarray_get_ndim,\n",
      "3045         NULL,\n",
      "3046         \"The number of dimensions in this object.\",\n",
      "3047         NULL},\n",
      "3048     {\"base\",\n",
      "3049         (getter)CudaNdarray_get_base,\n",
      "3050         NULL,\n",
      "3051         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "3052         NULL},\n",
      "3053 \n",
      "3054     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3055 };\n",
      "3056 \n",
      "3057 PyObject *CudaNdarray_repr(PyObject *self)\n",
      "3058 {\n",
      "3059     CudaNdarray *object = (CudaNdarray *)self;\n",
      "3060     PyObject * np_object = CudaNdarray_CreateArrayObj(object);\n",
      "3061     PyObject * str = PyObject_Str((PyObject *) np_object);\n",
      "3062     char * cstr = PyString_AsString(str);\n",
      "3063     PyObject * out = PyString_FromFormat(\"%s%s%s\",\n",
      "3064                         \"CudaNdarray(\",\n",
      "3065                         cstr,\n",
      "3066                         \")\");\n",
      "3067     Py_DECREF(str);\n",
      "3068     Py_DECREF(np_object);\n",
      "3069     #if PY_MAJOR_VERSION >= 3\n",
      "3070     // In Python 3 PyString_FromFormat return a Bytes object\n",
      "3071     PyObject* out2 = PyObject_Str(out);\n",
      "3072     Py_DECREF(out);\n",
      "3073     return out2;\n",
      "3074     #endif\n",
      "3075     return out;\n",
      "3076 }\n",
      "3077 \n",
      "3078 static PyTypeObject CudaNdarrayType =\n",
      "3079 {\n",
      "3080 #if PY_MAJOR_VERSION >= 3\n",
      "3081     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3082 #else\n",
      "3083     PyObject_HEAD_INIT(NULL)\n",
      "3084     0,                         /*ob_size*/\n",
      "3085 #endif\n",
      "3086     \"CudaNdarray\",             /*tp_name*/\n",
      "3087     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3088     0,                         /*tp_itemsize*/\n",
      "3089     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3090     0,                         /*tp_print*/\n",
      "3091     0,                         /*tp_getattr*/\n",
      "3092     0,                         /*tp_setattr*/\n",
      "3093     0,                         /*tp_compare*/\n",
      "3094     CudaNdarray_repr,          /*tp_repr*/\n",
      "3095     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3096     0,                         /*tp_as_sequence*/\n",
      "3097     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3098     0,                         /*tp_hash */\n",
      "3099     0,                         /*tp_call*/\n",
      "3100     0,                         /*tp_str*/\n",
      "3101     0,                         /*tp_getattro*/\n",
      "3102     0,                         /*tp_setattro*/\n",
      "3103     0,                         /*tp_as_buffer*/\n",
      "3104 #if PY_MAJOR_VERSION >= 3\n",
      "3105     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3106     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3107 #else\n",
      "3108     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3109 #endif\n",
      "3110     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3111     0,                         /* tp_traverse */\n",
      "3112     0,                         /* tp_clear */\n",
      "3113     0,                         /* tp_richcompare */\n",
      "3114     0,                         /* tp_weaklistoffset */\n",
      "3115     0,                         /* tp_iter */\n",
      "3116     0,                         /* tp_iternext */\n",
      "3117     CudaNdarray_methods,       /* tp_methods */\n",
      "3118     CudaNdarray_members,       /* tp_members */\n",
      "3119     CudaNdarray_getset,        /* tp_getset */\n",
      "3120     0,                         /* tp_base */\n",
      "3121     0,                         /* tp_dict */\n",
      "3122     0,                         /* tp_descr_get */\n",
      "3123     0,                         /* tp_descr_set */\n",
      "3124     0,                         /* tp_dictoffset */\n",
      "3125     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3126     0,                         /* tp_alloc */\n",
      "3127     CudaNdarray_new,           /* tp_new */\n",
      "3128 };\n",
      "3129 \n",
      "3130 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3131 {\n",
      "3132     dst[0] = sizeof(float*);\n",
      "3133     dst[1] = sizeof(int);\n",
      "3134 }\n",
      "3135 \n",
      "3136 PyObject *\n",
      "3137 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3138 {\n",
      "3139     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3140     if(gpu_data == NULL){\n",
      "3141         return NULL;\n",
      "3142     }\n",
      "3143     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3144 \n",
      "3145     cudaError_t cudaErr = cudaGetLastError();\n",
      "3146     if (cudaSuccess != cudaErr){\n",
      "3147 \n",
      "3148         device_free(gpu_data);\n",
      "3149         return PyErr_Format(PyExc_RuntimeError,\n",
      "3150                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3151                             cudaGetErrorString(cudaErr));\n",
      "3152     }\n",
      "3153 \n",
      "3154     // Transfer the result to cpu\n",
      "3155     int gpu_sizes[] = {-1,-1};\n",
      "3156     cublasStatus_t err;\n",
      "3157     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3158     device_free(gpu_data);\n",
      "3159 \n",
      "3160     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3161         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3162         return NULL;\n",
      "3163     }\n",
      "3164     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3165                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3166 }\n",
      "3167 \n",
      "3168 static int cublas_init();\n",
      "3169 static void cublas_shutdown();\n",
      "3170 // Initialize the gpu.\n",
      "3171 // Takes two optional parameters, the device number and if we should use cnmem.\n",
      "3172 // If the device number is provided, it sets that device to be the active device.\n",
      "3173 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3174 // it does not set an active device.\n",
      "3175 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3176 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.\n",
      "3177 PyObject *\n",
      "3178 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3179 {\n",
      "3180     int card_nb = 0;\n",
      "3181     int card_number_provided = 1;\n",
      "3182     float cnmem = 0; // Theano flag lib.cnmem\n",
      "3183     // if we're given something wildly invalid, this will throw a TypeError\n",
      "3184     if(!PyArg_ParseTuple(args, \"|if\", &card_nb, &cnmem))\n",
      "3185         return NULL;\n",
      "3186     if(cnmem)\n",
      "3187         g_use_cnmem = true;\n",
      "3188 \n",
      "3189     if(PyTuple_Size(args) == 0) {\n",
      "3190         card_number_provided = 0;\n",
      "3191         card_nb = 0;\n",
      "3192     }\n",
      "3193 \n",
      "3194     int deviceCount;\n",
      "3195     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3196     if(cudaSuccess != err) {\n",
      "3197         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3198                             \"Unable to get the number of gpus available: %s\",\n",
      "3199                             cudaGetErrorString(cudaGetLastError()));\n",
      "3200     }\n",
      "3201 \n",
      "3202     // as soon as the first successful call to a cuda* function is made, a\n",
      "3203     // gpu context has been created\n",
      "3204     g_gpu_context_active = 1;\n",
      "3205 \n",
      "3206     if(deviceCount <= 0) {\n",
      "3207         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3208                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3209     }\n",
      "3210     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3211         return PyErr_Format(PyExc_ValueError,\n",
      "3212                             \"Bad device number %d. Only %d devices available.\",\n",
      "3213                             card_nb,\n",
      "3214                             deviceCount);\n",
      "3215     }\n",
      "3216 \n",
      "3217     cudaDeviceProp deviceProp;\n",
      "3218     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3219     if(cudaSuccess != err) {\n",
      "3220         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3221                             \"Unable to get properties of gpu %i: %s\",\n",
      "3222                             card_nb,\n",
      "3223                             cudaGetErrorString(cudaGetLastError()));\n",
      "3224     }\n",
      "3225 \n",
      "3226     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3227         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3228                             \"There is no device that supports CUDA\");\n",
      "3229     }\n",
      "3230 \n",
      "3231     if(card_number_provided) {\n",
      "3232         err = cudaSetDevice(card_nb);\n",
      "3233         if(cudaSuccess != err) {\n",
      "3234             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3235                                 \"Unable to set device %i: %s\",\n",
      "3236                                 card_nb,\n",
      "3237                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3238         }\n",
      "3239         if (cublas_init() == -1)\n",
      "3240             return NULL;\n",
      "3241     }\n",
      "3242     if(card_number_provided && g_use_cnmem) {\n",
      "3243         size_t mem = 0;\n",
      "3244         if (cnmem > 1)\n",
      "3245             mem = cnmem * 1024 * 1024;\n",
      "3246         else{\n",
      "3247             // Clip to 95% to let memory for the driver.\n",
      "3248             // 98% didn't worked in some cases.\n",
      "3249             if (cnmem > .95){\n",
      "3250                 cnmem = .95;\n",
      "3251             }\n",
      "3252             size_t free = 0, total = 0;\n",
      "3253             cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "3254             if (err != cudaSuccess){\n",
      "3255                 // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "3256                 // Currently this returns the same thing as err, but if in future\n",
      "3257                 // it returns something else I still don't see why we should ignore\n",
      "3258                 // it.  All we want to do here is reset the flag.\n",
      "3259                 cudaGetLastError();\n",
      "3260                 PyErr_Format(PyExc_RuntimeError,\n",
      "3261                              \"Error while getting memory info about the gpu: %s\",\n",
      "3262                              cudaGetErrorString(err));\n",
      "3263                 return NULL;\n",
      "3264             }\n",
      "3265             mem = total * cnmem;\n",
      "3266         }\n",
      "3267         if(initCnmem(card_number_provided, card_nb, mem) == -1){\n",
      "3268             return NULL;\n",
      "3269         }\n",
      "3270     }\n",
      "3271 \n",
      "3272     Py_INCREF(Py_None);\n",
      "3273     return Py_None;\n",
      "3274 }\n",
      "3275 \n",
      "3276 PyObject *\n",
      "3277 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3278     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3279     // really necessary.\n",
      "3280     int currentDevice;\n",
      "3281     cudaGetDevice(&currentDevice);\n",
      "3282     return PyInt_FromLong(currentDevice);\n",
      "3283 }\n",
      "3284 \n",
      "3285 PyObject *\n",
      "3286 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3287     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3288     // really necessary.\n",
      "3289     int currentDevice;\n",
      "3290     cudaGetDevice(&currentDevice);\n",
      "3291 \n",
      "3292     cudaDeviceProp deviceProp;\n",
      "3293     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3294     return PyString_FromString(deviceProp.name);\n",
      "3295 }\n",
      "3296 \n",
      "3297 PyObject *\n",
      "3298 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3299     // Don't handle errors here\n",
      "3300     cublas_shutdown();\n",
      "3301     g_gpu_context_active = 0; // context has now been closed down\n",
      "3302     if(g_use_cnmem) {\n",
      "3303         cnmemStatus_t status = cnmemFinalize();\n",
      "3304         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "3305             fprintf(stderr, \"CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\\n\",\n",
      "3306                     cnmemGetErrorString(status));\n",
      "3307             if(status == CNMEM_STATUS_CUDA_ERROR) {\n",
      "3308                 fprintf(stderr, \"  Cuda-Reason=%s\\n\",\n",
      "3309                         cudaGetErrorString(cudaGetLastError()));\n",
      "3310             }\n",
      "3311         }\n",
      "3312     }\n",
      "3313     cudaThreadExit();\n",
      "3314 \n",
      "3315     Py_INCREF(Py_None);\n",
      "3316     return Py_None;\n",
      "3317 }\n",
      "3318 \n",
      "3319 /*\n",
      "3320  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3321  */\n",
      "3322 PyObject *\n",
      "3323 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3324 {\n",
      "3325     int verbose = 0;\n",
      "3326     PyObject *gpu_ptr = NULL;\n",
      "3327     PyObject *shapes = NULL;\n",
      "3328     PyObject *strides = NULL;\n",
      "3329     PyObject *base = NULL;\n",
      "3330     PyObject *rval = NULL;\n",
      "3331 \n",
      "3332     //args should consist of 3 python objects\n",
      "3333     //The first is the gpu ptr\n",
      "3334     //The second if the shape\n",
      "3335     //The third if the strides\n",
      "3336     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3337         return NULL;\n",
      "3338 \n",
      "3339     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3340     if (!PyLong_Check(gpu_ptr))\n",
      "3341     {\n",
      "3342         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3343         return NULL;\n",
      "3344     }\n",
      "3345 \n",
      "3346     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3347     if (nd < 0)\n",
      "3348     {\n",
      "3349         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3350         return NULL;\n",
      "3351     }\n",
      "3352     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3353     if (nd_stride < 0)\n",
      "3354     {\n",
      "3355         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3356         return NULL;\n",
      "3357     }\n",
      "3358 \n",
      "3359     if (nd != nd_stride)\n",
      "3360     {\n",
      "3361         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3362         return NULL;\n",
      "3363     }\n",
      "3364 \n",
      "3365     rval = CudaNdarray_New();\n",
      "3366 \n",
      "3367     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3368     {\n",
      "3369         //CudaNdarray_set_nd set the error msg\n",
      "3370         return NULL;\n",
      "3371     }\n",
      "3372     // set gpu pointeur\n",
      "3373     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3374     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3375     {\n",
      "3376         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3377         return NULL;\n",
      "3378 \n",
      "3379     }\n",
      "3380 \n",
      "3381     // Set dims and strides\n",
      "3382     for (int i = nd-1; i >= 0; --i)\n",
      "3383     {\n",
      "3384         PyObject * idx = PyLong_FromLong(i);\n",
      "3385         if (idx == NULL)\n",
      "3386         {\n",
      "3387             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3388             return NULL;\n",
      "3389         }\n",
      "3390         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3391         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3392         if (!PyInt_Check(dim_))\n",
      "3393         {\n",
      "3394             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3395             return NULL;\n",
      "3396         }\n",
      "3397         if (!PyInt_Check(strd_))\n",
      "3398         {\n",
      "3399             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3400             return NULL;\n",
      "3401         }\n",
      "3402         int dim = PyInt_AsLong(dim_);\n",
      "3403         int strd = PyInt_AsLong(strd_);\n",
      "3404         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3405         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3406         Py_DECREF(idx);\n",
      "3407         Py_DECREF(dim_);\n",
      "3408         Py_DECREF(strd_);\n",
      "3409     }\n",
      "3410     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3411     return rval;\n",
      "3412 }\n",
      "3413 \n",
      "3414 PyObject *\n",
      "3415 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3416 {\n",
      "3417     PyObject *l=NULL;\n",
      "3418     PyObject *r=NULL;\n",
      "3419     PyObject * rval = NULL;\n",
      "3420 \n",
      "3421     //args should consist of two python objects (\"OO\")\n",
      "3422     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3423         return NULL;\n",
      "3424 \n",
      "3425     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3426     {\n",
      "3427         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
      "3428         goto CudaNdarray_dot_fail;\n",
      "3429     }\n",
      "3430     if (((CudaNdarray*)l)->nd != 2)\n",
      "3431     {\n",
      "3432         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3433         goto CudaNdarray_dot_fail;\n",
      "3434     }\n",
      "3435     if (((CudaNdarray*)r)->nd != 2)\n",
      "3436     {\n",
      "3437         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3438         goto CudaNdarray_dot_fail;\n",
      "3439     }\n",
      "3440     rval = CudaNdarray_New();\n",
      "3441     if (!rval)\n",
      "3442     {\n",
      "3443         goto CudaNdarray_dot_fail;\n",
      "3444     }\n",
      "3445     int dims[2];\n",
      "3446     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3447     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3448     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3449     {\n",
      "3450         goto CudaNdarray_dot_fail;\n",
      "3451     }\n",
      "3452     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3453     {\n",
      "3454         goto CudaNdarray_dot_fail;\n",
      "3455     }\n",
      "3456 \n",
      "3457     return rval;\n",
      "3458 \n",
      "3459     CudaNdarray_dot_fail:\n",
      "3460     Py_XDECREF(rval);\n",
      "3461     return NULL;\n",
      "3462 }\n",
      "3463 \n",
      "3464 static PyObject *\n",
      "3465 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3466 {\n",
      "3467     /*\n",
      "3468      * TODO: DOC what this function should do in the various cases of\n",
      "3469      * What is 'strict' supposed to mean in the context of this function?\n",
      "3470      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3471      *\n",
      "3472      */\n",
      "3473     PyObject *py_data=NULL;\n",
      "3474     PyArrayObject * data = NULL;\n",
      "3475     int strict = 0;\n",
      "3476     PyObject * broadcastable=NULL;\n",
      "3477     PyObject * storage=NULL;\n",
      "3478     CudaNdarray * rval=NULL;\n",
      "3479 \n",
      "3480     //Python object references which are provided to the caller are borrowed references\n",
      "3481     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3482 \n",
      "3483     if (!PyTuple_Check(broadcastable)){\n",
      "3484         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3485         return NULL;\n",
      "3486     }\n",
      "3487     Py_INCREF(py_data);\n",
      "3488     Py_INCREF(broadcastable);\n",
      "3489 \n",
      "3490     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3491 \n",
      "3492     if (strict || CudaNdarray_Check(py_data))\n",
      "3493     {\n",
      "3494         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3495         if (!CudaNdarray_Check(py_data))\n",
      "3496         {\n",
      "3497             Py_DECREF(py_data);\n",
      "3498             Py_DECREF(broadcastable);\n",
      "3499             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3500             return NULL;\n",
      "3501         }\n",
      "3502         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3503         {\n",
      "3504             Py_DECREF(py_data);\n",
      "3505             Py_DECREF(broadcastable);\n",
      "3506             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3507             return NULL;\n",
      "3508         }\n",
      "3509         for (int i = 0; i < cnda->nd; ++i)\n",
      "3510         {\n",
      "3511             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3512             {\n",
      "3513                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3514                 Py_DECREF(py_data);\n",
      "3515                 Py_DECREF(broadcastable);\n",
      "3516                 return NULL;\n",
      "3517             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3518                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3519                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3520                 Py_DECREF(py_data);\n",
      "3521                 Py_DECREF(broadcastable);\n",
      "3522                 return NULL;\n",
      "3523             }\n",
      "3524         }\n",
      "3525         Py_DECREF(broadcastable);\n",
      "3526         return py_data;\n",
      "3527     }\n",
      "3528     else\n",
      "3529     {\n",
      "3530         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3531         if (!data)\n",
      "3532         {\n",
      "3533             //err message already defined\n",
      "3534             Py_DECREF(py_data);\n",
      "3535             Py_DECREF(broadcastable);\n",
      "3536             return NULL;\n",
      "3537         }\n",
      "3538         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3539         {\n",
      "3540             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3541             {\n",
      "3542                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3543                 Py_DECREF(data);\n",
      "3544                 Py_DECREF(py_data);\n",
      "3545                 Py_DECREF(broadcastable);\n",
      "3546                 return NULL;\n",
      "3547             }\n",
      "3548         }\n",
      "3549         if (storage && CudaNdarray_Check(storage))\n",
      "3550         {\n",
      "3551             rval = (CudaNdarray*) storage;\n",
      "3552             Py_INCREF(rval);\n",
      "3553         }\n",
      "3554         else\n",
      "3555         {\n",
      "3556             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3557         }\n",
      "3558         if (rval)\n",
      "3559         {\n",
      "3560             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3561             {\n",
      "3562                 Py_DECREF(rval);\n",
      "3563                 rval = NULL;\n",
      "3564             }\n",
      "3565         }\n",
      "3566         Py_DECREF(data);\n",
      "3567         Py_DECREF(py_data);\n",
      "3568         Py_DECREF(broadcastable);\n",
      "3569         return (PyObject*)rval;\n",
      "3570     }\n",
      "3571 }\n",
      "3572 \n",
      "3573 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3574 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3575 static PyMethodDef module_methods[] = {\n",
      "3576     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3577     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3578     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3579     {\"select_a_gpu\", CudaNdarray_select_a_gpu, METH_NOARGS, \"Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU.\"},\n",
      "3580     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3581     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3582     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3583     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3584     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3585 #if COMPUTE_GPU_MEM_USED\n",
      "3586     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3587 #endif\n",
      "3588     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3589     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3590     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3591     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3592     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3593     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3594      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3595     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3596 };\n",
      "3597 \n",
      "3598 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3599 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3600 \n",
      "3601 #if PY_MAJOR_VERSION == 3\n",
      "3602 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3603 {\n",
      "3604     PyModuleDef_HEAD_INIT,\n",
      "3605     CNDA_MOD_NAME,\n",
      "3606     CNDA_DOCSTRING,\n",
      "3607     -1,     /* size of per-interpreter state of the module,\n",
      "3608                or -1 if the module keeps state in global variables. */\n",
      "3609     module_methods\n",
      "3610 };\n",
      "3611 \n",
      "3612 PyMODINIT_FUNC\n",
      "3613 PyInit_cuda_ndarray(void)\n",
      "3614 #else\n",
      "3615 PyMODINIT_FUNC\n",
      "3616 initcuda_ndarray(void)\n",
      "3617 #endif\n",
      "3618 {\n",
      "3619     import_array();\n",
      "3620 \n",
      "3621     PyObject* m;\n",
      "3622 \n",
      "3623     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3624 #if PY_MAJOR_VERSION == 3\n",
      "3625         return NULL;\n",
      "3626 #else\n",
      "3627         return;\n",
      "3628 #endif\n",
      "3629     }\n",
      "3630 \n",
      "3631 #if PY_MAJOR_VERSION == 3\n",
      "3632     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3633 #else\n",
      "3634     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3635 #endif\n",
      "3636 \n",
      "3637     if (m == NULL) {\n",
      "3638 #if PY_MAJOR_VERSION == 3\n",
      "3639         return NULL;\n",
      "3640 #else\n",
      "3641         return;\n",
      "3642 #endif\n",
      "3643     }\n",
      "3644 \n",
      "3645     Py_INCREF(&CudaNdarrayType);\n",
      "3646     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3647 #if COMPUTE_GPU_MEM_USED\n",
      "3648     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3649         _alloc_size_table[i].ptr=NULL;\n",
      "3650         _alloc_size_table[i].size=0;\n",
      "3651     }\n",
      "3652 #endif\n",
      "3653     //    cublasInit();\n",
      "3654     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3655     //{\n",
      "3656         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3657     //}\n",
      "3658     if (0) //TODO: is this necessary?\n",
      "3659     {\n",
      "3660         int deviceId = 0; // TODO: what number goes here?\n",
      "3661         cudaSetDevice(deviceId);\n",
      "3662         cudaError_t err = cudaGetLastError();\n",
      "3663         if( cudaSuccess != err)\n",
      "3664         {\n",
      "3665             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3666         }\n",
      "3667     }\n",
      "3668 \n",
      "3669 #if PY_MAJOR_VERSION == 3\n",
      "3670     return m;\n",
      "3671 #endif\n",
      "3672 }\n",
      "3673 \n",
      "3674 \n",
      "3675 //////////////////////////////////////\n",
      "3676 //\n",
      "3677 // C API FOR CudaNdarray\n",
      "3678 //\n",
      "3679 //////////////////////////////////////\n",
      "3680 \n",
      "3681 int\n",
      "3682 CudaNdarray_Check(const PyObject * ob)\n",
      "3683 {\n",
      "3684     //TODO: doesn't work with inheritance\n",
      "3685     return CudaNdarray_CheckExact(ob);\n",
      "3686 }\n",
      "3687 int\n",
      "3688 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3689 {\n",
      "3690     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3691 }\n",
      "3692 \n",
      "3693 PyObject *\n",
      "3694 CudaNdarray_New(int nd)\n",
      "3695 {\n",
      "3696     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3697     if (self == NULL)\n",
      "3698     {\n",
      "3699         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3700         return NULL;\n",
      "3701     }\n",
      "3702     CudaNdarray_null_init(self);\n",
      "3703 \n",
      "3704     if (nd == 0)\n",
      "3705     {\n",
      "3706         self->nd = 0;\n",
      "3707     }\n",
      "3708     else if (nd > 0)\n",
      "3709     {\n",
      "3710         if (CudaNdarray_set_nd(self, nd))\n",
      "3711         {\n",
      "3712             Py_DECREF(self);\n",
      "3713             return NULL;\n",
      "3714         }\n",
      "3715     }\n",
      "3716     ++_outstanding_mallocs[1];\n",
      "3717     return (PyObject *)self;\n",
      "3718 }\n",
      "3719 \n",
      "3720 \n",
      "3721 \n",
      "3722 //////////////////////////////\n",
      "3723 //\n",
      "3724 // Published helper functions\n",
      "3725 //\n",
      "3726 //////////////////////////////\n",
      "3727 \n",
      "3728 static int\n",
      "3729 cublas_init()\n",
      "3730 {\n",
      "3731     cublasStatus_t err;\n",
      "3732     err = cublasCreate(&handle);\n",
      "3733     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3734     {\n",
      "3735         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3736             PyErr_SetString(PyExc_RuntimeError,\n",
      "3737                             \"cublasCreate() returned this error \"\n",
      "3738                             \"'the CUDA Runtime initialization failed'\");\n",
      "3739         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3740             PyErr_SetString(PyExc_RuntimeError,\n",
      "3741                             \"cublasCreate() returned this error \"\n",
      "3742                             \"'the resources could not be allocated'\");\n",
      "3743         else\n",
      "3744             PyErr_SetString(PyExc_RuntimeError,\n",
      "3745                             \"unknow error during returned by cublasCreate()\");\n",
      "3746         return -1;\n",
      "3747     }\n",
      "3748     // Set the default stream as the one to execute on (default)\n",
      "3749     cublasSetStream(handle, NULL);\n",
      "3750     // Pointer to scalars are on the host (also default)\n",
      "3751     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3752 #if CUDA_VERSION >= 5000\n",
      "3753     // atomics can be used in kernels to speed up operations (not default)\n",
      "3754     // This may lead to a slight variance from run to run in some operations\n",
      "3755     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3756 #endif\n",
      "3757     return 0;\n",
      "3758 }\n",
      "3759 \n",
      "3760 static void\n",
      "3761 cublas_shutdown()\n",
      "3762 {\n",
      "3763     if (handle != NULL)\n",
      "3764         cublasDestroy(handle);\n",
      "3765     // No point in handling any errors here\n",
      "3766     handle = NULL;\n",
      "3767 }\n",
      "3768 \n",
      "3769 int\n",
      "3770 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3771 {\n",
      "3772     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3773                                            PyArray_DIMS(obj));\n",
      "3774     if (err) {\n",
      "3775         return err;\n",
      "3776     }\n",
      "3777 \n",
      "3778     int typenum = PyArray_TYPE(obj);\n",
      "3779     if (typenum != REAL_TYPENUM)\n",
      "3780     {\n",
      "3781         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3782         return -1;\n",
      "3783     }\n",
      "3784     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3785     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3786         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3787     if (!py_src) {\n",
      "3788         return -1;\n",
      "3789     }\n",
      "3790     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3791     void *py_src_data = PyArray_DATA(py_src);\n",
      "3792     cudaError_t cerr;\n",
      "3793     CNDA_BEGIN_ALLOW_THREADS;\n",
      "3794     cerr = cudaMemcpy(self->devdata, py_src_data,\n",
      "3795                       py_src_size * sizeof(real),\n",
      "3796                       cudaMemcpyHostToDevice);\n",
      "3797     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "3798     CNDA_END_ALLOW_THREADS;\n",
      "3799     if (cudaSuccess != cerr)\n",
      "3800     {\n",
      "3801         PyErr_Format(PyExc_RuntimeError,\n",
      "3802                      \"Cuda error '%s' while copying %lli data element\"\n",
      "3803                      \" to device memory\",\n",
      "3804                      cudaGetErrorString(cerr),\n",
      "3805                      (long long)py_src_size);\n",
      "3806         Py_DECREF(py_src);\n",
      "3807         return -1;\n",
      "3808     }\n",
      "3809     Py_DECREF(py_src);\n",
      "3810     return 0;\n",
      "3811 }\n",
      "3812 \n",
      "3813 PyObject *\n",
      "3814 CudaNdarray_new_nd(int nd)\n",
      "3815 {\n",
      "3816     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3817     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3818     {\n",
      "3819         Py_XDECREF(rval);\n",
      "3820         rval = NULL;\n",
      "3821     }\n",
      "3822     return (PyObject *) rval;\n",
      "3823 }\n",
      "3824 \n",
      "3825 \n",
      "3826 /**\n",
      "3827  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3828  */\n",
      "3829 \n",
      "3830 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3831 {\n",
      "3832     if (self->data_allocated)\n",
      "3833     {\n",
      "3834         assert(self->devdata);\n",
      "3835         if (device_free(self->devdata))\n",
      "3836         {\n",
      "3837             self->devdata = NULL;\n",
      "3838             self->data_allocated = 0;\n",
      "3839             return -1;\n",
      "3840         }\n",
      "3841     }\n",
      "3842     // Get the original base object (base.base.base...)\n",
      "3843     PyObject * orig_base = base;\n",
      "3844     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3845     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3846     {\n",
      "3847         // base_base is itself a view\n",
      "3848         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3849     }\n",
      "3850     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3851     if (self->base != orig_base)\n",
      "3852     {\n",
      "3853         Py_XDECREF(self->base);\n",
      "3854         self->base = orig_base;\n",
      "3855         Py_XINCREF(self->base);\n",
      "3856     }\n",
      "3857     self->data_allocated = 0;\n",
      "3858     self->devdata = data;\n",
      "3859     return 0;\n",
      "3860 }\n",
      "3861 \n",
      "3862 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3863 {\n",
      "3864     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3865     {\n",
      "3866         y[i*sy] = x[i*sx];\n",
      "3867     }\n",
      "3868 }\n",
      "3869 \n",
      "3870 // N1 through N4 are the size of y\n",
      "3871 static __global__ void k_copy_4d(const int N1,\n",
      "3872         const int N2, const int N3, const int N4,\n",
      "3873         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3874         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3875         const int sy3, const int sy4)\n",
      "3876 {\n",
      "3877     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3878     int bx = blockIdx.x;\n",
      "3879     int by = blockIdx.y;\n",
      "3880 \n",
      "3881     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3882     {\n",
      "3883         for (int j = by; j < N2; j += gridDim.y)\n",
      "3884         {\n",
      "3885             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3886             {\n",
      "3887                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3888                 {\n",
      "3889                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3890                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3891                 }\n",
      "3892             }\n",
      "3893         }\n",
      "3894     }\n",
      "3895 }\n",
      "3896 \n",
      "3897 //copy from other into self\n",
      "3898 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3899                                     const CudaNdarray * other,\n",
      "3900                                     bool unbroadcast)\n",
      "3901 {\n",
      "3902     int verbose = 0;\n",
      "3903     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3904 \n",
      "3905     //standard elemwise size checks\n",
      "3906     if (self->nd == -1)\n",
      "3907     {\n",
      "3908         PyErr_SetString(PyExc_TypeError,\n",
      "3909                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3910         return -1;\n",
      "3911     }\n",
      "3912     CudaNdarray * new_other = NULL;\n",
      "3913 \n",
      "3914     if (self->nd < other->nd)\n",
      "3915     {\n",
      "3916         PyErr_Format(PyExc_NotImplementedError,\n",
      "3917             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3918             \"destination needs to be >= the number of dimensions of the \"\n",
      "3919             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3920         return -1;\n",
      "3921     }\n",
      "3922     else if (self->nd != other->nd)\n",
      "3923     {\n",
      "3924         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3925         int added_dims = self->nd - other->nd;\n",
      "3926         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3927         for(int i = 0; i < added_dims; i++)\n",
      "3928             pattern[i] = -1;\n",
      "3929         for(int i = 0; i < other->nd; i++)\n",
      "3930             pattern[i + added_dims] = i;\n",
      "3931         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3932         other = new_other;\n",
      "3933     }\n",
      "3934     assert(self->nd == other->nd);\n",
      "3935     //standard elemwise dim checks (also compute total size)\n",
      "3936     unsigned int size = 1;\n",
      "3937     unsigned int size_source = 1;\n",
      "3938     for (int i = 0; i< self->nd; ++i)\n",
      "3939     {\n",
      "3940         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3941             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3942         {\n",
      "3943           PyErr_Format(PyExc_ValueError,\n",
      "3944                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3945                        \" need same dimensions for dim %d,\"\n",
      "3946                        \" destination=%d, source=%d\",\n",
      "3947                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3948                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3949           Py_XDECREF(new_other);\n",
      "3950           return -1;\n",
      "3951         }\n",
      "3952         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3953         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3954     }\n",
      "3955     if (0 == size)\n",
      "3956     {\n",
      "3957         Py_XDECREF(new_other);\n",
      "3958         return 0; //nothing to copy, we're done.\n",
      "3959     }\n",
      "3960     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3961         CudaNdarray_is_c_contiguous(other) &&\n",
      "3962         size == size_source)\n",
      "3963     {\n",
      "3964         if (verbose)\n",
      "3965             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3966 \n",
      "3967         cublasStatus_t err;\n",
      "3968         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3969                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3970         CNDA_THREAD_SYNC;\n",
      "3971         Py_XDECREF(new_other);\n",
      "3972         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3973         {\n",
      "3974             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3975             return -1;\n",
      "3976         }\n",
      "3977         return 0;\n",
      "3978     }\n",
      "3979     //TODO: rewrite these copy operations to be more efficient\n",
      "3980     //      See, for example the transpose example in the cuda_sdk.\n",
      "3981     switch (self->nd)\n",
      "3982     {\n",
      "3983         case 0: // scalar\n",
      "3984             {\n",
      "3985                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3986                 assert(0);\n",
      "3987             }; break;\n",
      "3988         case 1: // vector\n",
      "3989             {\n",
      "3990                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3991                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3992                 unsigned int n_blocks = std::min(size,\n",
      "3993                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3994                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3995                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3996                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3997                                             CudaNdarray_DEV_DATA(other),\n",
      "3998                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3999                                             CudaNdarray_DEV_DATA(self),\n",
      "4000                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "4001                 CNDA_THREAD_SYNC;\n",
      "4002                 cudaError_t err = cudaGetLastError();\n",
      "4003                 if( cudaSuccess != err)\n",
      "4004                 {\n",
      "4005                     PyErr_Format(PyExc_RuntimeError,\n",
      "4006                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4007                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "4008                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "4009                     Py_XDECREF(new_other);\n",
      "4010                     return -1;\n",
      "4011                 }\n",
      "4012             }; break;\n",
      "4013         case 4: // 4-tensor\n",
      "4014             {\n",
      "4015                 if (verbose)\n",
      "4016                 {\n",
      "4017                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "4018                     {\n",
      "4019                         Py_XDECREF(new_other);\n",
      "4020                         return -1;\n",
      "4021                     }\n",
      "4022                 }\n",
      "4023 \n",
      "4024                 // The blocks implement the looping over the first two axes so\n",
      "4025                 // this needs to be (N1, N2)\n",
      "4026                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "4027                                         NUM_VECTOR_OP_BLOCKS),\n",
      "4028                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "4029                                         NUM_VECTOR_OP_BLOCKS));\n",
      "4030                 // For the threads, just make as many as possible\n",
      "4031                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "4032                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "4033                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "4034                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "4035 \n",
      "4036                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "4037                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "4038 \n",
      "4039                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "4040                                             // size of y\n",
      "4041                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "4042                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "4043                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "4044                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "4045                                             CudaNdarray_DEV_DATA(other), // x\n",
      "4046                                             // x strides\n",
      "4047                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4048                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "4049                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "4050                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "4051                                             CudaNdarray_DEV_DATA(self), // y\n",
      "4052                                             // y strides\n",
      "4053                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "4054                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "4055                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "4056                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "4057                                             );\n",
      "4058                 CNDA_THREAD_SYNC;\n",
      "4059                 cudaError_t err = cudaGetLastError();\n",
      "4060                 if( cudaSuccess != err)\n",
      "4061                 {\n",
      "4062                     PyErr_Format(PyExc_RuntimeError,\n",
      "4063                                  \"Cuda error: %s: %s.\",\n",
      "4064                                  \"k_copy_4d\",\n",
      "4065                                  cudaGetErrorString(err));\n",
      "4066                     Py_XDECREF(new_other);\n",
      "4067                     return -1;\n",
      "4068                 }\n",
      "4069             }; break;\n",
      "4070         default:\n",
      "4071             {\n",
      "4072                 cudaError_t err = cudaGetLastError();\n",
      "4073                 if(cudaSuccess != err){\n",
      "4074                     PyErr_Format(PyExc_RuntimeError,\n",
      "4075                                  \"Unexpected Cuda error: %s: %s\\n\",\n",
      "4076                                  \"CudaNdarray_CopyFromCudaNdarray\",\n",
      "4077                                  cudaGetErrorString(err));\n",
      "4078                     Py_XDECREF(new_other);\n",
      "4079                     return -1;\n",
      "4080                 }\n",
      "4081 \n",
      "4082                 if (verbose)\n",
      "4083                     fprintf(stderr,\n",
      "4084                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "4085                             unbroadcast);\n",
      "4086                 // call worker routine\n",
      "4087                 unsigned int threads_per_block = std::min(size,\n",
      "4088                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4089                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
      "4090                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "4091                 const CudaNdarray * cuda_dims = other;\n",
      "4092                 if(unbroadcast)\n",
      "4093                     cuda_dims = self;\n",
      "4094                 //copy from other into self\n",
      "4095                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "4096                         size,\n",
      "4097                         (unsigned int)other->nd,\n",
      "4098                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "4099                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "4100                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "4101                         CudaNdarray_DEV_DATA(self),\n",
      "4102                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "4103                 CNDA_THREAD_SYNC;\n",
      "4104                 err = cudaGetLastError();\n",
      "4105                 if(verbose>1)\n",
      "4106                     fprintf(stderr,\n",
      "4107                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "4108                             \" n_threads_per_block=%i)\\n\",\n",
      "4109                             n_blocks, threads_per_block);\n",
      "4110                 if( cudaSuccess != err)\n",
      "4111                 {\n",
      "4112                     //fprint_CudaNdarray(stderr, self);\n",
      "4113                     //fprint_CudaNdarray(stderr, other);\n",
      "4114                     PyErr_Format(PyExc_RuntimeError,\n",
      "4115                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4116                                  \" n_threads_per_block=%i)\\n\",\n",
      "4117                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "4118                                  cudaGetErrorString(err), n_blocks,\n",
      "4119                                  threads_per_block);\n",
      "4120                     Py_XDECREF(new_other);\n",
      "4121                     return -1;\n",
      "4122                 }\n",
      "4123             }\n",
      "4124     };\n",
      "4125     Py_XDECREF(new_other);\n",
      "4126     return 0;\n",
      "4127 }\n",
      "4128 \n",
      "4129 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4130 {\n",
      "4131     if (A->nd != 2)\n",
      "4132     {\n",
      "4133         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4134         return -1;\n",
      "4135     }\n",
      "4136     if (B->nd != 2)\n",
      "4137     {\n",
      "4138         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4139         return -1;\n",
      "4140     }\n",
      "4141     if (C->nd != 2)\n",
      "4142     {\n",
      "4143         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4144         return -1;\n",
      "4145     }\n",
      "4146 \n",
      "4147     // We must allow dimensions to be zeros.\n",
      "4148     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4149             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4150             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4151     {\n",
      "4152         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4153                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4154                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4155                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4156                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4157                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4158                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4159         return -1;\n",
      "4160     }\n",
      "4161 \n",
      "4162     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4163     // dimensions, we can make a copy.\n",
      "4164     CudaNdarray * A_new = NULL;\n",
      "4165     CudaNdarray * B_new = NULL;\n",
      "4166     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4167          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4168          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4169          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4170         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4171         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4172     {\n",
      "4173         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4174         if (!A_new)\n",
      "4175             return -1;\n",
      "4176         A = A_new;\n",
      "4177     }\n",
      "4178 \n",
      "4179     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4180          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4181          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4182          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4183         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4184         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4185     {\n",
      "4186         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4187         if (!B_new)\n",
      "4188         {\n",
      "4189             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4190             Py_XDECREF(A_new);\n",
      "4191             return -1;\n",
      "4192         }\n",
      "4193         B = B_new;\n",
      "4194     }\n",
      "4195 \n",
      "4196     // If matrix C has non-unit size and non-unit stride in both\n",
      "4197     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4198     // C either, because the calling code will expect the result to be\n",
      "4199     // in the original C container.\n",
      "4200     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4201          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4202          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4203          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4204         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4205         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4206     {\n",
      "4207         PyErr_Format(PyExc_AssertionError,\n",
      "4208                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4209                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4210                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4211                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4212                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4213         Py_XDECREF(A_new);\n",
      "4214         Py_XDECREF(B_new);\n",
      "4215         return -1;\n",
      "4216     }\n",
      "4217 \n",
      "4218     // the unit integer is divided logically into three fields of 4 bits\n",
      "4219     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4220     // the next higher 4 bits encode the B variable (or y)\n",
      "4221     // the next higher 4 bits encode the C variable (or x)\n",
      "4222     //\n",
      "4223     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4224     //                                                 1 for unit stride from row to row (Col major)\n",
      "4225 \n",
      "4226     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4227     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4228     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4229     // consider it a 'unit' stride too.\n",
      "4230     int unit = 0;\n",
      "4231     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4232         unit |= (0x0 << 8);\n",
      "4233     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4234         unit |= (0x1 << 8);\n",
      "4235     } else {\n",
      "4236         unit |= (0x2 << 8);\n",
      "4237     }\n",
      "4238     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4239         unit |= (0x0 << 4);\n",
      "4240     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4241         unit |= (0x1 << 4);\n",
      "4242     } else {\n",
      "4243         unit |= (0x2 << 4);\n",
      "4244     }\n",
      "4245     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4246         unit |= (0x0 << 0);\n",
      "4247     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4248         unit |= (0x1 << 0);\n",
      "4249     } else {\n",
      "4250         unit |= (0x2 << 0);\n",
      "4251     }\n",
      "4252 \n",
      "4253     /* create appropriate strides for malformed matrices that are row or column\n",
      "4254      * vectors\n",
      "4255      */\n",
      "4256     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4257     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4258     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4259     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4260     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4261     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4262 \n",
      "4263     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4264     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4265     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4266     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4267     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4268     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4269     // There should be no negative stride at that point\n",
      "4270 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4271     if (sx == 0){sx = 1;}\\\n",
      "4272     if (sy == 0){sy = 1;}\\\n",
      "4273     if (sz == 0){sz = 1;}\\\n",
      "4274     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4275         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4276     } else { \\\n",
      "4277         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4278         Py_XDECREF(A_new);\\\n",
      "4279         Py_XDECREF(B_new);\\\n",
      "4280         return -1; \\\n",
      "4281     }\n",
      "4282 \n",
      "4283     cublasStatus_t err;\n",
      "4284     switch(unit)\n",
      "4285     {\n",
      "4286         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4287         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4288         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4289         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4290         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4291         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4292         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4293         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4294         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4295                  return -1;\n",
      "4296     };\n",
      "4297     CNDA_THREAD_SYNC;\n",
      "4298     Py_XDECREF(A_new);\n",
      "4299     Py_XDECREF(B_new);\n",
      "4300 \n",
      "4301     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4302     {\n",
      "4303         PyErr_Format(PyExc_RuntimeError,\n",
      "4304                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4305                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4306                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4307                      err,  cublasGetErrorString(err),\n",
      "4308                      unit, N,\n",
      "4309                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4310                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4311                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4312                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4313 \n",
      "4314         return -1;\n",
      "4315     }\n",
      "4316     return 0;\n",
      "4317 }\n",
      "4318 \n",
      "4319 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4320 {\n",
      "4321     /**\n",
      "4322     * C <- alpha A B + beta C\n",
      "4323     *    A : matrix\n",
      "4324     *    B, C: vector\n",
      "4325     *    alpha, beta: scalars\n",
      "4326     */\n",
      "4327     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4328     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4329     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4330 \n",
      "4331     // We must allow dimensions to be zeros.\n",
      "4332     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4333             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4334     {\n",
      "4335         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4336                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4337                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4338                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4339                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4340         return -1;\n",
      "4341     }\n",
      "4342 \n",
      "4343     // If matrix A has non-unit size and non-unit stride in both\n",
      "4344     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4345     // make a copy.\n",
      "4346     CudaNdarray * A_new = NULL;\n",
      "4347     CudaNdarray * B_new = NULL;\n",
      "4348     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4349          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4350          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4351          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4352         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4353         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4354     {\n",
      "4355         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4356         if (!A_new)\n",
      "4357             return -1;\n",
      "4358         A = A_new;\n",
      "4359     }\n",
      "4360 \n",
      "4361     // If vector B as a negative stride, we also have to make a copy.\n",
      "4362     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4363     {\n",
      "4364         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4365         if (!B_new)\n",
      "4366         {\n",
      "4367             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4368             // do anything\n",
      "4369             Py_XDECREF(A_new);\n",
      "4370             return -1;\n",
      "4371         }\n",
      "4372         B = B_new;\n",
      "4373     }\n",
      "4374 \n",
      "4375     // cudablas does not handle negative strides as expected\n",
      "4376     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4377         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4378     {\n",
      "4379         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4380                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4381                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4382         Py_XDECREF(A_new);\n",
      "4383         Py_XDECREF(B_new);\n",
      "4384         return -1;\n",
      "4385     }\n",
      "4386 \n",
      "4387     /* create appropriate strides for malformed matrices that are row or column\n",
      "4388      * vectors\n",
      "4389      */\n",
      "4390     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4391     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4392     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4393     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4394 \n",
      "4395     if (sa_0 == 0)\n",
      "4396         sa_0 = 1;\n",
      "4397     if (sa_1 == 0)\n",
      "4398         sa_1 = 1;\n",
      "4399 \n",
      "4400     // This is important because we can end up not calling Sgemv at all\n",
      "4401     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4402     if (CudaNdarray_SIZE(C)) {\n",
      "4403         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4404             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4405                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4406         {\n",
      "4407             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4408                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4409                     &alpha,\n",
      "4410                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4411                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4412                     &beta,\n",
      "4413                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4414         }\n",
      "4415         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4416                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4417                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4418         {\n",
      "4419             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4420                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4421                     &alpha,\n",
      "4422                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4423                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4424                     &beta,\n",
      "4425                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4426         }\n",
      "4427         else\n",
      "4428         {\n",
      "4429             PyErr_Format(PyExc_AssertionError,\n",
      "4430                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4431                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4432                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4433                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4434                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4435                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4436                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4437                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4438                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4439                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4440             Py_XDECREF(A_new);\n",
      "4441             Py_XDECREF(B_new);\n",
      "4442             return -1;\n",
      "4443         }\n",
      "4444     }\n",
      "4445 \n",
      "4446     CNDA_THREAD_SYNC;\n",
      "4447     Py_XDECREF(A_new);\n",
      "4448     Py_XDECREF(B_new);\n",
      "4449 \n",
      "4450     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4451     {\n",
      "4452         PyErr_Format(PyExc_RuntimeError,\n",
      "4453                      \"cublasSgemv failed (%i)\",\n",
      "4454                      err);\n",
      "4455         return -1;\n",
      "4456     }\n",
      "4457     return 0;\n",
      "4458 }\n",
      "4459 \n",
      "4460 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4461     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4462     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4463     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4464 \n",
      "4465     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4466         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4467         PyErr_Format(PyExc_ValueError,\n",
      "4468                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4469                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4470                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4471                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4472                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4473         return -1;\n",
      "4474     }\n",
      "4475 \n",
      "4476     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4477     CudaNdarray * x_new = NULL;\n",
      "4478     if(x_strides == 0){\n",
      "4479         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4480             PyErr_Format(PyExc_RuntimeError,\n",
      "4481                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4482                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4483                          \" that has more than 1 element!\");\n",
      "4484             return -1;\n",
      "4485         }\n",
      "4486         x_strides = 1;\n",
      "4487     } else if(x_strides < 0){\n",
      "4488         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4489         x = x_new;\n",
      "4490         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4491     }\n",
      "4492 \n",
      "4493     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4494     CudaNdarray * y_new = NULL;\n",
      "4495     if(y_strides == 0){\n",
      "4496         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4497             PyErr_Format(PyExc_RuntimeError,\n",
      "4498                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4499                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4500                          \" that has more than 1 elements!\");\n",
      "4501             Py_XDECREF(x_new);\n",
      "4502             return -1;\n",
      "4503         }\n",
      "4504         y_strides = 1;\n",
      "4505     } else if(y_strides < 0){\n",
      "4506         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4507         y = y_new;\n",
      "4508         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4509     }\n",
      "4510 \n",
      "4511     // Create appropriate strides if A is a row or column vector\n",
      "4512     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4513                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4514     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4515                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4516 \n",
      "4517     // This is important because we can end up not calling Sger at all\n",
      "4518     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4519     if(CudaNdarray_SIZE(A)){\n",
      "4520         // If A is in col-major\n",
      "4521         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4522             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4523                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4524         {\n",
      "4525             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4526                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4527                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4528                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4529         }\n",
      "4530         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4531         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4532                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4533                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4534         {\n",
      "4535             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4536                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4537                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4538                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4539         }\n",
      "4540         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4541         else\n",
      "4542         {\n",
      "4543             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4544                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4545             Py_XDECREF(x_new);\n",
      "4546             Py_XDECREF(y_new);\n",
      "4547             return -1;\n",
      "4548         }\n",
      "4549     }\n",
      "4550     CNDA_THREAD_SYNC;\n",
      "4551     Py_XDECREF(x_new);\n",
      "4552     Py_XDECREF(y_new);\n",
      "4553 \n",
      "4554     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4555     {\n",
      "4556         PyErr_Format(PyExc_RuntimeError,\n",
      "4557                      \"cublasSger failed (%i)\",\n",
      "4558                      err);\n",
      "4559         return -1;\n",
      "4560     }\n",
      "4561 \n",
      "4562     return 0;\n",
      "4563 }\n",
      "4564 \n",
      "4565 /**\n",
      "4566  *\n",
      "4567  * Precondition:\n",
      "4568  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4569  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4570  *\n",
      "4571  *  TODO: templatize this function to support other reductions.\n",
      "4572  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4573  */\n",
      "4574 \n",
      "4575 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4576         const unsigned int nd,\n",
      "4577         const int * dims_a,\n",
      "4578         const int * log2_dims_a,\n",
      "4579         const int * a_str,\n",
      "4580         const float * a_data,\n",
      "4581         const int * z_str,\n",
      "4582         float * z_data)\n",
      "4583 {\n",
      "4584     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4585     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4586 \n",
      "4587     //structure data contains the strides and dimensions of both a and z\n",
      "4588     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4589     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4590     // a_str[0], ... a_str[nd-1],\n",
      "4591     // z_str[0], ... z_str[nd-1]\n",
      "4592     extern __shared__ int structure_data[];\n",
      "4593     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4594     {\n",
      "4595         structure_data[i+0*nd] = dims_a[i];\n",
      "4596         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4597         structure_data[i+2*nd] = a_str[i];\n",
      "4598         structure_data[i+3*nd] = z_str[i];\n",
      "4599     }\n",
      "4600     dims_a = structure_data;\n",
      "4601     log2_dims_a = structure_data + nd;\n",
      "4602     a_str = structure_data + 2*nd;\n",
      "4603     z_str = structure_data + 3*nd;\n",
      "4604 \n",
      "4605     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4606 \n",
      "4607     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4608     {\n",
      "4609         unsigned int ii = i;\n",
      "4610         const float * a_data_i = a_data;\n",
      "4611         float * z_data_i = z_data;\n",
      "4612         unsigned int n_reduce_elements = 1;\n",
      "4613         unsigned int n_reduce_dims = 0;\n",
      "4614         unsigned int reduce_dim0 = nd-1;\n",
      "4615 \n",
      "4616 \n",
      "4617         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4618         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4619         for (unsigned int d = 0; d < nd; ++d)\n",
      "4620         {\n",
      "4621             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4622             {\n",
      "4623                 n_reduce_elements *= dims_a[d];\n",
      "4624                 n_reduce_dims += 1;\n",
      "4625                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4626             }\n",
      "4627             else //'d' is not a dimension that we are reducing over\n",
      "4628             {\n",
      "4629                 unsigned int pos_d;\n",
      "4630                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4631                 {\n",
      "4632                     // this branch is not preferred,\n",
      "4633                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4634                     pos_d = (ii % dims_a[d]);\n",
      "4635                     ii = (ii / dims_a[d]);\n",
      "4636                 }\n",
      "4637                 else\n",
      "4638                 {\n",
      "4639                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4640                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4641                 }\n",
      "4642                 a_data_i += pos_d * a_str[d];\n",
      "4643                 z_data_i += pos_d * z_str[d];\n",
      "4644             }\n",
      "4645         }\n",
      "4646         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4647         // do a similar loop\n",
      "4648 \n",
      "4649         float sum = 0.0f;\n",
      "4650         switch(n_reduce_dims)\n",
      "4651         {\n",
      "4652             case 0:\n",
      "4653                 {\n",
      "4654                     sum = a_data_i[0];\n",
      "4655                 }\n",
      "4656                 break;\n",
      "4657             case 1:\n",
      "4658                 {\n",
      "4659                     const int stride = a_str[reduce_dim0];\n",
      "4660                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4661                     while (a_data_i != a_data_i_max)\n",
      "4662                     {\n",
      "4663                         sum += a_data_i[0];\n",
      "4664                         a_data_i += stride;\n",
      "4665                     }\n",
      "4666                 }\n",
      "4667                 break;\n",
      "4668             case 2:\n",
      "4669                 {\n",
      "4670                     int rd = reduce_dim0+1;\n",
      "4671                     for (; rd < nd; ++rd)\n",
      "4672                     {\n",
      "4673                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4674                             break;\n",
      "4675                     }\n",
      "4676                     const int stride0 = a_str[reduce_dim0];\n",
      "4677                     const int stride1 = a_str[rd];\n",
      "4678                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4679                     {\n",
      "4680                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4681                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4682                         while (a_data_ri != a_data_ri_max)\n",
      "4683                         {\n",
      "4684                             sum += a_data_ri[0];\n",
      "4685                             a_data_ri += stride0;\n",
      "4686                         }\n",
      "4687                     }\n",
      "4688                 };\n",
      "4689                 break;\n",
      "4690             default:\n",
      "4691                 {\n",
      "4692                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4693                     {\n",
      "4694                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4695                         unsigned int reduce_ii = reduce_i;\n",
      "4696                         const float * a_data_ri = a_data_i;\n",
      "4697 \n",
      "4698                         //This loop finds the element in the a slice to add.\n",
      "4699                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4700                         {\n",
      "4701                             unsigned int pos_d;\n",
      "4702                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4703                             {\n",
      "4704                                 if (log2_dims_a[rd]==-1)\n",
      "4705                                 {\n",
      "4706                                     // this branch is not preferred,\n",
      "4707                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4708                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4709                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4710                                 }\n",
      "4711                                 else\n",
      "4712                                 {\n",
      "4713                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4714                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4715                                 }\n",
      "4716                                 a_data_ri += pos_d * a_str[rd];\n",
      "4717                             }\n",
      "4718                         }\n",
      "4719                         sum += a_data_ri[0];\n",
      "4720                     }\n",
      "4721                 }\n",
      "4722         }\n",
      "4723         z_data_i[0] = sum;\n",
      "4724     }\n",
      "4725 }\n",
      "4726 \n",
      "4727 static __global__ void kernel_reduce_sum_1011(\n",
      "4728         const unsigned int d0,\n",
      "4729         const unsigned int d1,\n",
      "4730         const unsigned int d2,\n",
      "4731         const unsigned int d3,\n",
      "4732         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4733         float * Z, const int sZ0)\n",
      "4734 {\n",
      "4735     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4736     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4737     extern __shared__ float buf[];\n",
      "4738     float mysum = 0.0f;\n",
      "4739 \n",
      "4740     if (warpSize != 32)\n",
      "4741     {\n",
      "4742         return;  //TODO: set error code\n",
      "4743     }\n",
      "4744 \n",
      "4745     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4746     {\n",
      "4747         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4748         mysum += Ai;\n",
      "4749     }\n",
      "4750     buf[threadNum] = mysum;\n",
      "4751     __syncthreads();\n",
      "4752 \n",
      "4753     // rest of function is handled by one warp\n",
      "4754     if (threadNum < warpSize)\n",
      "4755     {\n",
      "4756         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4757         {\n",
      "4758             mysum += buf[i];\n",
      "4759         }\n",
      "4760         buf[threadNum] = mysum;\n",
      "4761         if (threadNum < 16)\n",
      "4762         {\n",
      "4763             //reduce so that threadNum 0 has the sum of everything\n",
      "4764             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4765             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4766             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4767             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4768             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4769             if (threadNum == 0)\n",
      "4770             {\n",
      "4771                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4772             }\n",
      "4773         }\n",
      "4774     }\n",
      "4775 }\n",
      "4776 /**\n",
      "4777  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4778  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4779  */\n",
      "4780 int\n",
      "4781 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4782 {\n",
      "4783     int verbose = 0;\n",
      "4784     //check input rank\n",
      "4785     if (self->nd != A->nd)\n",
      "4786     {\n",
      "4787         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4788         return -1;\n",
      "4789     }\n",
      "4790     for (int i = 0; i < self->nd; ++i)\n",
      "4791     {\n",
      "4792         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4793         {\n",
      "4794             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4795                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4796             return -1;\n",
      "4797         }\n",
      "4798     }\n",
      "4799 \n",
      "4800     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4801     if (verbose)\n",
      "4802     {\n",
      "4803         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4804         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4805         fprint_CudaNdarray(stderr, A);\n",
      "4806         fprint_CudaNdarray(stderr, self);\n",
      "4807     }\n",
      "4808     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4809             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4810             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4811             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4812        )\n",
      "4813     {\n",
      "4814         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4815         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4816         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4817         n_threads.z -= 1;\n",
      "4818         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4819         if (n_threads.z)\n",
      "4820         {\n",
      "4821             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4822             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4823             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4824                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4825                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4826                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4827                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4828                     CudaNdarray_DEV_DATA(A),\n",
      "4829                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4830                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4831                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4832                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4833                     CudaNdarray_DEV_DATA(self),\n",
      "4834                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4835             CNDA_THREAD_SYNC;\n",
      "4836             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4837             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4838         }\n",
      "4839     }\n",
      "4840 \n",
      "4841     int n_threads_per_block = std::min(n_summations,\n",
      "4842             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4843     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4844             NUM_VECTOR_OP_BLOCKS);\n",
      "4845     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4846 \n",
      "4847     if (verbose)\n",
      "4848     {\n",
      "4849         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4850     }\n",
      "4851     assert (self->nd > 0);\n",
      "4852     assert (self->nd == A->nd);\n",
      "4853     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4854             n_summations,\n",
      "4855             self->nd,\n",
      "4856             CudaNdarray_DEV_DIMS(A),\n",
      "4857             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4858             CudaNdarray_DEV_STRIDES(A),\n",
      "4859             CudaNdarray_DEV_DATA(A),\n",
      "4860             CudaNdarray_DEV_STRIDES(self),\n",
      "4861             CudaNdarray_DEV_DATA(self));\n",
      "4862     CNDA_THREAD_SYNC;\n",
      "4863     cudaError_t err = cudaGetLastError();\n",
      "4864     if (cudaSuccess != err)\n",
      "4865     {\n",
      "4866         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4867         return -1;\n",
      "4868     }\n",
      "4869     return 0;\n",
      "4870 }\n",
      "4871 int\n",
      "4872 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4873 {\n",
      "4874     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4875     return -1;\n",
      "4876 }\n",
      "4877 int\n",
      "4878 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4879 {\n",
      "4880     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4881     return -1;\n",
      "4882 }\n",
      "4883 int\n",
      "4884 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4885 {\n",
      "4886     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4887     return -1;\n",
      "4888 }\n",
      "4889 \n",
      "4890 \n",
      "4891 /**\n",
      "4892  *\n",
      "4893  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4894  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4895  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4896  *\n",
      "4897  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4898  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4899  */\n",
      "4900 int\n",
      "4901 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4902 {\n",
      "4903     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4904     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4905     int * newstrides = newdims + len;\n",
      "4906     int * dims_taken = newstrides + len;\n",
      "4907     if (!newdims)\n",
      "4908     {\n",
      "4909         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4910         return -1;\n",
      "4911     }\n",
      "4912     for (int i = 0; i < self->nd; ++i)\n",
      "4913     {\n",
      "4914         dims_taken[i] = 0;\n",
      "4915     }\n",
      "4916     for (int i = 0; i < len; ++i)\n",
      "4917     {\n",
      "4918         if (pattern[i] < 0)\n",
      "4919         {\n",
      "4920             newdims[i] = 1;\n",
      "4921             newstrides[i] = 0;\n",
      "4922         }\n",
      "4923         else if(dims_taken[pattern[i]])\n",
      "4924         {\n",
      "4925             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4926                          pattern[i]);\n",
      "4927             free(newdims);\n",
      "4928             return -1;\n",
      "4929         }\n",
      "4930         else if (pattern[i]>= self->nd)\n",
      "4931         {\n",
      "4932             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4933                          pattern[i], self->nd);\n",
      "4934             free(newdims);\n",
      "4935             return -1;\n",
      "4936         }\n",
      "4937         else\n",
      "4938         {\n",
      "4939             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4940             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4941             dims_taken[pattern[i]] = 1;\n",
      "4942         }\n",
      "4943     }\n",
      "4944     //Check if we dropped not broadcastable dims\n",
      "4945     for (int i = 0; i < self->nd; ++i)\n",
      "4946     {\n",
      "4947         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4948         {\n",
      "4949             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4950             free(newdims);\n",
      "4951             return -1;\n",
      "4952         }\n",
      "4953     }\n",
      "4954     //swap this structure in for the one in self, and sync to the card\n",
      "4955     if (CudaNdarray_set_nd(self, len))\n",
      "4956     {\n",
      "4957         free(newdims);\n",
      "4958         return -1;\n",
      "4959     }\n",
      "4960     for (int i = 0; i < len; ++i)\n",
      "4961     {\n",
      "4962         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4963         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4964     }\n",
      "4965     if (cnda_copy_structure_to_device(self))\n",
      "4966     {\n",
      "4967         free(newdims);\n",
      "4968         return -1;\n",
      "4969     }\n",
      "4970     free(newdims);\n",
      "4971     return 0;\n",
      "4972 }\n",
      "4973 \n",
      "4974 \n",
      "4975 \n",
      "4976 /**\n",
      "4977  *\n",
      "4978  *  This is the function that bind to python.\n",
      "4979  *  See CudaNdarray_dimshuffle to call from C.\n",
      "4980  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "4981  */\n",
      "4982 PyObject *\n",
      "4983 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "4984 {\n",
      "4985     PyObject * self = NULL;\n",
      "4986     PyObject * pattern_object = NULL;\n",
      "4987     int * pattern = NULL;\n",
      "4988     PyObject * rval = NULL;\n",
      "4989     int success = -1;\n",
      "4990     //const int * dims = NULL;\n",
      "4991 \n",
      "4992     //args should consist of two python objects (\"OO\")\n",
      "4993     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "4994         return NULL;\n",
      "4995 \n",
      "4996     if (!CudaNdarray_Check(self) )\n",
      "4997     {\n",
      "4998         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "4999         return NULL;\n",
      "5000     }\n",
      "5001 \n",
      "5002     //parse pattern_object into int * pattern\n",
      "5003 \n",
      "5004     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "5005 \n",
      "5006     if (pattern_dim < 0)\n",
      "5007     {\n",
      "5008         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "5009         return NULL;\n",
      "5010     }\n",
      "5011 \n",
      "5012     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "5013 \n",
      "5014     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "5015     {\n",
      "5016         PyObject * idx = PyLong_FromLong(i);\n",
      "5017 \n",
      "5018         if (idx == NULL)\n",
      "5019         {\n",
      "5020             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "5021             goto CudaNdarray_dimshuffle_fail;\n",
      "5022         }\n",
      "5023 \n",
      "5024         long elem_value = 0;\n",
      "5025 \n",
      "5026         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "5027 \n",
      "5028         if (elem == NULL)\n",
      "5029         {\n",
      "5030             Py_XDECREF( elem);\n",
      "5031             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5032             goto CudaNdarray_dimshuffle_fail;\n",
      "5033         }\n",
      "5034 \n",
      "5035         elem_value = PyInt_AsLong(elem);\n",
      "5036 \n",
      "5037         if (elem_value == -1 && PyErr_Occurred() )\n",
      "5038         {\n",
      "5039             Py_XDECREF(elem);\n",
      "5040             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5041             goto CudaNdarray_dimshuffle_fail;\n",
      "5042         }\n",
      "5043 \n",
      "5044         pattern[i] = elem_value;\n",
      "5045 \n",
      "5046         Py_XDECREF( elem );\n",
      "5047         Py_XDECREF( idx );\n",
      "5048     }\n",
      "5049 \n",
      "5050     //allocate rval\n",
      "5051     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "5052 \n",
      "5053     if (rval == NULL)\n",
      "5054     {\n",
      "5055         //CudaNdarray_New should have set the exception string\n",
      "5056         goto CudaNdarray_dimshuffle_fail;\n",
      "5057     }\n",
      "5058 \n",
      "5059 \n",
      "5060     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "5061     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "5062     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "5063     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "5064 \n",
      "5065     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "5066 \n",
      "5067     if (success != 0)\n",
      "5068     {\n",
      "5069         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "5070         goto CudaNdarray_dimshuffle_fail;\n",
      "5071     }\n",
      "5072 \n",
      "5073     free(pattern);\n",
      "5074 \n",
      "5075     return rval;\n",
      "5076 \n",
      "5077     CudaNdarray_dimshuffle_fail:\n",
      "5078 \n",
      "5079     if (pattern != NULL)\n",
      "5080         free(pattern);\n",
      "5081 \n",
      "5082     Py_XDECREF(rval);\n",
      "5083     return NULL;\n",
      "5084 }\n",
      "5085 \n",
      "5086 \n",
      "5087 int\n",
      "5088 cnda_structure_size(int nd)\n",
      "5089 {\n",
      "5090     // dim0, dim1, ...\n",
      "5091     // str0, str1, ...\n",
      "5092     // log2(dim0), log2(dim1), ...\n",
      "5093     return nd + nd + nd;\n",
      "5094 }\n",
      "5095 \n",
      "5096 const int *\n",
      "5097 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "5098 {\n",
      "5099     return self->host_structure;\n",
      "5100 }\n",
      "5101 \n",
      "5102 const int *\n",
      "5103 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "5104 {\n",
      "5105     return self->host_structure + self->nd;\n",
      "5106 }\n",
      "5107 const int *\n",
      "5108 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "5109 {\n",
      "5110     return self->host_structure + 2*self->nd;\n",
      "5111 }\n",
      "5112 \n",
      "5113 int\n",
      "5114 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "5115 {\n",
      "5116     int verbose = 0;\n",
      "5117 \n",
      "5118     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "5119     {\n",
      "5120         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "5121         return 0;\n",
      "5122     }\n",
      "5123 \n",
      "5124     if (cnda1->nd != cnda2->nd)\n",
      "5125     {\n",
      "5126         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "5127         return 0;\n",
      "5128     }\n",
      "5129 \n",
      "5130     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "5131     {\n",
      "5132         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5133         {\n",
      "5134             if(verbose)\n",
      "5135                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5136             return 0;\n",
      "5137         }\n",
      "5138     }\n",
      "5139 \n",
      "5140     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5141     {\n",
      "5142         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5143         return 0;\n",
      "5144     }\n",
      "5145     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5146     {\n",
      "5147         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5148         return 0;\n",
      "5149     }\n",
      "5150     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5151     {\n",
      "5152         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5153         // no need to check devdata if data is not allocated\n",
      "5154         return 0;\n",
      "5155     }\n",
      "5156 \n",
      "5157     return 1;\n",
      "5158 }\n",
      "5159 \n",
      "5160 \n",
      "5161 int\n",
      "5162 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5163 {\n",
      "5164     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5165 }\n",
      "5166 \n",
      "5167 int\n",
      "5168 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5169 {\n",
      "5170     //If the device structure do not exists, create it.\n",
      "5171     //We allocate it here as we do not need it often.\n",
      "5172     //In fact, we need it so infrequently that we expect\n",
      "5173     //that most object won't need it. Not allocating it\n",
      "5174     //save a significant when creating object.\n",
      "5175     //This speed up a benchmark by 8% with the gc.\n",
      "5176     if (!self->dev_structure)\n",
      "5177     {\n",
      "5178         int struct_size = cnda_structure_size(self->nd);\n",
      "5179         if (struct_size)\n",
      "5180         {\n",
      "5181             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5182             if (NULL == self->dev_structure)\n",
      "5183             {\n",
      "5184                 return -1;\n",
      "5185             }\n",
      "5186         }\n",
      "5187     }\n",
      "5188     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5189                         sizeof(int),\n",
      "5190                         self->host_structure,\n",
      "5191                         1,\n",
      "5192                         self->dev_structure,\n",
      "5193                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5194     {\n",
      "5195         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5196         return -1;\n",
      "5197     }\n",
      "5198     self->dev_structure_fresh = 1;\n",
      "5199     return 0;\n",
      "5200 }\n",
      "5201 \n",
      "5202 const int *\n",
      "5203 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5204 {\n",
      "5205     if (!self->dev_structure_fresh)\n",
      "5206     {\n",
      "5207         if (cnda_copy_structure_to_device(self))\n",
      "5208             return NULL;\n",
      "5209     }\n",
      "5210     return self->dev_structure;\n",
      "5211 }\n",
      "5212 const int *\n",
      "5213 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5214 {\n",
      "5215     if (!self->dev_structure_fresh)\n",
      "5216     {\n",
      "5217         if (cnda_copy_structure_to_device(self))\n",
      "5218             return NULL;\n",
      "5219     }\n",
      "5220     return self->dev_structure + self->nd;\n",
      "5221 }\n",
      "5222 const int *\n",
      "5223 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5224 {\n",
      "5225     if (!self->dev_structure_fresh)\n",
      "5226     {\n",
      "5227         if (cnda_copy_structure_to_device(self))\n",
      "5228             return NULL;\n",
      "5229     }\n",
      "5230     return self->dev_structure + 2*self->nd;\n",
      "5231 }\n",
      "5232 float *\n",
      "5233 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5234 {\n",
      "5235     return self->devdata;\n",
      "5236 }\n",
      "5237 \n",
      "5238 /**\n",
      "5239  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5240  */\n",
      "5241 size_t\n",
      "5242 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5243 {\n",
      "5244     if (self->nd == -1) return 0;\n",
      "5245     size_t size = 1;\n",
      "5246     for (int i = 0; i < self->nd; ++i)\n",
      "5247     {\n",
      "5248         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5249     }\n",
      "5250     return size;\n",
      "5251 }\n",
      "5252 \n",
      "5253 PyObject *\n",
      "5254 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5255 {\n",
      "5256     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5257 }\n",
      "5258 \n",
      "5259 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5260 {\n",
      "5261     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5262 }\n",
      "5263 \n",
      "5264 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5265 {\n",
      "5266     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5267 }\n",
      "5268 \n",
      "5269 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5270 {\n",
      "5271     cudaError_t err = cudaGetLastError();\n",
      "5272     if( cudaSuccess != err)\n",
      "5273     {\n",
      "5274         PyErr_Format(PyExc_RuntimeError,\n",
      "5275                      \"Cuda error: %s: %s.\",\n",
      "5276                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5277                      cudaGetErrorString(err));\n",
      "5278         return -1;\n",
      "5279     }\n",
      "5280     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5281             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5282     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5283     for (int i = 0; i < self->nd; ++i)\n",
      "5284     {\n",
      "5285         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5286     }\n",
      "5287     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5288     for (int i = 0; i < self->nd; ++i)\n",
      "5289     {\n",
      "5290         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5291     }\n",
      "5292 \n",
      "5293     if (self->dev_structure)\n",
      "5294     {\n",
      "5295         int data=0;\n",
      "5296         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5297         for (int i = 0; i < self->nd; ++i)\n",
      "5298         {\n",
      "5299             cublasGetVector(1, sizeof(int),\n",
      "5300                             self->dev_structure+i, 1,\n",
      "5301                             &data, 1);\n",
      "5302             fprintf(fd, \"%i\\t\", data);\n",
      "5303         }\n",
      "5304         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5305         for (int i = 0; i < self->nd; ++i)\n",
      "5306         {\n",
      "5307             cublasGetVector(1, sizeof(int),\n",
      "5308                             self->dev_structure + self->nd+i, 1,\n",
      "5309                             &data, 1);\n",
      "5310             fprintf(fd, \"%i \\t\", data);\n",
      "5311         }\n",
      "5312         fprintf(fd, \"\\n\");\n",
      "5313     }\n",
      "5314     else\n",
      "5315     {\n",
      "5316         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5317     }\n",
      "5318 \n",
      "5319     err = cudaGetLastError();\n",
      "5320     if( cudaSuccess != err)\n",
      "5321     {\n",
      "5322         PyErr_Format(PyExc_RuntimeError,\n",
      "5323                      \"Cuda error: %s: %s.\",\n",
      "5324                      \"fprint_CudaNdarray\",\n",
      "5325                      cudaGetErrorString(err));\n",
      "5326         return -1;\n",
      "5327     }\n",
      "5328     return 0;\n",
      "5329 }\n",
      "5330 \n",
      "5331 \n",
      "5332 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5333                             const int * dims, int fortran)\n",
      "5334 {\n",
      "5335     bool allocated = false;\n",
      "5336     if (*arr == NULL)\n",
      "5337     {\n",
      "5338         // This allocates the metadata but not the data\n",
      "5339         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5340         if (*arr == NULL)\n",
      "5341             return -1;\n",
      "5342         allocated = true;\n",
      "5343     }\n",
      "5344 \n",
      "5345     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5346     {\n",
      "5347         if (allocated)\n",
      "5348         {\n",
      "5349             Py_DECREF(*arr);\n",
      "5350             *arr = NULL;\n",
      "5351         }\n",
      "5352         return -1;\n",
      "5353     }\n",
      "5354     return 0;\n",
      "5355 }\n",
      "5356 \n",
      "5357 \n",
      "5358 /*\n",
      "5359   Local Variables:\n",
      "5360   mode:c++\n",
      "5361   c-basic-offset:4\n",
      "5362   c-file-style:\"stroustrup\"\n",
      "5363   indent-tabs-mode:nil\n",
      "5364   fill-column:79\n",
      "5365   End:\n",
      "5366 */\n",
      "5367 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5368 \n",
      "===============================\n",
      "nvcc fatal   : The version ('90000') of the host compiler ('Apple clang') is not supported\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -m64 -Xcompiler -DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden -Xlinker -rpath,/Users/apple/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-3.5.2-64/cuda_ndarray -I/Users/apple/anaconda/lib/python3.5/site-packages/theano/sandbox/cuda -I/Users/apple/anaconda/lib/python3.5/site-packages/numpy/core/include -I/Users/apple/anaconda/include/python3.5m -I/Users/apple/anaconda/lib/python3.5/site-packages/theano/gof -o /Users/apple/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-3.5.2-64/cuda_ndarray/cuda_ndarray.so mod.cu -L/Users/apple/anaconda/lib -lcublas -lcudart -Xcompiler -undefined,dynamic_lookup -Xlinker -pie')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['nvcc', '-shared', '-O3', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden', '-Xlinker', '-rpath,/Users/apple/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-3.5.2-64/cuda_ndarray', '-I/Users/apple/anaconda/lib/python3.5/site-packages/theano/sandbox/cuda', '-I/Users/apple/anaconda/lib/python3.5/site-packages/numpy/core/include', '-I/Users/apple/anaconda/include/python3.5m', '-I/Users/apple/anaconda/lib/python3.5/site-packages/theano/gof', '-o', '/Users/apple/.theano/compiledir_Darwin-16.7.0-x86_64-i386-64bit-i386-3.5.2-64/cuda_ndarray/cuda_ndarray.so', 'mod.cu', '-L/Users/apple/anaconda/lib', '-lcublas', '-lcudart', '-Xcompiler', '-undefined,dynamic_lookup', '-Xlinker', '-pie']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95851, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFJCAYAAABU5W56AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1bf7eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = list(data.columns[2:])\n",
    "classes_count = [sum(data[data_class] == 1) for data_class in classes]\n",
    "plt.bar(range(len(classes)), classes_count)\n",
    "plt.xticks(range(len(classes)), classes)\n",
    "for i in range(len(classes)):\n",
    "    plt.text(-0.2 + i, classes_count[i] - 370, classes_count[i], color='white', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(classes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  213,   480,   721,   811,   869,  1223,  1563,  1695,  1703,\n",
       "             1744,\n",
       "            ...\n",
       "            94807, 94815, 94895, 94960, 95070, 95091, 95108, 95220, 95269,\n",
       "            95394],\n",
       "           dtype='int64', length=965)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.severe_toxic==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213     F'UCK YOU ADMINISTRATORS (I WILL BE BACK TO TR...\n",
       "480     AH HAAHHAHAHAH YAAA BITCH YAAAA!@!!!!!!AH HAAH...\n",
       "721     FROZEN SUCKS LION KING IS THE BESTFROZEN SUCKS...\n",
       "811                     This user is a fucking douche bag\n",
       "869     fuck  \\n\\n62.158.73.165  62.158.73.165  62.158...\n",
       "1223    Fuck you \\n\\nYou suck. Leave me alone. Fuck ti...\n",
       "1563    I am going to shoot you in the head and laugh ...\n",
       "1695    you're a dirty faggot \\n\\nlick my hairy nut sa...\n",
       "1703    Stop being a shithead, or I will rip off your ...\n",
       "1744    and why the fuck you say fuck you up there? fu...\n",
       "1916    Fuck Wikipedia and fuck all the leftist, liber...\n",
       "1951    stop reverting my edit\\nyour so fucken gay get...\n",
       "2030    SECURITYFUCK HOMELAND SECURITYFUCK HOMELAND SE...\n",
       "2263    Are you a fucking dick-head?  People like you ...\n",
       "2484    Fuck you Orange Suede Sof \\n\\nFuck you Orange ...\n",
       "2492    Hello \\n\\ngo fuck yourself, kid.  Adults are t...\n",
       "2592    Fuck you you stupid and gay bastard who thinls...\n",
       "2675    You can suck my ass, you stupid asshole. THIS ...\n",
       "2817    What a faggot.  The man above says he's an adu...\n",
       "2852    Your a fat cunt \\n Your a fucking fat ugly cun...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.severe_toxic==1].comment_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Nonsense?  kiss off, geek. what I said is true...\n",
       "20     Why hasn't Alitalia been removed rom the allia...\n",
       "26     \"\\nThe Graceful Slick....\\nIs non other than a...\n",
       "30     \"\\n\\n Stupid? \\n\\nAs soon as I saw the phrase ...\n",
       "32     \"\\nBan one side of an argument by a bullshit n...\n",
       "39                           Are you gay?? Sound like it\n",
       "62     and in the program on last night Crime Museum:...\n",
       "79     \"\\n\\nI didn't call you a \"\"biased backward yan...\n",
       "81     \"\\n\\nIt was very constructive you are just ver...\n",
       "86     \"\\n\\n You know what? Fuck you! I tried to be c...\n",
       "89     HE THINKS THEY ARE COPYRIGHT VIOLATION. PITY H...\n",
       "104    \"\\n\\n Corrected spelling of \"\"recognised\"\" to ...\n",
       "120    \"\\n\\nWow, you're a little touchy aren't you? Y...\n",
       "124                         YOU NEED A PUSSY, YOUR GEEK!\n",
       "144    COINCIDENCE? \\n\\nHave you noticed that in ever...\n",
       "149    For the last time, Turkish people ARE NOT WHIT...\n",
       "160    loser, get a life\\n\\nyou must be fat, ugly and...\n",
       "213    F'UCK YOU ADMINISTRATORS (I WILL BE BACK TO TR...\n",
       "220            u aint got no pancake mix lady so stopppp\n",
       "234    \"\\nok a direct quote from the page for fuck is...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.toxic==1].comment_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308     These Iranian bastards are at it again. Not on...\n",
       "571     \"\\n\\n The Great One! \\nI dont really have a pr...\n",
       "595     probably that you are a german... fucking germ...\n",
       "889     \"\\n\\n Contested deletion \\n\\nThis page should ...\n",
       "1013    Also, it's well known that slavic women don't ...\n",
       "1015    How can you defend those barbarian invaders? \\...\n",
       "1172    i think this som bitch is fuckin nuts...i mean...\n",
       "1916    Fuck Wikipedia and fuck all the leftist, liber...\n",
       "1951    stop reverting my edit\\nyour so fucken gay get...\n",
       "2111    He IS in Gay German Nazi Faggots. Please do no...\n",
       "2187    If you guys really discuss napoleon you need t...\n",
       "2297    Suck it!!! Asyou're used to do,ignorant Romani...\n",
       "2322                LOL \\n\\nStop eating shit you nigger!!\n",
       "2467    Swedes are the racist trolls. They support the...\n",
       "2592    Fuck you you stupid and gay bastard who thinls...\n",
       "2776       MATE, are you jewish?\\n\\nGayness is in the air\n",
       "2798    NIGHTSTALLIONS FIANCE GOT FUCKED BY A NIGGER A...\n",
       "2817    What a faggot.  The man above says he's an adu...\n",
       "3003    \"\\n\\nToo funny\\n\\ncorey suggests a famous peop...\n",
       "3062    It's \\n\\nGo Go Jewranger\\nGo Go Jewranger\\nHar...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.identity_hate==1].comment_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86061"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.toxic + data.severe_toxic + data.obscene + data.threat + data.insult + data.identity_hate==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95851, (95851, 8))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9790 + 86061, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5957"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.toxic + data.severe_toxic + data.obscene + data.threat + data.insult + data.identity_hate>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5957, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.toxic + data.severe_toxic + data.obscene + data.threat + data.insult + data.identity_hate>1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_intersect():\n",
    "    arrays_intersect = []\n",
    "    for class_name in classes:\n",
    "        current_intersect = []\n",
    "        for class_name_2 in classes:\n",
    "            if class_name_2 == class_name:\n",
    "                continue\n",
    "            current_intersect += [data[data.loc[:, class_name] + data.loc[:, class_name_2] > 1].shape[0]]\n",
    "        arrays_intersect.append(current_intersect)\n",
    "    return arrays_intersect\n",
    "classes_intersect = get_intersect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGxCAYAAAB89YyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIiIiMjosIAhIiIio/P/AcVxUI3BuWwSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106102c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax_arr = plt.subplots(ncols=2, nrows=int(len(classes_intersect)/2))\n",
    "plt.suptitle(\"Intersection between classes\", y = 1.09, fontsize=20)\n",
    "for i in range(len(classes)):\n",
    "    ax_arr[i//2, i%2].bar(range(len(classes_intersect[i])), classes_intersect[i])\n",
    "    ax_arr[i//2, i%2].set_title(classes[i])\n",
    "    l = list(np.delete(classes[:], i))\n",
    "    l = [0] + l\n",
    "    ax_arr[i//2, i%2].set_xticklabels(l)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "\n",
    "#     review_text = BeautifulSoup(review).get_text()\n",
    "   \n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", str(review))\n",
    "   \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "  \n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_array = np.array(data)  #convert array to numpy type array\n",
    "\n",
    "data_train ,data_valid = train_test_split(data_array,test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76680, 8), (19171, 8))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805564577244 0.194435422756\n",
      "0.79170984456 0.20829015544\n",
      "0.806615776081 0.193384223919\n",
      "0.786885245902 0.213114754098\n",
      "0.80314795383 0.19685204617\n",
      "0.808353808354 0.191646191646\n"
     ]
    }
   ],
   "source": [
    "print(sum(data_train[:, 2] ==1)/sum(data.iloc[:, 2] == 1), sum(data_valid[:, 2] ==1)/sum(data.iloc[:, 2] == 1))\n",
    "print(sum(data_train[:, 3] ==1)/sum(data.iloc[:, 3] == 1), sum(data_valid[:, 3] ==1)/sum(data.iloc[:, 3] == 1))\n",
    "print(sum(data_train[:, 4] ==1)/sum(data.iloc[:, 4] == 1), sum(data_valid[:, 4] ==1)/sum(data.iloc[:, 4] == 1))\n",
    "print(sum(data_train[:, 5] ==1)/sum(data.iloc[:, 5] == 1), sum(data_valid[:, 5] ==1)/sum(data.iloc[:, 5] == 1))\n",
    "print(sum(data_train[:, 6] ==1)/sum(data.iloc[:, 6] == 1), sum(data_valid[:, 6] ==1)/sum(data.iloc[:, 6] == 1))\n",
    "print(sum(data_train[:, 7] ==1)/sum(data.iloc[:, 7] == 1), sum(data_valid[:, 7] ==1)/sum(data.iloc[:, 7] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = [] \n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in data_train[:, 1]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in data_valid[:, 1]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424423"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'was', 'black', 'italic', 'text']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'people',\n",
       " 'places',\n",
       " 'or',\n",
       " 'things',\n",
       " 'you',\n",
       " 'have',\n",
       " 'written',\n",
       " 'about',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'sufficiently',\n",
       " 'well',\n",
       " 'known',\n",
       " 'to',\n",
       " 'merit',\n",
       " 'articles',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-23 16:13:01,250 : INFO : collecting all words and their counts\n",
      "2017-12-23 16:13:01,251 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-23 16:13:01,289 : INFO : PROGRESS: at sentence #10000, processed 153065 words, keeping 13685 word types\n",
      "2017-12-23 16:13:01,332 : INFO : PROGRESS: at sentence #20000, processed 306679 words, keeping 20907 word types\n",
      "2017-12-23 16:13:01,374 : INFO : PROGRESS: at sentence #30000, processed 450862 words, keeping 26336 word types\n",
      "2017-12-23 16:13:01,416 : INFO : PROGRESS: at sentence #40000, processed 608277 words, keeping 31266 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-23 16:13:01,460 : INFO : PROGRESS: at sentence #50000, processed 764851 words, keeping 35642 word types\n",
      "2017-12-23 16:13:01,502 : INFO : PROGRESS: at sentence #60000, processed 920122 words, keeping 39662 word types\n",
      "2017-12-23 16:13:01,546 : INFO : PROGRESS: at sentence #70000, processed 1074872 words, keeping 43542 word types\n",
      "2017-12-23 16:13:01,592 : INFO : PROGRESS: at sentence #80000, processed 1231415 words, keeping 47522 word types\n",
      "2017-12-23 16:13:01,634 : INFO : PROGRESS: at sentence #90000, processed 1383058 words, keeping 50850 word types\n",
      "2017-12-23 16:13:01,684 : INFO : PROGRESS: at sentence #100000, processed 1542999 words, keeping 54252 word types\n",
      "2017-12-23 16:13:01,729 : INFO : PROGRESS: at sentence #110000, processed 1694363 words, keeping 57417 word types\n",
      "2017-12-23 16:13:01,769 : INFO : PROGRESS: at sentence #120000, processed 1835603 words, keeping 60246 word types\n",
      "2017-12-23 16:13:01,811 : INFO : PROGRESS: at sentence #130000, processed 1988153 words, keeping 63246 word types\n",
      "2017-12-23 16:13:01,853 : INFO : PROGRESS: at sentence #140000, processed 2151108 words, keeping 66133 word types\n",
      "2017-12-23 16:13:01,897 : INFO : PROGRESS: at sentence #150000, processed 2302666 words, keeping 68833 word types\n",
      "2017-12-23 16:13:01,943 : INFO : PROGRESS: at sentence #160000, processed 2461019 words, keeping 71672 word types\n",
      "2017-12-23 16:13:01,986 : INFO : PROGRESS: at sentence #170000, processed 2617619 words, keeping 74346 word types\n",
      "2017-12-23 16:13:02,024 : INFO : PROGRESS: at sentence #180000, processed 2770524 words, keeping 76593 word types\n",
      "2017-12-23 16:13:02,064 : INFO : PROGRESS: at sentence #190000, processed 2923561 words, keeping 79056 word types\n",
      "2017-12-23 16:13:02,103 : INFO : PROGRESS: at sentence #200000, processed 3065741 words, keeping 81126 word types\n",
      "2017-12-23 16:13:02,146 : INFO : PROGRESS: at sentence #210000, processed 3221383 words, keeping 83513 word types\n",
      "2017-12-23 16:13:02,188 : INFO : PROGRESS: at sentence #220000, processed 3379876 words, keeping 85896 word types\n",
      "2017-12-23 16:13:02,242 : INFO : PROGRESS: at sentence #230000, processed 3539754 words, keeping 88155 word types\n",
      "2017-12-23 16:13:02,286 : INFO : PROGRESS: at sentence #240000, processed 3700945 words, keeping 90356 word types\n",
      "2017-12-23 16:13:02,324 : INFO : PROGRESS: at sentence #250000, processed 3852368 words, keeping 92461 word types\n",
      "2017-12-23 16:13:02,366 : INFO : PROGRESS: at sentence #260000, processed 4005051 words, keeping 94492 word types\n",
      "2017-12-23 16:13:02,409 : INFO : PROGRESS: at sentence #270000, processed 4162525 words, keeping 96519 word types\n",
      "2017-12-23 16:13:02,452 : INFO : PROGRESS: at sentence #280000, processed 4317126 words, keeping 98581 word types\n",
      "2017-12-23 16:13:02,493 : INFO : PROGRESS: at sentence #290000, processed 4472537 words, keeping 100601 word types\n",
      "2017-12-23 16:13:02,533 : INFO : PROGRESS: at sentence #300000, processed 4628607 words, keeping 102678 word types\n",
      "2017-12-23 16:13:02,575 : INFO : PROGRESS: at sentence #310000, processed 4783072 words, keeping 104696 word types\n",
      "2017-12-23 16:13:02,619 : INFO : PROGRESS: at sentence #320000, processed 4941568 words, keeping 106610 word types\n",
      "2017-12-23 16:13:02,663 : INFO : PROGRESS: at sentence #330000, processed 5096088 words, keeping 108514 word types\n",
      "2017-12-23 16:13:02,706 : INFO : PROGRESS: at sentence #340000, processed 5253360 words, keeping 110461 word types\n",
      "2017-12-23 16:13:02,749 : INFO : PROGRESS: at sentence #350000, processed 5410114 words, keeping 112279 word types\n",
      "2017-12-23 16:13:02,792 : INFO : PROGRESS: at sentence #360000, processed 5560839 words, keeping 113957 word types\n",
      "2017-12-23 16:13:02,834 : INFO : PROGRESS: at sentence #370000, processed 5709270 words, keeping 115676 word types\n",
      "2017-12-23 16:13:02,878 : INFO : PROGRESS: at sentence #380000, processed 5866310 words, keeping 117633 word types\n",
      "2017-12-23 16:13:02,920 : INFO : PROGRESS: at sentence #390000, processed 6015668 words, keeping 119317 word types\n",
      "2017-12-23 16:13:02,966 : INFO : PROGRESS: at sentence #400000, processed 6172184 words, keeping 121157 word types\n",
      "2017-12-23 16:13:03,009 : INFO : PROGRESS: at sentence #410000, processed 6326731 words, keeping 122939 word types\n",
      "2017-12-23 16:13:03,052 : INFO : PROGRESS: at sentence #420000, processed 6478114 words, keeping 124627 word types\n",
      "2017-12-23 16:13:03,070 : INFO : collected 125512 word types from a corpus of 6547496 raw words and 424423 sentences\n",
      "2017-12-23 16:13:03,071 : INFO : Loading a fresh vocabulary\n",
      "2017-12-23 16:13:03,169 : INFO : min_count=40 retains 7928 unique words (6% of original 125512, drops 117584)\n",
      "2017-12-23 16:13:03,170 : INFO : min_count=40 leaves 6107460 word corpus (93% of original 6547496, drops 440036)\n",
      "2017-12-23 16:13:03,202 : INFO : deleting the raw counts dictionary of 125512 items\n",
      "2017-12-23 16:13:03,208 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2017-12-23 16:13:03,209 : INFO : downsampling leaves estimated 4491549 word corpus (73.5% of prior 6107460)\n",
      "2017-12-23 16:13:03,209 : INFO : estimated required memory for 7928 words and 300 dimensions: 22991200 bytes\n",
      "2017-12-23 16:13:03,238 : INFO : resetting layer weights\n",
      "2017-12-23 16:13:03,378 : INFO : training model with 4 workers on 7928 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-23 16:13:04,393 : INFO : PROGRESS: at 5.10% examples, 1137903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:05,401 : INFO : PROGRESS: at 9.99% examples, 1108937 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:06,406 : INFO : PROGRESS: at 14.99% examples, 1116655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:07,409 : INFO : PROGRESS: at 20.37% examples, 1137545 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:08,415 : INFO : PROGRESS: at 25.99% examples, 1158791 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:09,420 : INFO : PROGRESS: at 31.34% examples, 1167115 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:10,421 : INFO : PROGRESS: at 36.60% examples, 1170307 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:11,422 : INFO : PROGRESS: at 41.88% examples, 1169881 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:12,423 : INFO : PROGRESS: at 47.40% examples, 1177519 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:13,428 : INFO : PROGRESS: at 52.91% examples, 1183332 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:14,431 : INFO : PROGRESS: at 58.47% examples, 1189115 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:15,431 : INFO : PROGRESS: at 64.00% examples, 1193199 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:16,431 : INFO : PROGRESS: at 69.57% examples, 1196784 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:17,433 : INFO : PROGRESS: at 75.02% examples, 1199756 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:18,435 : INFO : PROGRESS: at 80.43% examples, 1200300 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:19,446 : INFO : PROGRESS: at 85.07% examples, 1189523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:20,451 : INFO : PROGRESS: at 90.41% examples, 1189306 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:21,452 : INFO : PROGRESS: at 95.74% examples, 1190570 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-23 16:13:22,234 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-23 16:13:22,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-23 16:13:22,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-23 16:13:22,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-23 16:13:22,249 : INFO : training on 32737480 raw words (22457100 effective words) took 18.9s, 1190687 effective words/s\n",
      "2017-12-23 16:13:22,249 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-12-23 16:13:22,326 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-12-23 16:13:22,326 : INFO : not storing attribute syn0norm\n",
      "2017-12-23 16:13:22,327 : INFO : not storing attribute cum_table\n",
      "2017-12-23 16:13:22,570 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7928,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0_lockf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00755183, -0.0237998 , -0.119234  ,  0.04472408, -0.05962428,\n",
       "        0.00220911,  0.02842643, -0.0110237 , -0.06125546,  0.03542334,\n",
       "       -0.01479588,  0.03279963, -0.05294646, -0.07037295,  0.05067915,\n",
       "        0.10604518, -0.03859404,  0.08926969,  0.01185556, -0.05778293,\n",
       "       -0.07302342, -0.07048602, -0.06215798, -0.05763784,  0.06365775,\n",
       "        0.11505347, -0.09744398, -0.0609575 , -0.03519806,  0.02786094,\n",
       "        0.03359922, -0.02064945, -0.00982771, -0.04410329,  0.02345378,\n",
       "        0.0102284 ,  0.05238913,  0.03977534,  0.02010577,  0.01410025,\n",
       "        0.0609512 , -0.05892008, -0.00663022,  0.05530287,  0.09666941,\n",
       "        0.05683028, -0.03143321, -0.04705799,  0.0563247 ,  0.10170096,\n",
       "        0.04170763, -0.0310665 , -0.04988116, -0.04965904,  0.06281916,\n",
       "        0.04297115,  0.16508695,  0.04473774,  0.07236359,  0.02479101,\n",
       "       -0.00616275, -0.08847532, -0.07477806, -0.00678895, -0.07413031,\n",
       "        0.03245664,  0.04418996,  0.00558636,  0.07073478, -0.11068749,\n",
       "        0.00682621,  0.05215376, -0.02921668,  0.08871403, -0.03573134,\n",
       "        0.0304288 ,  0.04301678, -0.06395606, -0.11429285, -0.09702389,\n",
       "       -0.12331937,  0.11588367,  0.01625533,  0.01453651, -0.03113631,\n",
       "       -0.15168712,  0.1127726 , -0.00053659, -0.05534183, -0.09086359,\n",
       "        0.04968588,  0.11273529,  0.00507345, -0.07046519,  0.01779279,\n",
       "       -0.07867304, -0.11921126, -0.00972809,  0.05614664, -0.03759543,\n",
       "        0.04956903,  0.08528537,  0.01792188,  0.01080233,  0.09354152,\n",
       "       -0.07599717,  0.0280165 ,  0.00920863, -0.03291697,  0.02125178,\n",
       "       -0.02613156, -0.05964653, -0.11155182,  0.14352556,  0.01090941,\n",
       "       -0.05925932,  0.02086073, -0.04133775, -0.06815561, -0.06136861,\n",
       "        0.02214629, -0.0976987 , -0.15598148,  0.05240128,  0.00841177,\n",
       "       -0.02325407, -0.02749071,  0.10355533,  0.11266981,  0.02146333,\n",
       "       -0.02958493,  0.04575199, -0.00684597,  0.01063512, -0.0734456 ,\n",
       "        0.01783392, -0.0888539 ,  0.01408351, -0.05877089, -0.00973003,\n",
       "        0.01109672,  0.04406355,  0.08174731, -0.06042152,  0.02558856,\n",
       "       -0.0241852 ,  0.01758832,  0.04130918, -0.01581442,  0.07784802,\n",
       "        0.05716455, -0.02197406,  0.00091289, -0.02843523,  0.04201981,\n",
       "       -0.01893538,  0.08046964, -0.0384075 ,  0.04186311, -0.05815169,\n",
       "       -0.0549432 , -0.01808084,  0.00098523, -0.07846205, -0.05431886,\n",
       "        0.07090009, -0.05596636,  0.07904267,  0.01618693, -0.0270777 ,\n",
       "        0.04593673,  0.086836  , -0.07727469,  0.00834349, -0.06656614,\n",
       "       -0.0614468 ,  0.0444766 , -0.00745112, -0.10962148, -0.0099417 ,\n",
       "       -0.04001387,  0.00404748,  0.06913853, -0.03828475, -0.01699632,\n",
       "       -0.0137101 ,  0.01753058, -0.01184999,  0.07774465,  0.04916317,\n",
       "        0.09057753,  0.01991751, -0.05493913, -0.00753488,  0.09115074,\n",
       "       -0.00748944,  0.08369146,  0.10053917,  0.01960269, -0.02435903,\n",
       "        0.01326547, -0.00073907,  0.01591381,  0.0416248 , -0.01261949,\n",
       "       -0.06228442,  0.02233315,  0.01301221, -0.06203975,  0.0159591 ,\n",
       "       -0.02073364,  0.05920785, -0.04476738, -0.02194248, -0.02044011,\n",
       "        0.05765492, -0.0640716 ,  0.05352771,  0.04671062, -0.01763538,\n",
       "       -0.06643171,  0.01952421, -0.04855463, -0.03248491, -0.10883823,\n",
       "        0.00031896,  0.0694168 ,  0.02948194,  0.08736611, -0.08024501,\n",
       "        0.01577924,  0.01982914,  0.01364853,  0.02160713,  0.11381659,\n",
       "        0.1082384 , -0.0240143 ,  0.00054276, -0.04210394,  0.11507818,\n",
       "       -0.02289706,  0.07699366, -0.07511517,  0.08875223, -0.01055016,\n",
       "       -0.00202852, -0.0366902 ,  0.05431854, -0.09793179, -0.01928309,\n",
       "       -0.087074  , -0.04212596,  0.02077562, -0.0317861 ,  0.02217254,\n",
       "       -0.04202659, -0.01351254,  0.03121911, -0.03251673,  0.04230381,\n",
       "        0.02223732,  0.01693099, -0.05356165, -0.04640583,  0.00684508,\n",
       "       -0.00346405, -0.00072913,  0.00548553, -0.00702689, -0.04747998,\n",
       "        0.07607067,  0.13427986,  0.05393824, -0.00316284,  0.05868043,\n",
       "       -0.02618558, -0.00839677,  0.03521826, -0.08087728,  0.00610351,\n",
       "       -0.06219931, -0.07252719,  0.01626681, -0.08012028,  0.05003257,\n",
       "       -0.00399794,  0.0041014 ,  0.00586424,  0.0641365 , -0.00607103,\n",
       "       -0.02562262, -0.00801974, -0.02216476,  0.01497286,  0.02696903,\n",
       "        0.08197463, -0.078381  ,  0.05744243,  0.02520816, -0.10104991], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('300features_40minwords_10context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    \n",
    "    nwords = 0.\n",
    " \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "   \n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "  \n",
    "    counter = 0\n",
    "  \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        \n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "      \n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 76680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 76680\n",
      "Review 2000 of 76680\n",
      "Review 3000 of 76680\n",
      "Review 4000 of 76680\n",
      "Review 5000 of 76680\n",
      "Review 6000 of 76680\n",
      "Review 7000 of 76680\n",
      "Review 8000 of 76680\n",
      "Review 9000 of 76680\n",
      "Review 10000 of 76680\n",
      "Review 11000 of 76680\n",
      "Review 12000 of 76680\n",
      "Review 13000 of 76680\n",
      "Review 14000 of 76680\n",
      "Review 15000 of 76680\n",
      "Review 16000 of 76680\n",
      "Review 17000 of 76680\n",
      "Review 18000 of 76680\n",
      "Review 19000 of 76680\n",
      "Review 20000 of 76680\n",
      "Review 21000 of 76680\n",
      "Review 22000 of 76680\n",
      "Review 23000 of 76680\n",
      "Review 24000 of 76680\n",
      "Review 25000 of 76680\n",
      "Review 26000 of 76680\n",
      "Review 27000 of 76680\n",
      "Review 28000 of 76680\n",
      "Review 29000 of 76680\n",
      "Review 30000 of 76680\n",
      "Review 31000 of 76680\n",
      "Review 32000 of 76680\n",
      "Review 33000 of 76680\n",
      "Review 34000 of 76680\n",
      "Review 35000 of 76680\n",
      "Review 36000 of 76680\n",
      "Review 37000 of 76680\n",
      "Review 38000 of 76680\n",
      "Review 39000 of 76680\n",
      "Review 40000 of 76680\n",
      "Review 41000 of 76680\n",
      "Review 42000 of 76680\n",
      "Review 43000 of 76680\n",
      "Review 44000 of 76680\n",
      "Review 45000 of 76680\n",
      "Review 46000 of 76680\n",
      "Review 47000 of 76680\n",
      "Review 48000 of 76680\n",
      "Review 49000 of 76680\n",
      "Review 50000 of 76680\n",
      "Review 51000 of 76680\n",
      "Review 52000 of 76680\n",
      "Review 53000 of 76680\n",
      "Review 54000 of 76680\n",
      "Review 55000 of 76680\n",
      "Review 56000 of 76680\n",
      "Review 57000 of 76680\n",
      "Review 58000 of 76680\n",
      "Review 59000 of 76680\n",
      "Review 60000 of 76680\n",
      "Review 61000 of 76680\n",
      "Review 62000 of 76680\n",
      "Review 63000 of 76680\n",
      "Review 64000 of 76680\n",
      "Review 65000 of 76680\n",
      "Review 66000 of 76680\n",
      "Review 67000 of 76680\n",
      "Review 68000 of 76680\n",
      "Review 69000 of 76680\n",
      "Review 70000 of 76680\n",
      "Review 71000 of 76680\n",
      "Review 72000 of 76680\n",
      "Review 73000 of 76680\n",
      "Review 74000 of 76680\n",
      "Review 75000 of 76680\n",
      "Review 76000 of 76680\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 19171\n",
      "Review 1000 of 19171\n",
      "Review 2000 of 19171\n",
      "Review 3000 of 19171\n",
      "Review 4000 of 19171\n",
      "Review 5000 of 19171\n",
      "Review 6000 of 19171\n",
      "Review 7000 of 19171\n",
      "Review 8000 of 19171\n",
      "Review 9000 of 19171\n",
      "Review 10000 of 19171\n",
      "Review 11000 of 19171\n",
      "Review 12000 of 19171\n",
      "Review 13000 of 19171\n",
      "Review 14000 of 19171\n",
      "Review 15000 of 19171\n",
      "Review 16000 of 19171\n",
      "Review 17000 of 19171\n",
      "Review 18000 of 19171\n",
      "Review 19000 of 19171\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in data_train[:, 1]:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_valid_reviews = []\n",
    "for review in data_valid[:, 1]:\n",
    "    clean_valid_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "\n",
    "validDataVecs = getAvgFeatureVecs( clean_valid_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataVecs[np.isnan(trainDataVecs)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(validDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validDataVecs[np.isnan(validDataVecs)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_data.iloc[:, 1]=='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 226998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 226998\n",
      "Review 2000 of 226998\n",
      "Review 3000 of 226998\n",
      "Review 4000 of 226998\n",
      "Review 5000 of 226998\n",
      "Review 6000 of 226998\n",
      "Review 7000 of 226998\n",
      "Review 8000 of 226998\n",
      "Review 9000 of 226998\n",
      "Review 10000 of 226998\n",
      "Review 11000 of 226998\n",
      "Review 12000 of 226998\n",
      "Review 13000 of 226998\n",
      "Review 14000 of 226998\n",
      "Review 15000 of 226998\n",
      "Review 16000 of 226998\n",
      "Review 17000 of 226998\n",
      "Review 18000 of 226998\n",
      "Review 19000 of 226998\n",
      "Review 20000 of 226998\n",
      "Review 21000 of 226998\n",
      "Review 22000 of 226998\n",
      "Review 23000 of 226998\n",
      "Review 24000 of 226998\n",
      "Review 25000 of 226998\n",
      "Review 26000 of 226998\n",
      "Review 27000 of 226998\n",
      "Review 28000 of 226998\n",
      "Review 29000 of 226998\n",
      "Review 30000 of 226998\n",
      "Review 31000 of 226998\n",
      "Review 32000 of 226998\n",
      "Review 33000 of 226998\n",
      "Review 34000 of 226998\n",
      "Review 35000 of 226998\n",
      "Review 36000 of 226998\n",
      "Review 37000 of 226998\n",
      "Review 38000 of 226998\n",
      "Review 39000 of 226998\n",
      "Review 40000 of 226998\n",
      "Review 41000 of 226998\n",
      "Review 42000 of 226998\n",
      "Review 43000 of 226998\n",
      "Review 44000 of 226998\n",
      "Review 45000 of 226998\n",
      "Review 46000 of 226998\n",
      "Review 47000 of 226998\n",
      "Review 48000 of 226998\n",
      "Review 49000 of 226998\n",
      "Review 50000 of 226998\n",
      "Review 51000 of 226998\n",
      "Review 52000 of 226998\n",
      "Review 53000 of 226998\n",
      "Review 54000 of 226998\n",
      "Review 55000 of 226998\n",
      "Review 56000 of 226998\n",
      "Review 57000 of 226998\n",
      "Review 58000 of 226998\n",
      "Review 59000 of 226998\n",
      "Review 60000 of 226998\n",
      "Review 61000 of 226998\n",
      "Review 62000 of 226998\n",
      "Review 63000 of 226998\n",
      "Review 64000 of 226998\n",
      "Review 65000 of 226998\n",
      "Review 66000 of 226998\n",
      "Review 67000 of 226998\n",
      "Review 68000 of 226998\n",
      "Review 69000 of 226998\n",
      "Review 70000 of 226998\n",
      "Review 71000 of 226998\n",
      "Review 72000 of 226998\n",
      "Review 73000 of 226998\n",
      "Review 74000 of 226998\n",
      "Review 75000 of 226998\n",
      "Review 76000 of 226998\n",
      "Review 77000 of 226998\n",
      "Review 78000 of 226998\n",
      "Review 79000 of 226998\n",
      "Review 80000 of 226998\n",
      "Review 81000 of 226998\n",
      "Review 82000 of 226998\n",
      "Review 83000 of 226998\n",
      "Review 84000 of 226998\n",
      "Review 85000 of 226998\n",
      "Review 86000 of 226998\n",
      "Review 87000 of 226998\n",
      "Review 88000 of 226998\n",
      "Review 89000 of 226998\n",
      "Review 90000 of 226998\n",
      "Review 91000 of 226998\n",
      "Review 92000 of 226998\n",
      "Review 93000 of 226998\n",
      "Review 94000 of 226998\n",
      "Review 95000 of 226998\n",
      "Review 96000 of 226998\n",
      "Review 97000 of 226998\n",
      "Review 98000 of 226998\n",
      "Review 99000 of 226998\n",
      "Review 100000 of 226998\n",
      "Review 101000 of 226998\n",
      "Review 102000 of 226998\n",
      "Review 103000 of 226998\n",
      "Review 104000 of 226998\n",
      "Review 105000 of 226998\n",
      "Review 106000 of 226998\n",
      "Review 107000 of 226998\n",
      "Review 108000 of 226998\n",
      "Review 109000 of 226998\n",
      "Review 110000 of 226998\n",
      "Review 111000 of 226998\n",
      "Review 112000 of 226998\n",
      "Review 113000 of 226998\n",
      "Review 114000 of 226998\n",
      "Review 115000 of 226998\n",
      "Review 116000 of 226998\n",
      "Review 117000 of 226998\n",
      "Review 118000 of 226998\n",
      "Review 119000 of 226998\n",
      "Review 120000 of 226998\n",
      "Review 121000 of 226998\n",
      "Review 122000 of 226998\n",
      "Review 123000 of 226998\n",
      "Review 124000 of 226998\n",
      "Review 125000 of 226998\n",
      "Review 126000 of 226998\n",
      "Review 127000 of 226998\n",
      "Review 128000 of 226998\n",
      "Review 129000 of 226998\n",
      "Review 130000 of 226998\n",
      "Review 131000 of 226998\n",
      "Review 132000 of 226998\n",
      "Review 133000 of 226998\n",
      "Review 134000 of 226998\n",
      "Review 135000 of 226998\n",
      "Review 136000 of 226998\n",
      "Review 137000 of 226998\n",
      "Review 138000 of 226998\n",
      "Review 139000 of 226998\n",
      "Review 140000 of 226998\n",
      "Review 141000 of 226998\n",
      "Review 142000 of 226998\n",
      "Review 143000 of 226998\n",
      "Review 144000 of 226998\n",
      "Review 145000 of 226998\n",
      "Review 146000 of 226998\n",
      "Review 147000 of 226998\n",
      "Review 148000 of 226998\n",
      "Review 149000 of 226998\n",
      "Review 150000 of 226998\n",
      "Review 151000 of 226998\n",
      "Review 152000 of 226998\n",
      "Review 153000 of 226998\n",
      "Review 154000 of 226998\n",
      "Review 155000 of 226998\n",
      "Review 156000 of 226998\n",
      "Review 157000 of 226998\n",
      "Review 158000 of 226998\n",
      "Review 159000 of 226998\n",
      "Review 160000 of 226998\n",
      "Review 161000 of 226998\n",
      "Review 162000 of 226998\n",
      "Review 163000 of 226998\n",
      "Review 164000 of 226998\n",
      "Review 165000 of 226998\n",
      "Review 166000 of 226998\n",
      "Review 167000 of 226998\n",
      "Review 168000 of 226998\n",
      "Review 169000 of 226998\n",
      "Review 170000 of 226998\n",
      "Review 171000 of 226998\n",
      "Review 172000 of 226998\n",
      "Review 173000 of 226998\n",
      "Review 174000 of 226998\n",
      "Review 175000 of 226998\n",
      "Review 176000 of 226998\n",
      "Review 177000 of 226998\n",
      "Review 178000 of 226998\n",
      "Review 179000 of 226998\n",
      "Review 180000 of 226998\n",
      "Review 181000 of 226998\n",
      "Review 182000 of 226998\n",
      "Review 183000 of 226998\n",
      "Review 184000 of 226998\n",
      "Review 185000 of 226998\n",
      "Review 186000 of 226998\n",
      "Review 187000 of 226998\n",
      "Review 188000 of 226998\n",
      "Review 189000 of 226998\n",
      "Review 190000 of 226998\n",
      "Review 191000 of 226998\n",
      "Review 192000 of 226998\n",
      "Review 193000 of 226998\n",
      "Review 194000 of 226998\n",
      "Review 195000 of 226998\n",
      "Review 196000 of 226998\n",
      "Review 197000 of 226998\n",
      "Review 198000 of 226998\n",
      "Review 199000 of 226998\n",
      "Review 200000 of 226998\n",
      "Review 201000 of 226998\n",
      "Review 202000 of 226998\n",
      "Review 203000 of 226998\n",
      "Review 204000 of 226998\n",
      "Review 205000 of 226998\n",
      "Review 206000 of 226998\n",
      "Review 207000 of 226998\n",
      "Review 208000 of 226998\n",
      "Review 209000 of 226998\n",
      "Review 210000 of 226998\n",
      "Review 211000 of 226998\n",
      "Review 212000 of 226998\n",
      "Review 213000 of 226998\n",
      "Review 214000 of 226998\n",
      "Review 215000 of 226998\n",
      "Review 216000 of 226998\n",
      "Review 217000 of 226998\n",
      "Review 218000 of 226998\n",
      "Review 219000 of 226998\n",
      "Review 220000 of 226998\n",
      "Review 221000 of 226998\n",
      "Review 222000 of 226998\n",
      "Review 223000 of 226998\n",
      "Review 224000 of 226998\n",
      "Review 225000 of 226998\n",
      "Review 226000 of 226998\n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test_data.iloc[:, 1]:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(testDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataVecs[np.isnan(testDataVecs)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(testDataVecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn_model():\n",
    "#     ad_optimizer = optimizers.Adam(clipnorm=1., lr=0.3)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=num_features, activation='tanh'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_model = build_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1987 - acc: 0.9319     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1506 - acc: 0.9430     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1472 - acc: 0.9447     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1455 - acc: 0.9452     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1443 - acc: 0.9453     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1436 - acc: 0.9457     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1430 - acc: 0.9460     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1422 - acc: 0.9460     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1414 - acc: 0.9462     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1409 - acc: 0.9467     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1400 - acc: 0.9468     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1391 - acc: 0.9472     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1382 - acc: 0.9483     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1374 - acc: 0.9484     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1367 - acc: 0.9487     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1362 - acc: 0.9490     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1358 - acc: 0.9491     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1353 - acc: 0.9490     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1349 - acc: 0.9492     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1345 - acc: 0.9494     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1342 - acc: 0.9495     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1339 - acc: 0.9495     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1337 - acc: 0.9497     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1333 - acc: 0.9499     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1328 - acc: 0.9502     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1325 - acc: 0.9504     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1322 - acc: 0.9501     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1320 - acc: 0.9502     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1315 - acc: 0.9505     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1311 - acc: 0.9507     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1311 - acc: 0.9510     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1307 - acc: 0.9508     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1304 - acc: 0.9506     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1302 - acc: 0.9509     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1298 - acc: 0.9511     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1296 - acc: 0.9510     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1295 - acc: 0.9512     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1291 - acc: 0.9513     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1289 - acc: 0.9516     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1287 - acc: 0.9518     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1284 - acc: 0.9516     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1282 - acc: 0.9516     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1280 - acc: 0.9516     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1278 - acc: 0.9519     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1276 - acc: 0.9520     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1275 - acc: 0.9523     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1273 - acc: 0.9524     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1272 - acc: 0.9521     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1270 - acc: 0.9521     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1269 - acc: 0.9525     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1267 - acc: 0.9521     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1266 - acc: 0.9524     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1263 - acc: 0.9527     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1264 - acc: 0.9526     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1262 - acc: 0.9525     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1259 - acc: 0.9525     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1259 - acc: 0.9528     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1258 - acc: 0.9529     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1255 - acc: 0.9529     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1256 - acc: 0.9526     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1253 - acc: 0.9528     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1254 - acc: 0.9532     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1252 - acc: 0.9530     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1251 - acc: 0.9530     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1249 - acc: 0.9532     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1249 - acc: 0.9529     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1248 - acc: 0.9530     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1246 - acc: 0.9528     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1245 - acc: 0.9532     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1244 - acc: 0.9535     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1243 - acc: 0.9531     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1243 - acc: 0.9535     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1243 - acc: 0.9531     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1241 - acc: 0.9533     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1240 - acc: 0.9533     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1240 - acc: 0.9529     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1238 - acc: 0.9534     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1239 - acc: 0.9537     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1237 - acc: 0.9536     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1236 - acc: 0.9538     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1234 - acc: 0.9537     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1235 - acc: 0.9534     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1234 - acc: 0.9537     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1231 - acc: 0.9537     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1232 - acc: 0.9540     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1229 - acc: 0.9538     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1230 - acc: 0.9539     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1229 - acc: 0.9540     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1230 - acc: 0.9539     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1228 - acc: 0.9540     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1228 - acc: 0.9542     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1226 - acc: 0.9539     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1228 - acc: 0.9537     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1224 - acc: 0.9544     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1225 - acc: 0.9539     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1223 - acc: 0.9541     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1224 - acc: 0.9542     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1223 - acc: 0.9541     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1222 - acc: 0.9542     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1221 - acc: 0.9544     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca0405748>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit(trainDataVecs, data_train[:, 2].astype(int), nb_epoch=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75936/76680 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12120002333889048, 0.9546948356807512]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nn_model.evaluate(trainDataVecs, data_train[:, 2].astype(int), batch_size=32)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15936/19171 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13797402020929925, 0.95075895885341477]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nn_model.evaluate(validDataVecs, data_valid[:, 2].astype(int), batch_size=64)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219104/226998 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds1 = nn_model.predict_proba(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00092868],\n",
       "       [ 0.0013055 ],\n",
       "       [ 0.00160392],\n",
       "       ..., \n",
       "       [ 0.00672257],\n",
       "       [ 0.00170642],\n",
       "       [ 0.00437022]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.isnan(preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.2191 - acc: 0.9279     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1507 - acc: 0.9436     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1475 - acc: 0.9444     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1455 - acc: 0.9450     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1443 - acc: 0.9452     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1432 - acc: 0.9458     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1425 - acc: 0.9453     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 3s - loss: 0.1419 - acc: 0.9456     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 3s - loss: 0.1415 - acc: 0.9460     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1410 - acc: 0.9461     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1405 - acc: 0.9464     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1402 - acc: 0.9466     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 3s - loss: 0.1398 - acc: 0.9465     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 3s - loss: 0.1394 - acc: 0.9468     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1390 - acc: 0.9471     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1386 - acc: 0.9474     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1379 - acc: 0.9477     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1375 - acc: 0.9481     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1371 - acc: 0.9483     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1367 - acc: 0.9487     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1362 - acc: 0.9486     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1358 - acc: 0.9485     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1356 - acc: 0.9490     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1351 - acc: 0.9489     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1347 - acc: 0.9490     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1345 - acc: 0.9495     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1342 - acc: 0.9496     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1340 - acc: 0.9498     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1335 - acc: 0.9497     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1333 - acc: 0.9499     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1329 - acc: 0.9504     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1327 - acc: 0.9501     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1324 - acc: 0.9506     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1324 - acc: 0.9505     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1322 - acc: 0.9505     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 3s - loss: 0.1318 - acc: 0.9510     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1317 - acc: 0.9511     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1314 - acc: 0.9511     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1313 - acc: 0.9513     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1311 - acc: 0.9507     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1310 - acc: 0.9512     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1310 - acc: 0.9509     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1307 - acc: 0.9512     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1306 - acc: 0.9512     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1305 - acc: 0.9513     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1303 - acc: 0.9515     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1302 - acc: 0.9515     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1301 - acc: 0.9514     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1298 - acc: 0.9520     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1297 - acc: 0.9515     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1296 - acc: 0.9516     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1294 - acc: 0.9520     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1292 - acc: 0.9519     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1292 - acc: 0.9517     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1292 - acc: 0.9518     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1289 - acc: 0.9522     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1288 - acc: 0.9520     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1289 - acc: 0.9521     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1287 - acc: 0.9522     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1286 - acc: 0.9517     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1285 - acc: 0.9522     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1284 - acc: 0.9521     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1284 - acc: 0.9519     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1282 - acc: 0.9522     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1281 - acc: 0.9522     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1278 - acc: 0.9519     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1279 - acc: 0.9524     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1279 - acc: 0.9521     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1280 - acc: 0.9522     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 2s - loss: 0.1278 - acc: 0.9522     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 1s - loss: 0.1276 - acc: 0.9525     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1276 - acc: 0.9520     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1273 - acc: 0.9525     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1274 - acc: 0.9523     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1272 - acc: 0.9528     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1272 - acc: 0.9524     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1272 - acc: 0.9525     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1269 - acc: 0.9528     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1269 - acc: 0.9527     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1269 - acc: 0.9524     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1268 - acc: 0.9526     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1268 - acc: 0.9524     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1266 - acc: 0.9526     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1266 - acc: 0.9528     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1265 - acc: 0.9529     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1265 - acc: 0.9525     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1264 - acc: 0.9528     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1263 - acc: 0.9529     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1264 - acc: 0.9525     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1264 - acc: 0.9526     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1260 - acc: 0.9528     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1261 - acc: 0.9530     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1260 - acc: 0.9530     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1261 - acc: 0.9527     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1260 - acc: 0.9526     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1259 - acc: 0.9531     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1257 - acc: 0.9529     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1259 - acc: 0.9524     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1257 - acc: 0.9531     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1258 - acc: 0.9529     \n",
      "220928/226998 [============================>.] - ETA: 0sEpoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0696 - acc: 0.9900     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0293 - acc: 0.9900     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0277 - acc: 0.9900     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0272 - acc: 0.9898     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0268 - acc: 0.9897     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0266 - acc: 0.9898     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0264 - acc: 0.9899     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0263 - acc: 0.9898     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0261 - acc: 0.9899     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0260 - acc: 0.9898     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0259 - acc: 0.9900     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0257 - acc: 0.9900     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0257 - acc: 0.9899     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0255 - acc: 0.9902     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0255 - acc: 0.9899     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0254 - acc: 0.9899     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0253 - acc: 0.9901     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0252 - acc: 0.9901     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0251 - acc: 0.9900     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0250 - acc: 0.9900     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0249 - acc: 0.9899     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0249 - acc: 0.9901     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0247 - acc: 0.9900     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0247 - acc: 0.9899     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0246 - acc: 0.9901     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0244 - acc: 0.9901     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0243 - acc: 0.9901     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0243 - acc: 0.9901     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0242 - acc: 0.9900     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0242 - acc: 0.9900     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0241 - acc: 0.9901     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0239 - acc: 0.9903     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0240 - acc: 0.9903     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0238 - acc: 0.9900     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0238 - acc: 0.9901     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0237 - acc: 0.9902     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0236 - acc: 0.9905     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0236 - acc: 0.9902     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0235 - acc: 0.9903     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0234 - acc: 0.9905     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0234 - acc: 0.9904     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0233 - acc: 0.9904     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0232 - acc: 0.9903     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0231 - acc: 0.9903     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0231 - acc: 0.9903     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0231 - acc: 0.9904     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0230 - acc: 0.9906     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0229 - acc: 0.9904     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0228 - acc: 0.9907     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0228 - acc: 0.9905     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0228 - acc: 0.9905     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0226 - acc: 0.9905     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0226 - acc: 0.9906     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0225 - acc: 0.9906     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0225 - acc: 0.9905     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0223 - acc: 0.9907     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0222 - acc: 0.9906     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0222 - acc: 0.9906     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0221 - acc: 0.9908     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0220 - acc: 0.9907     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0220 - acc: 0.9908     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0219 - acc: 0.9908     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0218 - acc: 0.9909     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0217 - acc: 0.9908     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0217 - acc: 0.9908     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0216 - acc: 0.9909     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0215 - acc: 0.9909     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0214 - acc: 0.9908     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0213 - acc: 0.9911     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0213 - acc: 0.9912     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0211 - acc: 0.9910     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0212 - acc: 0.9911     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0211 - acc: 0.9909     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0211 - acc: 0.9911     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0210 - acc: 0.9912     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0209 - acc: 0.9911     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0208 - acc: 0.9911     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0208 - acc: 0.9912     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0207 - acc: 0.9913     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0206 - acc: 0.9914     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0205 - acc: 0.9913     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0206 - acc: 0.9911     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0205 - acc: 0.9912     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0203 - acc: 0.9912     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0203 - acc: 0.9912     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0202 - acc: 0.9914     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0202 - acc: 0.9913     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0202 - acc: 0.9913     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0200 - acc: 0.9914     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0200 - acc: 0.9914     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0199 - acc: 0.9912     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0199 - acc: 0.9915     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0198 - acc: 0.9915     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0198 - acc: 0.9915     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0197 - acc: 0.9916     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0197 - acc: 0.9916     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0196 - acc: 0.9915     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0196 - acc: 0.9917     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0195 - acc: 0.9915     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0194 - acc: 0.9915     \n",
      "201792/226998 [=========================>....] - ETA: 0sEpoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1552 - acc: 0.9567     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0932 - acc: 0.9673     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0887 - acc: 0.9687     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0870 - acc: 0.9689     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0859 - acc: 0.9691     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0851 - acc: 0.9693     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0844 - acc: 0.9698     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0841 - acc: 0.9700     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0837 - acc: 0.9698     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0834 - acc: 0.9703     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0830 - acc: 0.9702     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0828 - acc: 0.9704     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0825 - acc: 0.9707     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0823 - acc: 0.9709     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0822 - acc: 0.9708     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0820 - acc: 0.9706     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0818 - acc: 0.9710     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0815 - acc: 0.9712     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0813 - acc: 0.9711     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0812 - acc: 0.9711     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0809 - acc: 0.9713     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0807 - acc: 0.9711     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0805 - acc: 0.9717     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0803 - acc: 0.9714     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0799 - acc: 0.9715     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0795 - acc: 0.9715     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0792 - acc: 0.9720     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0788 - acc: 0.9722     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0784 - acc: 0.9719     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0780 - acc: 0.9720     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0776 - acc: 0.9724     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0772 - acc: 0.9724     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0770 - acc: 0.9725     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0767 - acc: 0.9727     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0762 - acc: 0.9729     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0759 - acc: 0.9730     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0757 - acc: 0.9729     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0754 - acc: 0.9733     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0751 - acc: 0.9731     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0748 - acc: 0.9735     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0746 - acc: 0.9735     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0742 - acc: 0.9732     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0740 - acc: 0.9736     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0738 - acc: 0.9736     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0736 - acc: 0.9737     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0733 - acc: 0.9738     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0732 - acc: 0.9741     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0731 - acc: 0.9736     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0727 - acc: 0.9742     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0727 - acc: 0.9741     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0725 - acc: 0.9739     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0725 - acc: 0.9741     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0721 - acc: 0.9742     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0720 - acc: 0.9744     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0720 - acc: 0.9746     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0719 - acc: 0.9746     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0717 - acc: 0.9744     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0715 - acc: 0.9746     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0714 - acc: 0.9749     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0713 - acc: 0.9746     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0710 - acc: 0.9748     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0711 - acc: 0.9748     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0709 - acc: 0.9750     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0706 - acc: 0.9752     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0708 - acc: 0.9751     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0707 - acc: 0.9751     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0704 - acc: 0.9752     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0705 - acc: 0.9752     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0704 - acc: 0.9752     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0702 - acc: 0.9753     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0702 - acc: 0.9749     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0700 - acc: 0.9755     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0698 - acc: 0.9756     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0698 - acc: 0.9752     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0698 - acc: 0.9756     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0697 - acc: 0.9755     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0696 - acc: 0.9755     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0696 - acc: 0.9752     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0695 - acc: 0.9756     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0692 - acc: 0.9755     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0693 - acc: 0.9756     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0693 - acc: 0.9754     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0693 - acc: 0.9756     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0692 - acc: 0.9753     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0690 - acc: 0.9757     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0691 - acc: 0.9754     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0688 - acc: 0.9758     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0689 - acc: 0.9759     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0687 - acc: 0.9755     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0688 - acc: 0.9758     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0686 - acc: 0.9756     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0685 - acc: 0.9759     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0686 - acc: 0.9760     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0685 - acc: 0.9761     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0684 - acc: 0.9757     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0683 - acc: 0.9760     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0684 - acc: 0.9759     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0684 - acc: 0.9760     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0682 - acc: 0.9761     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0680 - acc: 0.9760     \n",
      "205440/226998 [==========================>...] - ETA: 0sEpoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0594 - acc: 0.9954     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0147 - acc: 0.9969     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0136 - acc: 0.9969     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0131 - acc: 0.9969     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0127 - acc: 0.9969     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0124 - acc: 0.9969     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0122 - acc: 0.9969     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0121 - acc: 0.9969     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0119 - acc: 0.9969     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0117 - acc: 0.9969     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0116 - acc: 0.9969     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0114 - acc: 0.9969     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0113 - acc: 0.9969     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0111 - acc: 0.9969     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0109 - acc: 0.9969     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0108 - acc: 0.9969     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0107 - acc: 0.9969     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0106 - acc: 0.9969     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0106 - acc: 0.9969     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0104 - acc: 0.9969     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0103 - acc: 0.9969     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0103 - acc: 0.9969     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0102 - acc: 0.9969     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0101 - acc: 0.9969     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0100 - acc: 0.9970     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0100 - acc: 0.9969     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0099 - acc: 0.9970     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0098 - acc: 0.9970     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0098 - acc: 0.9970     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0097 - acc: 0.9971     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0096 - acc: 0.9970     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0096 - acc: 0.9970     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0095 - acc: 0.9971     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0095 - acc: 0.9972     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0094 - acc: 0.9972     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0093 - acc: 0.9972     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0093 - acc: 0.9972     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0092 - acc: 0.9973     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0091 - acc: 0.9972     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0091 - acc: 0.9973     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0090 - acc: 0.9973     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0090 - acc: 0.9974     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0088 - acc: 0.9974     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0088 - acc: 0.9974     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0087 - acc: 0.9975     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0087 - acc: 0.9976     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0086 - acc: 0.9975     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0086 - acc: 0.9975     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0085 - acc: 0.9975     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0083 - acc: 0.9976     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0083 - acc: 0.9976     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0083 - acc: 0.9976     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0082 - acc: 0.9976     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0081 - acc: 0.9976     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0081 - acc: 0.9975     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0080 - acc: 0.9977     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0079 - acc: 0.9977     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0079 - acc: 0.9977     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0078 - acc: 0.9977     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0077 - acc: 0.9978     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0076 - acc: 0.9976     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0076 - acc: 0.9979     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0075 - acc: 0.9977     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0075 - acc: 0.9979     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0073 - acc: 0.9978     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0073 - acc: 0.9979     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0073 - acc: 0.9980     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0072 - acc: 0.9979     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0071 - acc: 0.9979     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0070 - acc: 0.9980     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0070 - acc: 0.9980     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0069 - acc: 0.9978     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0069 - acc: 0.9979     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0069 - acc: 0.9980     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0067 - acc: 0.9980     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0067 - acc: 0.9979     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0067 - acc: 0.9980     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0066 - acc: 0.9980     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0065 - acc: 0.9981     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0065 - acc: 0.9981     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0064 - acc: 0.9980     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0064 - acc: 0.9981     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0063 - acc: 0.9983     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0062 - acc: 0.9982     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0062 - acc: 0.9982     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0061 - acc: 0.9982     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0060 - acc: 0.9982     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0061 - acc: 0.9981     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0060 - acc: 0.9982     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0059 - acc: 0.9982     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0058 - acc: 0.9982     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0058 - acc: 0.9983     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0057 - acc: 0.9982     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0056 - acc: 0.9983     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0055 - acc: 0.9983     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0056 - acc: 0.9984     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0055 - acc: 0.9984     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0054 - acc: 0.9984     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0053 - acc: 0.9984     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0054 - acc: 0.9984     \n",
      "200128/226998 [=========================>....] - ETA: 0sEpoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.1409 - acc: 0.9563     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0972 - acc: 0.9629     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0938 - acc: 0.9640     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0925 - acc: 0.9649     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0916 - acc: 0.9652     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0911 - acc: 0.9655     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0904 - acc: 0.9658     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0899 - acc: 0.9660     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0896 - acc: 0.9664     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0892 - acc: 0.9664     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0889 - acc: 0.9666     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0885 - acc: 0.9670     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0880 - acc: 0.9671     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0875 - acc: 0.9672     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0868 - acc: 0.9673     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0864 - acc: 0.9678     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0858 - acc: 0.9675     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0854 - acc: 0.9680     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0849 - acc: 0.9682     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0846 - acc: 0.9683     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0843 - acc: 0.9685     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0840 - acc: 0.9688     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0837 - acc: 0.9690     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0834 - acc: 0.9692     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0831 - acc: 0.9691     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0829 - acc: 0.9691     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0825 - acc: 0.9694     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0823 - acc: 0.9694     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0821 - acc: 0.9696     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0819 - acc: 0.9697     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0817 - acc: 0.9701     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0815 - acc: 0.9698     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0813 - acc: 0.9698     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0810 - acc: 0.9703     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0810 - acc: 0.9700     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0808 - acc: 0.9702     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0807 - acc: 0.9703     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0806 - acc: 0.9703     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0804 - acc: 0.9702     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0802 - acc: 0.9704     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0800 - acc: 0.9703     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0799 - acc: 0.9708     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0797 - acc: 0.9708     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0796 - acc: 0.9709     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0796 - acc: 0.9704     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0793 - acc: 0.9710     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0792 - acc: 0.9708     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0791 - acc: 0.9709     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0790 - acc: 0.9707     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0790 - acc: 0.9711     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0788 - acc: 0.9712     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0786 - acc: 0.9711     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0786 - acc: 0.9715     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0783 - acc: 0.9713     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0783 - acc: 0.9713     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0782 - acc: 0.9714     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0780 - acc: 0.9712     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0780 - acc: 0.9714     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0780 - acc: 0.9715     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0777 - acc: 0.9714     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0776 - acc: 0.9715     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0775 - acc: 0.9715     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0774 - acc: 0.9718     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0773 - acc: 0.9717     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0771 - acc: 0.9718     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0771 - acc: 0.9716     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0770 - acc: 0.9718     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0769 - acc: 0.9717     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0768 - acc: 0.9718     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0766 - acc: 0.9718     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0767 - acc: 0.9718     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0765 - acc: 0.9718     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0764 - acc: 0.9720     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0763 - acc: 0.9718     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0764 - acc: 0.9721     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0761 - acc: 0.9722     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0761 - acc: 0.9719     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0761 - acc: 0.9719     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0761 - acc: 0.9718     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0759 - acc: 0.9722     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0758 - acc: 0.9723     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0758 - acc: 0.9722     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0756 - acc: 0.9723     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0756 - acc: 0.9722     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0755 - acc: 0.9724     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0753 - acc: 0.9723     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0753 - acc: 0.9724     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0753 - acc: 0.9725     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0752 - acc: 0.9724     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0752 - acc: 0.9724     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0751 - acc: 0.9724     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0751 - acc: 0.9722     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0750 - acc: 0.9724     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0749 - acc: 0.9723     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0748 - acc: 0.9727     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0749 - acc: 0.9725     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0748 - acc: 0.9726     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0747 - acc: 0.9725     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0746 - acc: 0.9727     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0746 - acc: 0.9726     \n",
      "206592/226998 [==========================>...] - ETA: 0sEpoch 1/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0845 - acc: 0.9901     \n",
      "Epoch 2/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0334 - acc: 0.9913     \n",
      "Epoch 3/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0314 - acc: 0.9912     \n",
      "Epoch 4/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0300 - acc: 0.9912     \n",
      "Epoch 5/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0292 - acc: 0.9912     \n",
      "Epoch 6/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0286 - acc: 0.9912     \n",
      "Epoch 7/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0281 - acc: 0.9913     \n",
      "Epoch 8/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0277 - acc: 0.9914     \n",
      "Epoch 9/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0273 - acc: 0.9915     \n",
      "Epoch 10/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0270 - acc: 0.9916     \n",
      "Epoch 11/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0268 - acc: 0.9917     \n",
      "Epoch 12/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0266 - acc: 0.9917     \n",
      "Epoch 13/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0264 - acc: 0.9918     \n",
      "Epoch 14/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0262 - acc: 0.9918     \n",
      "Epoch 15/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0261 - acc: 0.9917     \n",
      "Epoch 16/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0259 - acc: 0.9919     \n",
      "Epoch 17/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0257 - acc: 0.9919     \n",
      "Epoch 18/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0257 - acc: 0.9918     \n",
      "Epoch 19/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0254 - acc: 0.9920     \n",
      "Epoch 20/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0254 - acc: 0.9919     \n",
      "Epoch 21/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0252 - acc: 0.9920     \n",
      "Epoch 22/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0251 - acc: 0.9921     \n",
      "Epoch 23/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0249 - acc: 0.9920     \n",
      "Epoch 24/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0249 - acc: 0.9921     \n",
      "Epoch 25/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0248 - acc: 0.9921     \n",
      "Epoch 26/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0246 - acc: 0.9922     \n",
      "Epoch 27/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0245 - acc: 0.9923     \n",
      "Epoch 28/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0244 - acc: 0.9922     \n",
      "Epoch 29/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0243 - acc: 0.9923     \n",
      "Epoch 30/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0241 - acc: 0.9923     \n",
      "Epoch 31/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0240 - acc: 0.9922     \n",
      "Epoch 32/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0239 - acc: 0.9923     \n",
      "Epoch 33/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0238 - acc: 0.9923     \n",
      "Epoch 34/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0237 - acc: 0.9924     \n",
      "Epoch 35/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0236 - acc: 0.9926     \n",
      "Epoch 36/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0235 - acc: 0.9926     \n",
      "Epoch 37/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0234 - acc: 0.9925     \n",
      "Epoch 38/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0234 - acc: 0.9925     \n",
      "Epoch 39/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0232 - acc: 0.9926     \n",
      "Epoch 40/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0231 - acc: 0.9927     \n",
      "Epoch 41/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0229 - acc: 0.9926     \n",
      "Epoch 42/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0230 - acc: 0.9927     \n",
      "Epoch 43/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0229 - acc: 0.9928     \n",
      "Epoch 44/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0227 - acc: 0.9927     \n",
      "Epoch 45/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0227 - acc: 0.9926     \n",
      "Epoch 46/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0226 - acc: 0.9927     \n",
      "Epoch 47/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0227 - acc: 0.9925     \n",
      "Epoch 48/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0224 - acc: 0.9927     \n",
      "Epoch 49/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0224 - acc: 0.9929     \n",
      "Epoch 50/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0224 - acc: 0.9929     \n",
      "Epoch 51/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0223 - acc: 0.9929     \n",
      "Epoch 52/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0222 - acc: 0.9930     \n",
      "Epoch 53/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0221 - acc: 0.9928     \n",
      "Epoch 54/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0220 - acc: 0.9930     \n",
      "Epoch 55/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0219 - acc: 0.9930     \n",
      "Epoch 56/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0219 - acc: 0.9931     \n",
      "Epoch 57/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0219 - acc: 0.9929     \n",
      "Epoch 58/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0218 - acc: 0.9930     \n",
      "Epoch 59/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0216 - acc: 0.9930     \n",
      "Epoch 60/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0216 - acc: 0.9930     \n",
      "Epoch 61/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0217 - acc: 0.9929     \n",
      "Epoch 62/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0215 - acc: 0.9930     \n",
      "Epoch 63/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0214 - acc: 0.9930     \n",
      "Epoch 64/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0215 - acc: 0.9930     \n",
      "Epoch 65/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0213 - acc: 0.9930     \n",
      "Epoch 66/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0214 - acc: 0.9931     \n",
      "Epoch 67/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0213 - acc: 0.9930     \n",
      "Epoch 68/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0213 - acc: 0.9929     \n",
      "Epoch 69/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0212 - acc: 0.9931     \n",
      "Epoch 70/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0211 - acc: 0.9931     \n",
      "Epoch 71/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0209 - acc: 0.9932     \n",
      "Epoch 72/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0209 - acc: 0.9931     \n",
      "Epoch 73/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0209 - acc: 0.9931     \n",
      "Epoch 74/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0210 - acc: 0.9930     \n",
      "Epoch 75/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0208 - acc: 0.9932     \n",
      "Epoch 76/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0208 - acc: 0.9932     \n",
      "Epoch 77/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0206 - acc: 0.9932     \n",
      "Epoch 78/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0208 - acc: 0.9932     \n",
      "Epoch 79/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0206 - acc: 0.9932     \n",
      "Epoch 80/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0206 - acc: 0.9931     \n",
      "Epoch 81/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0205 - acc: 0.9932     \n",
      "Epoch 82/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0203 - acc: 0.9933     \n",
      "Epoch 83/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0204 - acc: 0.9933     \n",
      "Epoch 84/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0204 - acc: 0.9933     \n",
      "Epoch 85/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0202 - acc: 0.9934     \n",
      "Epoch 86/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0203 - acc: 0.9932     \n",
      "Epoch 87/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0202 - acc: 0.9935     \n",
      "Epoch 88/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0201 - acc: 0.9934     \n",
      "Epoch 89/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0201 - acc: 0.9934     \n",
      "Epoch 90/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0200 - acc: 0.9935     \n",
      "Epoch 91/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0200 - acc: 0.9934     \n",
      "Epoch 92/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0200 - acc: 0.9935     \n",
      "Epoch 93/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0198 - acc: 0.9935     \n",
      "Epoch 94/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0198 - acc: 0.9934     \n",
      "Epoch 95/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0198 - acc: 0.9936     \n",
      "Epoch 96/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0197 - acc: 0.9935     \n",
      "Epoch 97/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0196 - acc: 0.9935     \n",
      "Epoch 98/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0196 - acc: 0.9936     \n",
      "Epoch 99/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0196 - acc: 0.9934     \n",
      "Epoch 100/100\n",
      "76680/76680 [==============================] - 0s - loss: 0.0195 - acc: 0.9935     \n",
      "212352/226998 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in range(2, 8):\n",
    "    nn_model_col = build_nn_model()\n",
    "    nn_model_col.fit(trainDataVecs, data_train[:, i].astype(int), nb_epoch=100, batch_size=64)\n",
    "    preds_col = nn_model_col.predict_proba(testDataVecs)\n",
    "    preds.append(preds_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32), array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32), array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32), array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32), array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32), array([[ 0.00092868],\n",
       "        [ 0.0013055 ],\n",
       "        [ 0.00160392],\n",
       "        ..., \n",
       "        [ 0.00672257],\n",
       "        [ 0.00170642],\n",
       "        [ 0.00437022]], dtype=float32)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00092868123,\n",
       " 0.0013055028,\n",
       " 0.0016039159,\n",
       " 0.0053261793,\n",
       " 0.0082362574,\n",
       " 0.0042188037,\n",
       " 0.00046883323,\n",
       " 0.008981863,\n",
       " 0.00038316575,\n",
       " 0.0014333342,\n",
       " 0.00032618409,\n",
       " 0.060730528,\n",
       " 0.10396533,\n",
       " 0.00083693763,\n",
       " 0.16167924,\n",
       " 0.0065378835,\n",
       " 0.0013064755,\n",
       " 0.041894995,\n",
       " 0.00044829401,\n",
       " 0.027719274,\n",
       " 0.63852429,\n",
       " 0.0053703953,\n",
       " 0.0048807366,\n",
       " 0.016028071,\n",
       " 0.17519397,\n",
       " 8.722832e-05,\n",
       " 0.00087378005,\n",
       " 0.014160906,\n",
       " 1.1549428e-05,\n",
       " 0.011034016,\n",
       " 4.3363219e-05,\n",
       " 0.00071047869,\n",
       " 0.006599871,\n",
       " 0.75729984,\n",
       " 0.00079978484,\n",
       " 0.061450407,\n",
       " 0.10396533,\n",
       " 0.0002222529,\n",
       " 0.0038070306,\n",
       " 0.0012209133,\n",
       " 0.0015237827,\n",
       " 0.10396533,\n",
       " 0.014196377,\n",
       " 0.0003549493,\n",
       " 0.0088825691,\n",
       " 0.0012906719,\n",
       " 0.0021035261,\n",
       " 0.10396533,\n",
       " 0.037681501,\n",
       " 0.0018901028,\n",
       " 0.00016360145,\n",
       " 0.00049982418,\n",
       " 0.0034200524,\n",
       " 0.10396533,\n",
       " 0.0059615686,\n",
       " 0.0013220236,\n",
       " 0.34625867,\n",
       " 0.00036115063,\n",
       " 0.95104641,\n",
       " 0.0058751586,\n",
       " 0.00076318829,\n",
       " 0.0019280822,\n",
       " 8.4442647e-05,\n",
       " 0.012385482,\n",
       " 0.0099369772,\n",
       " 0.31234282,\n",
       " 0.051092595,\n",
       " 0.61626655,\n",
       " 0.0012468256,\n",
       " 0.10396533,\n",
       " 0.0011218906,\n",
       " 0.0024927598,\n",
       " 0.10396533,\n",
       " 0.02316539,\n",
       " 0.0020675291,\n",
       " 0.0012990631,\n",
       " 0.0033915476,\n",
       " 0.017433228,\n",
       " 0.0011362031,\n",
       " 0.0001391707,\n",
       " 0.001879478,\n",
       " 0.0046992488,\n",
       " 0.054201774,\n",
       " 0.0025293049,\n",
       " 0.48692876,\n",
       " 0.021434521,\n",
       " 0.0046885111,\n",
       " 0.047196571,\n",
       " 0.0020126414,\n",
       " 0.0029327129,\n",
       " 0.0041228933,\n",
       " 0.0015733421,\n",
       " 0.0020159753,\n",
       " 0.001870839,\n",
       " 0.00075049099,\n",
       " 0.0054778066,\n",
       " 0.03030706,\n",
       " 0.0128273,\n",
       " 0.079377681,\n",
       " 0.00085674069,\n",
       " 0.00034647071,\n",
       " 0.005734331,\n",
       " 0.012659465,\n",
       " 0.0069386773,\n",
       " 9.4686184e-05,\n",
       " 0.0065303552,\n",
       " 0.0025151991,\n",
       " 0.12289558,\n",
       " 0.0020017191,\n",
       " 0.41685572,\n",
       " 0.020986037,\n",
       " 0.0001110791,\n",
       " 0.0050879023,\n",
       " 1.236278e-06,\n",
       " 0.0018015698,\n",
       " 6.9296992e-05,\n",
       " 0.43866012,\n",
       " 0.0059096762,\n",
       " 0.0077637127,\n",
       " 0.0033572244,\n",
       " 0.019683171,\n",
       " 0.00026602476,\n",
       " 0.10396533,\n",
       " 0.0010623665,\n",
       " 0.0023909882,\n",
       " 0.10396533,\n",
       " 0.014764813,\n",
       " 1.7780365e-05,\n",
       " 0.0020074043,\n",
       " 0.0028408235,\n",
       " 0.46988586,\n",
       " 0.0026975372,\n",
       " 0.0078654131,\n",
       " 0.006086533,\n",
       " 0.0029220963,\n",
       " 0.0020399836,\n",
       " 0.033970743,\n",
       " 0.0063933898,\n",
       " 0.92563593,\n",
       " 7.8283134e-05,\n",
       " 0.00050642545,\n",
       " 0.0038026711,\n",
       " 0.10396533,\n",
       " 0.024718873,\n",
       " 0.00072736834,\n",
       " 0.00086667528,\n",
       " 0.0018406013,\n",
       " 0.0027590168,\n",
       " 0.0033137845,\n",
       " 0.011016646,\n",
       " 0.0099935951,\n",
       " 0.00030332658,\n",
       " 0.014847026,\n",
       " 0.37700549,\n",
       " 0.0035846054,\n",
       " 0.0026889162,\n",
       " 0.013059699,\n",
       " 0.017940726,\n",
       " 0.01834663,\n",
       " 0.0018108556,\n",
       " 0.0013682507,\n",
       " 0.019269511,\n",
       " 0.011025336,\n",
       " 0.0062062661,\n",
       " 2.2415421e-05,\n",
       " 0.0011394254,\n",
       " 0.0097603342,\n",
       " 0.0013858395,\n",
       " 0.0048199245,\n",
       " 0.0013351116,\n",
       " 0.00041961568,\n",
       " 0.003837032,\n",
       " 0.0019827029,\n",
       " 0.00027417328,\n",
       " 6.753581e-05,\n",
       " 0.0035325042,\n",
       " 0.00057335949,\n",
       " 0.0036605976,\n",
       " 0.016499283,\n",
       " 0.01786226,\n",
       " 0.10396533,\n",
       " 0.008647088,\n",
       " 0.0010492831,\n",
       " 0.013478448,\n",
       " 0.0080597419,\n",
       " 0.0065070032,\n",
       " 0.0015699272,\n",
       " 0.10396533,\n",
       " 0.00071266637,\n",
       " 0.013772218,\n",
       " 3.7568709e-06,\n",
       " 0.1393057,\n",
       " 0.11375143,\n",
       " 0.0001956497,\n",
       " 0.032511488,\n",
       " 0.0112397,\n",
       " 0.0093642157,\n",
       " 0.013190669,\n",
       " 0.015179073,\n",
       " 0.00077502022,\n",
       " 0.0085193422,\n",
       " 7.1015544e-05,\n",
       " 0.25743809,\n",
       " 0.0097127594,\n",
       " 0.0017525244,\n",
       " 4.7620543e-07,\n",
       " 0.053005259,\n",
       " 0.0036471388,\n",
       " 0.0073795579,\n",
       " 0.065578356,\n",
       " 0.014038987,\n",
       " 0.001649059,\n",
       " 0.0012357179,\n",
       " 0.0008367594,\n",
       " 0.00064796797,\n",
       " 0.0023433252,\n",
       " 0.0012484649,\n",
       " 0.0084969774,\n",
       " 0.0018923655,\n",
       " 0.0010140463,\n",
       " 0.0024629554,\n",
       " 0.0030726285,\n",
       " 4.527979e-05,\n",
       " 0.20588334,\n",
       " 8.0888611e-05,\n",
       " 0.0043471055,\n",
       " 5.865936e-05,\n",
       " 0.0023073228,\n",
       " 0.0032048179,\n",
       " 5.9036975e-05,\n",
       " 0.0044842022,\n",
       " 0.86721897,\n",
       " 1.861122e-06,\n",
       " 0.61803663,\n",
       " 0.5160715,\n",
       " 0.087258898,\n",
       " 0.0032705986,\n",
       " 0.00066872558,\n",
       " 0.0070135719,\n",
       " 0.0096904766,\n",
       " 0.0018872272,\n",
       " 0.007686473,\n",
       " 0.00042252577,\n",
       " 0.00027103082,\n",
       " 0.00014003782,\n",
       " 0.14985403,\n",
       " 0.0050925049,\n",
       " 0.0095165465,\n",
       " 0.12838046,\n",
       " 0.007741902,\n",
       " 0.0032040516,\n",
       " 0.0040561617,\n",
       " 0.0018410832,\n",
       " 0.0018534695,\n",
       " 0.026240656,\n",
       " 0.00011442073,\n",
       " 0.040524632,\n",
       " 0.14820588,\n",
       " 0.78270012,\n",
       " 0.95260304,\n",
       " 0.10396533,\n",
       " 0.0015768816,\n",
       " 0.12119982,\n",
       " 0.027628222,\n",
       " 0.0059144292,\n",
       " 0.00044995538,\n",
       " 0.00020235988,\n",
       " 0.003158516,\n",
       " 0.0046966379,\n",
       " 0.00021476769,\n",
       " 0.021521246,\n",
       " 0.0014650848,\n",
       " 0.00046864941,\n",
       " 5.1809337e-05,\n",
       " 0.00072936603,\n",
       " 0.0004111469,\n",
       " 0.003783755,\n",
       " 0.0023917709,\n",
       " 0.0024140989,\n",
       " 0.001954156,\n",
       " 0.0045968061,\n",
       " 0.020068914,\n",
       " 0.0048108804,\n",
       " 0.0038627579,\n",
       " 0.016087787,\n",
       " 0.14241123,\n",
       " 0.015219743,\n",
       " 0.1066462,\n",
       " 0.00018890262,\n",
       " 0.0042420081,\n",
       " 0.00094842387,\n",
       " 0.0015817673,\n",
       " 0.0079739206,\n",
       " 0.0099506518,\n",
       " 0.0010357463,\n",
       " 0.0097909095,\n",
       " 0.0098929247,\n",
       " 0.0040623401,\n",
       " 0.19789371,\n",
       " 0.0015752938,\n",
       " 0.0039224713,\n",
       " 0.00012243747,\n",
       " 0.0013943067,\n",
       " 0.0005371343,\n",
       " 0.0029706708,\n",
       " 0.44563699,\n",
       " 0.0015028908,\n",
       " 0.0023359014,\n",
       " 0.087361619,\n",
       " 0.01501623,\n",
       " 0.013798408,\n",
       " 0.39651868,\n",
       " 0.00034675756,\n",
       " 0.0025087211,\n",
       " 0.0050470801,\n",
       " 0.00046876489,\n",
       " 0.0003540699,\n",
       " 0.00055914465,\n",
       " 0.0085914293,\n",
       " 0.00033637907,\n",
       " 0.0056382315,\n",
       " 0.0047182259,\n",
       " 0.017329538,\n",
       " 0.00052736251,\n",
       " 0.0036779463,\n",
       " 0.0080782175,\n",
       " 0.0081431819,\n",
       " 0.0099035185,\n",
       " 0.0028582106,\n",
       " 0.0014832104,\n",
       " 0.015023657,\n",
       " 2.8258381e-07,\n",
       " 0.0084430361,\n",
       " 0.00051192282,\n",
       " 0.0017176855,\n",
       " 0.0037295618,\n",
       " 0.024234641,\n",
       " 0.0091858571,\n",
       " 0.00068221387,\n",
       " 0.00012680062,\n",
       " 0.12459647,\n",
       " 0.0058251405,\n",
       " 0.00101105,\n",
       " 0.00010383649,\n",
       " 0.0011208828,\n",
       " 0.0021400077,\n",
       " 0.0024698596,\n",
       " 0.0093910992,\n",
       " 0.23114298,\n",
       " 0.0018636113,\n",
       " 0.00021430805,\n",
       " 0.0046184328,\n",
       " 0.002196118,\n",
       " 0.0069679581,\n",
       " 0.0026805508,\n",
       " 0.0079643792,\n",
       " 0.0011211834,\n",
       " 0.00022820056,\n",
       " 0.042363353,\n",
       " 0.00038129845,\n",
       " 0.0043043653,\n",
       " 0.0017420315,\n",
       " 0.0046855947,\n",
       " 0.0013612211,\n",
       " 0.00098640146,\n",
       " 0.00039229396,\n",
       " 0.17645739,\n",
       " 0.0080405055,\n",
       " 0.0061916024,\n",
       " 0.0030234349,\n",
       " 0.15965956,\n",
       " 0.070444465,\n",
       " 0.0031147182,\n",
       " 0.0018682603,\n",
       " 0.58820653,\n",
       " 0.0013752222,\n",
       " 0.025208451,\n",
       " 0.021582935,\n",
       " 0.0057206778,\n",
       " 8.4596167e-07,\n",
       " 0.35571748,\n",
       " 0.0046836203,\n",
       " 0.11856357,\n",
       " 0.0040702904,\n",
       " 0.00021536893,\n",
       " 2.8459892e-06,\n",
       " 0.014269058,\n",
       " 0.00090408401,\n",
       " 0.0044125216,\n",
       " 0.10145196,\n",
       " 0.00043171298,\n",
       " 0.45842996,\n",
       " 0.0005155963,\n",
       " 0.001986142,\n",
       " 0.005451622,\n",
       " 0.0084245848,\n",
       " 0.0021886744,\n",
       " 0.000872284,\n",
       " 0.055816188,\n",
       " 0.014379072,\n",
       " 0.0023555502,\n",
       " 0.00079674477,\n",
       " 0.011815954,\n",
       " 0.022076627,\n",
       " 0.00010490118,\n",
       " 0.016484812,\n",
       " 1.9546174e-05,\n",
       " 0.0017880558,\n",
       " 0.030287299,\n",
       " 0.016580345,\n",
       " 0.0021552253,\n",
       " 0.02421616,\n",
       " 0.039272431,\n",
       " 0.0013140861,\n",
       " 0.0042281169,\n",
       " 0.0021345771,\n",
       " 3.1036583e-05,\n",
       " 0.0095135942,\n",
       " 0.0015666636,\n",
       " 0.026551129,\n",
       " 0.006806639,\n",
       " 0.15181848,\n",
       " 0.00014175237,\n",
       " 0.00071089895,\n",
       " 0.0017334396,\n",
       " 0.013941341,\n",
       " 0.00065840653,\n",
       " 0.00053544971,\n",
       " 0.00099110452,\n",
       " 0.011668321,\n",
       " 0.0032359571,\n",
       " 0.0036039592,\n",
       " 8.4424129e-05,\n",
       " 0.0059827087,\n",
       " 0.090621598,\n",
       " 0.006083874,\n",
       " 0.02566627,\n",
       " 0.0068243202,\n",
       " 0.00081524323,\n",
       " 0.0034775976,\n",
       " 0.0053724307,\n",
       " 0.076344363,\n",
       " 0.10396533,\n",
       " 0.00084079162,\n",
       " 0.0035314451,\n",
       " 0.024840279,\n",
       " 0.016745614,\n",
       " 0.27897465,\n",
       " 0.00080404646,\n",
       " 0.027654724,\n",
       " 0.0046705529,\n",
       " 0.00047023324,\n",
       " 4.4499669e-05,\n",
       " 0.0075234054,\n",
       " 0.0018687031,\n",
       " 0.0089430213,\n",
       " 0.00060577743,\n",
       " 0.0054980069,\n",
       " 0.00013000888,\n",
       " 0.35604793,\n",
       " 0.0045238337,\n",
       " 0.31882676,\n",
       " 0.018574134,\n",
       " 0.99363589,\n",
       " 0.01387606,\n",
       " 0.034512646,\n",
       " 0.34721467,\n",
       " 5.4670759e-06,\n",
       " 0.88206369,\n",
       " 0.0039088055,\n",
       " 1.3365796e-05,\n",
       " 0.0015144605,\n",
       " 0.010435706,\n",
       " 0.0015615032,\n",
       " 0.023442049,\n",
       " 0.0014874019,\n",
       " 0.017670352,\n",
       " 0.013684032,\n",
       " 0.0038151373,\n",
       " 0.014366473,\n",
       " 0.60702348,\n",
       " 0.030531954,\n",
       " 0.001366058,\n",
       " 0.021662077,\n",
       " 0.0010462537,\n",
       " 0.00075670145,\n",
       " 0.0019776751,\n",
       " 0.012508681,\n",
       " 0.0011591712,\n",
       " 0.00016715025,\n",
       " 0.0023274126,\n",
       " 0.010549375,\n",
       " 0.0016790304,\n",
       " 0.42431259,\n",
       " 0.11923376,\n",
       " 0.044789288,\n",
       " 0.0012789761,\n",
       " 0.00010066206,\n",
       " 0.078657843,\n",
       " 0.0082258219,\n",
       " 0.0010112951,\n",
       " 0.00061121414,\n",
       " 0.00011376254,\n",
       " 0.018610416,\n",
       " 0.0041012787,\n",
       " 0.014144712,\n",
       " 0.015219764,\n",
       " 0.0054665725,\n",
       " 0.0098312339,\n",
       " 0.95137042,\n",
       " 0.0033681185,\n",
       " 0.00080599147,\n",
       " 0.0099056279,\n",
       " 0.001815134,\n",
       " 0.02840881,\n",
       " 0.0021757837,\n",
       " 0.0035074763,\n",
       " 0.01589605,\n",
       " 0.012490283,\n",
       " 0.00060789974,\n",
       " 0.0027936951,\n",
       " 0.15723865,\n",
       " 0.004857867,\n",
       " 0.0017578403,\n",
       " 0.0068362881,\n",
       " 0.014333804,\n",
       " 0.022128921,\n",
       " 0.069997169,\n",
       " 0.028641954,\n",
       " 0.022929711,\n",
       " 0.013570018,\n",
       " 0.0062402091,\n",
       " 0.0012892933,\n",
       " 0.028003518,\n",
       " 9.8829994e-05,\n",
       " 7.0024187e-05,\n",
       " 0.023174908,\n",
       " 0.14218009,\n",
       " 0.00078525761,\n",
       " 0.0014374986,\n",
       " 0.0076380344,\n",
       " 0.00089895976,\n",
       " 0.00044389666,\n",
       " 0.10396533,\n",
       " 0.0027568659,\n",
       " 0.0044798129,\n",
       " 0.014567861,\n",
       " 0.0010177357,\n",
       " 0.00019992729,\n",
       " 0.00099945976,\n",
       " 0.0072724521,\n",
       " 0.0015662333,\n",
       " 0.011109261,\n",
       " 0.0037306992,\n",
       " 0.2767309,\n",
       " 0.0040570092,\n",
       " 6.1139719e-05,\n",
       " 0.030307332,\n",
       " 0.075634383,\n",
       " 0.10396533,\n",
       " 0.0081870584,\n",
       " 3.009256e-05,\n",
       " 0.0012022383,\n",
       " 0.83840972,\n",
       " 0.0023253465,\n",
       " 0.043437783,\n",
       " 0.10396533,\n",
       " 0.016119057,\n",
       " 0.034594212,\n",
       " 0.64728415,\n",
       " 0.0034749068,\n",
       " 0.0027225513,\n",
       " 0.17827632,\n",
       " 0.0011974723,\n",
       " 0.0058727022,\n",
       " 0.00087199977,\n",
       " 0.0032766995,\n",
       " 0.013779415,\n",
       " 0.018184477,\n",
       " 0.0014259496,\n",
       " 0.00035497657,\n",
       " 0.0068404917,\n",
       " 0.0025087211,\n",
       " 8.2248705e-05,\n",
       " 0.0054857144,\n",
       " 0.0097702714,\n",
       " 0.13106769,\n",
       " 0.010829625,\n",
       " 0.0083442377,\n",
       " 1.6807837e-05,\n",
       " 0.0034918811,\n",
       " 0.25496233,\n",
       " 0.94295937,\n",
       " 0.95037687,\n",
       " 0.00064223242,\n",
       " 0.043128155,\n",
       " 0.00047715721,\n",
       " 0.13737114,\n",
       " 0.0021221694,\n",
       " 0.003018555,\n",
       " 0.0059775952,\n",
       " 0.0055821561,\n",
       " 6.0187995e-05,\n",
       " 0.29093108,\n",
       " 0.19049637,\n",
       " 0.011958687,\n",
       " 0.0020372197,\n",
       " 0.017815525,\n",
       " 0.0076870006,\n",
       " 0.42759368,\n",
       " 0.000946199,\n",
       " 0.0039410652,\n",
       " 0.0074586915,\n",
       " 0.012421696,\n",
       " 0.0012341834,\n",
       " 0.0037226176,\n",
       " 0.048018001,\n",
       " 0.017506752,\n",
       " 0.010012545,\n",
       " 0.00049420149,\n",
       " 2.2882576e-05,\n",
       " 0.025731901,\n",
       " 0.00011918078,\n",
       " 0.0018146529,\n",
       " 0.52235055,\n",
       " 0.0053191059,\n",
       " 0.078289293,\n",
       " 0.0044271797,\n",
       " 0.46640855,\n",
       " 0.00023310161,\n",
       " 0.0022124767,\n",
       " 0.0016756671,\n",
       " 0.0076224185,\n",
       " 0.020509494,\n",
       " 0.0010500301,\n",
       " 0.0033523203,\n",
       " 0.0034810249,\n",
       " 0.00069457188,\n",
       " 0.0048014973,\n",
       " 0.0098279668,\n",
       " 0.0035041012,\n",
       " 4.0596551e-05,\n",
       " 0.00022808637,\n",
       " 0.025235439,\n",
       " 0.0036269478,\n",
       " 0.001254836,\n",
       " 0.10396533,\n",
       " 0.00093534659,\n",
       " 0.00040676945,\n",
       " 0.0016812953,\n",
       " 0.018023234,\n",
       " 0.0049140113,\n",
       " 0.047790177,\n",
       " 0.017292971,\n",
       " 0.0040063164,\n",
       " 0.0048055388,\n",
       " 0.031719893,\n",
       " 0.079105377,\n",
       " 0.010024738,\n",
       " 0.23224887,\n",
       " 0.026420515,\n",
       " 0.0081379386,\n",
       " 0.01996889,\n",
       " 0.006323101,\n",
       " 0.050063539,\n",
       " 5.7866408e-05,\n",
       " 0.0010357463,\n",
       " 0.0014541883,\n",
       " 0.00012505113,\n",
       " 0.0018147592,\n",
       " 0.10396533,\n",
       " 0.013752717,\n",
       " 0.0085306922,\n",
       " 0.0019519709,\n",
       " 0.14357109,\n",
       " 0.05386154,\n",
       " 0.0002054848,\n",
       " 0.0036734915,\n",
       " 0.009350867,\n",
       " 0.0010814845,\n",
       " 0.0078702625,\n",
       " 0.0037795387,\n",
       " 0.0032092992,\n",
       " 0.00077850395,\n",
       " 0.029352939,\n",
       " 0.024840083,\n",
       " 0.00057906203,\n",
       " 0.10396533,\n",
       " 0.011189658,\n",
       " 0.00091999007,\n",
       " 0.5514065,\n",
       " 0.2336694,\n",
       " 0.016329456,\n",
       " 8.9614543e-05,\n",
       " 0.0044377279,\n",
       " 0.0050604832,\n",
       " 0.00068700768,\n",
       " 0.02184586,\n",
       " 0.0062905224,\n",
       " 0.88640416,\n",
       " 0.40028378,\n",
       " 0.0051241689,\n",
       " 0.0017918464,\n",
       " 0.0072754924,\n",
       " 0.014941175,\n",
       " 0.024942284,\n",
       " 0.0014101724,\n",
       " 0.011142342,\n",
       " 0.0014736665,\n",
       " 0.10396533,\n",
       " 0.34934226,\n",
       " 0.02822599,\n",
       " 0.0002042125,\n",
       " 0.00072342349,\n",
       " 0.00045858361,\n",
       " 0.00010209296,\n",
       " 9.3229726e-05,\n",
       " 0.0068937698,\n",
       " 0.14272031,\n",
       " 0.012741601,\n",
       " 0.0020440861,\n",
       " 0.0051015387,\n",
       " 0.091448106,\n",
       " 0.0029606393,\n",
       " 0.0011058231,\n",
       " 0.019621078,\n",
       " 0.0041686068,\n",
       " 0.004060992,\n",
       " 0.090119027,\n",
       " 0.0011992822,\n",
       " 0.0053077247,\n",
       " 0.00063615513,\n",
       " 0.093832895,\n",
       " 1.6417017e-07,\n",
       " 0.59522432,\n",
       " 0.17751952,\n",
       " 0.00043084307,\n",
       " 0.0023364194,\n",
       " 0.043200538,\n",
       " 0.0049649109,\n",
       " 0.041208703,\n",
       " 0.017512748,\n",
       " 0.0026286431,\n",
       " 0.001970133,\n",
       " 0.0019672278,\n",
       " 0.0015676477,\n",
       " 0.0091831842,\n",
       " 0.026478991,\n",
       " 0.4358972,\n",
       " 0.015600782,\n",
       " 0.002485397,\n",
       " 0.0053914771,\n",
       " 0.047285005,\n",
       " 0.33010107,\n",
       " 0.010573856,\n",
       " 0.0045863274,\n",
       " 0.0084663806,\n",
       " 0.00039849125,\n",
       " 0.030959984,\n",
       " 0.0094926562,\n",
       " 0.0023619654,\n",
       " 0.00077372772,\n",
       " 0.013878258,\n",
       " 0.0033108534,\n",
       " 0.027671091,\n",
       " 0.013349551,\n",
       " 0.0065721083,\n",
       " 0.00087574887,\n",
       " 0.0029863536,\n",
       " 0.023127886,\n",
       " 0.083053343,\n",
       " 0.0072272737,\n",
       " 0.010079104,\n",
       " 0.025581392,\n",
       " 0.0017489718,\n",
       " 0.0053054746,\n",
       " 0.0015057321,\n",
       " 0.011600833,\n",
       " 0.27374494,\n",
       " 0.0045661377,\n",
       " 0.18566597,\n",
       " 0.00034972822,\n",
       " 0.32301369,\n",
       " 0.035550356,\n",
       " 0.00030171519,\n",
       " 3.3727218e-05,\n",
       " 0.18877132,\n",
       " 0.0019858757,\n",
       " 0.00072528631,\n",
       " 0.0014360866,\n",
       " 7.4092422e-05,\n",
       " 0.12708408,\n",
       " 0.00031763141,\n",
       " 0.0011068101,\n",
       " 0.015449997,\n",
       " 0.0031521867,\n",
       " 0.0038376625,\n",
       " 0.0096574705,\n",
       " 0.0038956292,\n",
       " 0.0025549813,\n",
       " 0.29390362,\n",
       " 0.02262928,\n",
       " 0.00015330024,\n",
       " 0.0016761458,\n",
       " 3.1182455e-05,\n",
       " 0.21281943,\n",
       " 0.00017950812,\n",
       " 0.0052601234,\n",
       " 0.067458898,\n",
       " 0.018157395,\n",
       " 0.00086270098,\n",
       " 0.05418722,\n",
       " 4.2485786e-05,\n",
       " 0.00011409921,\n",
       " 0.0041628289,\n",
       " 0.0016757317,\n",
       " 0.0025554893,\n",
       " 0.00035396716,\n",
       " 0.084574349,\n",
       " 0.0050528687,\n",
       " 0.19382551,\n",
       " 0.0060086548,\n",
       " 0.0060263351,\n",
       " 0.0017065841,\n",
       " 0.0012380707,\n",
       " 0.00027547366,\n",
       " 0.0231562,\n",
       " 0.00010790079,\n",
       " 0.018697267,\n",
       " 0.028207431,\n",
       " 0.0027805404,\n",
       " 0.0064882762,\n",
       " 3.9560032e-06,\n",
       " 0.0034348476,\n",
       " 0.063221119,\n",
       " 0.00028892775,\n",
       " 0.0012187039,\n",
       " 0.0055403537,\n",
       " 0.0065877461,\n",
       " 0.0052028336,\n",
       " 0.0086693568,\n",
       " 0.0036067411,\n",
       " 0.02251556,\n",
       " 0.0082315458,\n",
       " 0.13351534,\n",
       " 0.019217711,\n",
       " 0.021064954,\n",
       " 0.0012050202,\n",
       " 1.3365796e-05,\n",
       " 0.00022306708,\n",
       " 0.0032160883,\n",
       " 0.0048659947,\n",
       " 0.001772264,\n",
       " 0.00019021081,\n",
       " 0.17649153,\n",
       " 0.024444753,\n",
       " 0.00019314006,\n",
       " 0.0067629344,\n",
       " 0.00059292582,\n",
       " 0.0086693568,\n",
       " 0.011743521,\n",
       " 0.0013651372,\n",
       " 1.6416986e-07,\n",
       " 0.015130111,\n",
       " 0.016119057,\n",
       " 0.4514738,\n",
       " 0.0033984883,\n",
       " 0.0075592371,\n",
       " 0.021661086,\n",
       " 0.014624277,\n",
       " 0.00087452517,\n",
       " 6.940704e-05,\n",
       " 0.0053789499,\n",
       " 0.011390212,\n",
       " 0.00029025797,\n",
       " 0.026183318,\n",
       " 0.0077033378,\n",
       " 0.0020826466,\n",
       " 0.2508541,\n",
       " 0.00072550162,\n",
       " 0.0055746068,\n",
       " 0.28984544,\n",
       " 0.00052096799,\n",
       " 0.019448385,\n",
       " 0.0011922272,\n",
       " 0.0055649639,\n",
       " 0.000582857,\n",
       " 0.0031795534,\n",
       " 0.0048365542,\n",
       " 0.00080215896,\n",
       " 0.0056729522,\n",
       " 8.9019355e-05,\n",
       " 0.62263805,\n",
       " 0.021083193,\n",
       " 0.0024332162,\n",
       " 0.015357084,\n",
       " 0.0036124776,\n",
       " 0.046315283,\n",
       " 0.0010521986,\n",
       " 0.029235927,\n",
       " 0.00043897252,\n",
       " 0.089418061,\n",
       " 0.0098689599,\n",
       " 0.00038261639,\n",
       " 0.4047716,\n",
       " 0.0010679002,\n",
       " 0.00083735038,\n",
       " 0.0050632018,\n",
       " 0.0058770049,\n",
       " 0.00062745356,\n",
       " 0.0016100998,\n",
       " 0.0016349397,\n",
       " 0.0019803999,\n",
       " 0.00031788435,\n",
       " 0.00046006136,\n",
       " 0.041269176,\n",
       " 0.0015524781,\n",
       " 0.0022413703,\n",
       " 0.00038695749,\n",
       " 0.32394007,\n",
       " 0.71214271,\n",
       " 0.001676276,\n",
       " 0.057772063,\n",
       " 0.62453127,\n",
       " 0.022185175,\n",
       " 0.0040879608,\n",
       " 0.00034138456,\n",
       " 0.0063572782,\n",
       " 0.0033240325,\n",
       " 0.014920844,\n",
       " 0.00077030237,\n",
       " 0.020384545,\n",
       " 0.058886692,\n",
       " 0.0012588846,\n",
       " 0.013068356,\n",
       " 0.0022471501,\n",
       " 0.00445429,\n",
       " 0.035229065,\n",
       " 0.00053359749,\n",
       " 0.036584683,\n",
       " 0.00018251917,\n",
       " 0.20557302,\n",
       " 0.10396533,\n",
       " 0.0014876286,\n",
       " 0.0008172823,\n",
       " 0.0047225854,\n",
       " 0.039897248,\n",
       " 0.00071245583,\n",
       " 0.0090233833,\n",
       " 0.00024367713,\n",
       " 0.00071300194,\n",
       " 0.0098019186,\n",
       " 0.089687586,\n",
       " 0.00056781853,\n",
       " 0.0070545468,\n",
       " 0.013843232,\n",
       " 0.0038788696,\n",
       " 0.10396533,\n",
       " 0.00011896095,\n",
       " 0.0044007921,\n",
       " 0.0075790989,\n",
       " 2.0399939e-05,\n",
       " 0.10396533,\n",
       " 0.00040479432,\n",
       " 0.087280259,\n",
       " 5.7613997e-06,\n",
       " 0.00021757782,\n",
       " 0.0022558868,\n",
       " 0.00052436063,\n",
       " 0.97440547,\n",
       " 2.2359325e-06,\n",
       " 0.00093626097,\n",
       " 0.001765451,\n",
       " 0.010421696,\n",
       " 0.069044225,\n",
       " 0.0017030176,\n",
       " 0.048281986,\n",
       " 0.064547263,\n",
       " 0.00016577127,\n",
       " 0.020126827,\n",
       " 0.030099703,\n",
       " 8.8279075e-06,\n",
       " 0.13493915,\n",
       " 0.0029462145,\n",
       " 0.0038675813,\n",
       " 0.0024914287,\n",
       " 0.0052204849,\n",
       " 0.77287889,\n",
       " 0.99732947,\n",
       " 0.0015778218,\n",
       " 0.55613738,\n",
       " 0.0015938211,\n",
       " 0.07071799,\n",
       " 0.0008172823,\n",
       " 0.0052375374,\n",
       " 0.0025978794,\n",
       " 7.6716206e-06,\n",
       " 0.11343301,\n",
       " 0.024107147,\n",
       " 0.006637726,\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(preds[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'id': test_data.id, 'toxic': list(preds[0].reshape(-1)),\\\n",
    "     'severe_toxic':  list(preds[1].reshape(-1)), 'obscene': list(preds[2].reshape(-1)),\\\n",
    "     'threat': list(preds[3].reshape(-1)), 'insult': list(preds[4].reshape(-1)),\\\n",
    "     'identity_hate': list(preds[5].reshape(-1))}\n",
    "toSubmit = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toSubmit.to_csv('./../submissions/submit1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(trainDataVecs, data_train[:, 2].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_clf_1 = clf.predict(validDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17138,   120],\n",
       "       [  993,   920]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(data_valid[:, 2].astype(int), preds_clf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94194356058630224"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(data_valid[:, 2].astype(int), preds_clf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Build the bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "  \n",
    "    review_text = BeautifulSoup(str(raw_review)).get_text() \n",
    "          \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    \n",
    "    words = letters_only.lower().split()                             \n",
    "   \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "   \n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/apple/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://en.wikipedia.org/wiki/Wikipedia_talk:No_original_research/archive15#YouTube_art_as_primary_source\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://finance.yahoo.com/news/7-fascinating-nuggets-another-bewildering-150348488.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews_bow = []\n",
    "for review in data_train[:, 1]:\n",
    "    clean_train_reviews_bow.append(review_to_words(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/apple/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_valid_reviews_bow = []\n",
    "for review in data_valid[:, 1]:\n",
    "    clean_valid_reviews_bow.append(review_to_words(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 10000) \n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews_bow)\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_data_features = vectorizer.transform(clean_valid_reviews_bow)\n",
    "valid_data_features = valid_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/apple/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://en.wikipedia.org/wiki/Wikipedia:ELYES\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.roadrunnerrecords.com/blabbermouth.net/news.aspx?mode=Article&newsitemID;=127089\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http:/www.localhikes.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.roadrunnerrecords.com/blabbermouth.net/news.aspx?mode=Article&newsitemID;=25111\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.haaretz.com/news/diplomacy-defense/2-279-calories-per-person-how-israel-made-sure-gaza-didn-t-starve.premium-1.470419\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://ftp.mozilla.org/pub/mozilla.org/firefox/nightly/latest-trunk-l10n/firefox-3.0a7pre.tr.win32.zip\n",
      "http://ftp-mozilla.netscape.com/pub/mozilla.org/firefox/releases/2.0.0.5/win32/tr/Firefox%20Setup%202.0.0.5.exe\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/apple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews_bow = []\n",
    "for review in test_data.iloc[:, 1]:\n",
    "    clean_test_reviews_bow.append(review_to_words(review))\n",
    "test_data_features = vectorizer.transform(clean_test_reviews_bow)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_bow = RandomForestClassifier(n_estimators = 100) \n",
    "forest_bow.fit(train_data_features, data_train[:, 2].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_forest_bow_1 = forest_bow.predict(valid_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16925,   333],\n",
       "       [  857,  1056]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(data_valid[:, 2].astype(int), preds_forest_bow_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93792707735642378"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(data_valid[:, 2].astype(int), preds_forest_bow_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### nn for bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn_model2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=10000, activation='tanh'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.2294 - acc: 0.9281     \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.1294 - acc: 0.9593     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.1055 - acc: 0.9646     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d6c000cc0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model_bow = build_nn_model2()\n",
    "nn_model_bow.fit(train_data_features, data_train[:, 2].astype(int), nb_epoch=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75712/76680 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06912601746892752, 0.97556077203964531]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nn_model_bow.evaluate(train_data_features, data_train[:, 2].astype(int), batch_size=64)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19171/19171 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18272877798696172, 0.95336706484373324]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = nn_model_bow.evaluate(valid_data_features, data_valid[:, 2].astype(int), batch_size=64)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "76680/76680 [==============================] - 7s - loss: 0.2268 - acc: 0.9291     \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.1261 - acc: 0.9599     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.1016 - acc: 0.9651     \n",
      "226688/226998 [============================>.] - ETA: 0sEpoch 1/3\n",
      "76680/76680 [==============================] - 9s - loss: 0.0738 - acc: 0.9857     \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0316 - acc: 0.9903     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0266 - acc: 0.9905     \n",
      "226752/226998 [============================>.] - ETA: 0sEpoch 1/3\n",
      "76680/76680 [==============================] - 9s - loss: 0.1601 - acc: 0.9547     \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 3s - loss: 0.0750 - acc: 0.9789     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 3s - loss: 0.0555 - acc: 0.9839     \n",
      "226592/226998 [============================>.] - ETA: 0sEpoch 1/3\n",
      "76680/76680 [==============================] - 10s - loss: 0.0494 - acc: 0.9930    \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0147 - acc: 0.9968     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 5s - loss: 0.0108 - acc: 0.9970     \n",
      "226784/226998 [============================>.] - ETA: 0sEpoch 1/3\n",
      "76680/76680 [==============================] - 11s - loss: 0.1570 - acc: 0.9573    \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0866 - acc: 0.9710     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 3s - loss: 0.0691 - acc: 0.9748     \n",
      "226720/226998 [============================>.] - ETA: 0sEpoch 1/3\n",
      "76680/76680 [==============================] - 10s - loss: 0.0753 - acc: 0.9878    \n",
      "Epoch 2/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0307 - acc: 0.9912     \n",
      "Epoch 3/3\n",
      "76680/76680 [==============================] - 4s - loss: 0.0235 - acc: 0.9917     \n",
      "226998/226998 [==============================] - 25s    \n"
     ]
    }
   ],
   "source": [
    "preds2 = []\n",
    "for i in range(2, 8):\n",
    "    nn_model_col = build_nn_model2()\n",
    "    nn_model_col.fit(train_data_features, data_train[:, i].astype(int), nb_epoch=3, batch_size=64)\n",
    "    preds_col = nn_model_col.predict_proba(test_data_features)\n",
    "    preds2.append(preds_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'id': test_data.id, 'toxic': list(preds2[0].reshape(-1)),\\\n",
    "     'severe_toxic':  list(preds2[1].reshape(-1)), 'obscene': list(preds2[2].reshape(-1)),\\\n",
    "     'threat': list(preds2[3].reshape(-1)), 'insult': list(preds2[4].reshape(-1)),\\\n",
    "     'identity_hate': list(preds2[5].reshape(-1))}\n",
    "toSubmit = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toSubmit.to_csv('./../submissions/submit2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(stop_words='english', max_features=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_train = tfvectorizer.fit_transform(data_train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_valid = tfvectorizer.transform(data_valid[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76680, 50000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn_model3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=50000, activation='tanh'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "76680/76680 [==============================] - 199s - loss: 0.2149 - acc: 0.9337   \n",
      "Epoch 2/2\n",
      "76680/76680 [==============================] - 337s - loss: 0.1079 - acc: 0.9637   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9c01bc50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = build_nn_model3()\n",
    "model_3.fit(tf_train.toarray(), data_train[:, 2].astype(int), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19171/19171 [==============================] - 10s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15321871035714693, 0.95587084659120547]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model_3.evaluate(tf_valid.toarray(), data_valid[:, 2].astype(int), batch_size=64)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_3.save('model_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
